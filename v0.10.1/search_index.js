var documenterSearchIndex = {"docs":
[{"location":"processors/#Processors","page":"Processors","title":"Processors","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger contains a flexible mechanism to represent CPUs, GPUs, and other devices that the scheduler can place user work on. The indiviual devices that are capable of computing a user operation are called \"processors\", and are subtypes of Dagger.Processor. Processors are automatically detected by Dagger at scheduler initialization, and placed in a hierarchy reflecting the physical (network-, link-, or memory-based) boundaries between processors in the hierarchy. The scheduler uses the information in this hierarchy to efficiently schedule and partition user operations.","category":"page"},{"location":"processors/#Hardware-capabilities,-topology,-and-data-locality","page":"Processors","title":"Hardware capabilities, topology, and data locality","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"The processor hierarchy is modeled as a multi-root tree, where each root is an OSProc, which represents a Julia OS process, and the \"children\" of the root or some other branch in the tree represent the processors which reside on the same logical server as the \"parent\" branch. All roots are connected to each other directly, in the common case. The processor hierarchy's topology is automatically detected and elaborated by callbacks in Dagger, which users may manipulate to add detection of extra processors.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Movement of data between any two processors is decomposable into a sequence of \"moves\" between a child and its parent, termed a \"generic path move\". Movement of data may also take \"shortcuts\" between nodes in the tree which are not directly connected if enabled by libraries or the user, which may make use of IPC mechanisms to transfer data more directly and efficiently (such as Infiniband, GPU RDMA, NVLINK, etc.). All data is considered local to some processor, and may only be operated on by another processor by first doing an explicit move operation to that processor.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"A move between a given pair of processors is implemented as a Julia function dispatching on the types of each processor, as well as the type of the data being moved. Users are permitted to define custom move functions to improve data movement efficiency, perform automatic value conversions, or even make use of special IPC facilities. Custom processors may also be defined by the user to represent a processor type which is not automatically detected by Dagger, such as novel GPUs, special OS process abstractions, FPGAs, etc.","category":"page"},{"location":"processors/#Future:-Network-Devices-and-Topology","page":"Processors","title":"Future: Network Devices and Topology","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"In the future, users will be able to define network devices attached to a given processor, which provides a direct connection to a network device on another processor, and may be used to transfer data between said processors. Data movement rules will most likely be defined by a similar (or even identical) mechanism to the current processor move mechanism. The multi-root tree will be expanded to a graph to allow representing these network devices (as they may potentially span non-root nodes).","category":"page"},{"location":"processors/#Redundancy","page":"Processors","title":"Redundancy","text":"","category":"section"},{"location":"processors/#Fault-Tolerance","page":"Processors","title":"Fault Tolerance","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger has a single means for ensuring redundancy, which is currently called \"fault tolerance\". Said redundancy is only targeted at a specific failure mode, namely the unexpected exit or \"killing\" of a worker process in the cluster. This failure mode often presents itself when running on a Linux and generating large memory allocations, where the Out Of Memory (OOM) killer process can kill user processes to free their allocated memory for the Linux kernel to use. The fault tolerance system mitigates the damage caused by the OOM killer performing its duties on one or more worker processes by detecting the fault as a process exit exception (generated by Julia), and then moving any \"lost\" work to other worker processes for re-computation.","category":"page"},{"location":"processors/#Future:-Multi-master,-Network-Failure-Correction,-etc.","page":"Processors","title":"Future: Multi-master, Network Failure Correction, etc.","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"This single redundancy mechanism helps alleviate a common issue among HPC and scientific users, however it does little to help when, for example, the master node exits, or a network link goes down. Such failure modes require a more complicated detection and recovery process, including multiple master processes, a distributed and replicated database such as etcd, and checkpointing of the scheduler to ensure an efficient recovery. Such a system does not yet exist, but contributions for such a change are desired.","category":"page"},{"location":"processors/#Dynamic-worker-pools","page":"Processors","title":"Dynamic worker pools","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger's default scheduler supports modifying the worker pool while the scheduler is running. This is done by modifying the Processors of the Context supplied to the scheduler at initialization using addprocs!(ctx, ps) and rmprocs(ctx, ps) where ps can be Processors or just process ids.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"An example of when this is useful is in HPC environments where individual jobs to start up workers are queued so that not all workers are guaranteed to be available at the same time.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"New workers will typically be assigned new tasks as soon as the scheduler sees them. Removed workers will finish all their assigned tasks but will not be assigned any new tasks. Note that this makes it difficult to determine when a worker is no longer in use by Dagger. Contributions to alleviate this uncertainty are welcome!","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Example:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"using Distributed\n\nps1 = addprocs(2, exeflags=\"--project\")\n@everywhere using Distributed, Dagger\n\n# Dummy task to wait for 0.5 seconds and then return the id of the worker\nts = delayed(vcat)((delayed(i -> (sleep(0.5); myid()))(i) for i in 1:20)...)\n\nctx = Context()\n# Scheduler is blocking, so we need a new task to add workers while it runs\njob = @async collect(ctx, ts)\n\n# Lets fire up some new workers\nps2 = addprocs(2, exeflags=\"--project\")\n@everywhere ps2 using Distributed, Dagger\n# New workers are not available until we do this\naddprocs!(ctx, ps2)\n\n# Lets hope the job didn't complete before workers were added :)\n@show fetch(job) |> unique\n\n# and cleanup after ourselves...\nworkers() |> rmprocs","category":"page"},{"location":"scheduler-internals/#Scheduler-Internals","page":"Scheduler Internals","title":"Scheduler Internals","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The scheduler is called Dagger.Sch. It contains a single internal instance of type ComputeState, which maintains all necessary state to represent the set of waiting, ready, and completed (or \"finished\") graph nodes, cached Chunks, and maps of interdependencies between nodes. It uses Julia's task infrastructure to asynchronously send work requests to remote compute processes, and uses a Julia Channel as an inbound queue for completed work. There is an outer loop which drives the scheduler, which continues executing until all nodes in the graph have completed executing and the final result of the graph is ready to be returned to the user. This outer loop continuously performs two main operations: the first is to launch the execution of nodes which have become \"ready\" to execute; the second is to \"finish\" nodes which have been completed.","category":"page"},{"location":"scheduler-internals/#Scheduler-Initialization","page":"Scheduler Internals","title":"Scheduler Initialization","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"At the very beginning of a scheduler's lifecycle, the ComputeState is elaborated based on the computed sets of dependencies between nodes, and all nodes are placed in a \"waiting\" state. If any of the nodes are found to only have inputs which are not Thunks, then they are moved from \"waiting\" to \"ready\". The set of available \"workers\" (the set of available compute processes located throughout the cluster) is recorded, of size Nworkers.","category":"page"},{"location":"scheduler-internals/#Scheduler-Outer-Loop","page":"Scheduler Internals","title":"Scheduler Outer Loop","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"At each outer loop iteration, up to Nworkers processes that are currently in the \"ready\" state will be moved into the \"running\" state, and asynchronously sent (along with input arguments) to one of the Nworkers processes for execution. Subsequently, if any nodes exist in the inbound queue (i.e. the nodes have completed execution and their result is stored on the process that executed the node), then the most recently-queued node is removed from the queue, \"finished\", and placed in the \"finished\" state.","category":"page"},{"location":"scheduler-internals/#Node-Execution","page":"Scheduler Internals","title":"Node Execution","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Executing a node (here called Ne) in the \"ready\" state comprises two tasks. The first task is to identify which node in the set of \"ready\" nodes will be Ne (the node to execute). This choice is based on a concept known as \"affinity\", which is a cost-based metric used to evaluate the suitability of executing a given node on a given process. The metric is based primarily on the location of the input arguments to the node, as well as the arguments computed size in bytes. A fixed amount of affinity is added for each argument when the process in question houses that argument. Affinity is then added based on some base affinity value multiplied by the argument's size in bytes. The total affinities for each node are then used to pick the most optimal node to execute (typically, the one with the highest affinity).","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The second task is to prepare and send the node to a process for execution. If the node has been executed in the past (due to it being an argument to multiple other nodes), then the node is finished, and its result is pulled from the cache. If the node has not yet been executed, it is first checked if it is a \"meta\" node. A \"meta\" node is explicitly designated as such by the user or library, and will execute directly on its inputs as chunks (the data contained in the chunks are not immediately retrieved from the processors they reside on). Such a node will be executed directly within the scheduler, under the assumption that such a node is not expensive to execute. If the node is not a \"meta\" node, the executing worker process chooses (in round-robin fashion) a suitable processor to execute to execute the node on, based on the node's function, the input argument types, and user-defined rules for processor selection. The input arguments are then asynchronously transferred (via processor move operation) to the selected processor, and the appropriate call to the processor is made with the function and input arguments. Once execution completes and a result is obtained, it is wrapped as a Chunk, and the Chunk's handle is returned to the scheduler's inbound queue for node finishing.","category":"page"},{"location":"scheduler-internals/#Node-Finishing","page":"Scheduler Internals","title":"Node Finishing","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"\"Finishing\" a node (here called Nf) performs three main tasks. The first task is to find all of the downstream \"children\" nodes of Nf (the set of nodes which use Nf's result as one of their input arguments) that have had all of their input arguments computed and are in the \"waiting\" state, and move them into the \"ready\" state. The second task is to check all of the inputs to Nf to determine if any of them no longer have children nodes which have not been finished; if such inputs match this pattern, their cached result may be freed by the scheduler to minimize data usage. The third task is to mark Nf as \"finished\", and also to indicate to the scheduler whether another node has become \"ready\" to execute.","category":"page"},{"location":"logging/#Logging-and-Graphing","page":"Logging and Graphing","title":"Logging and Graphing","text":"","category":"section"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Dagger's scheduler keeps track of the important and potentially expensive actions it does, such as moving data between workers or executing thunks, and tracks how much time and memory allocations these operations consume. Saving this information somewhere accessible is disabled by default, but it's quite easy to turn it on:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"ctx = Context()\nlog = Dagger.LocalEventLog()\nctx.log_sink = log","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Now anytime ctx is used as the context for a scheduler, the scheduler will log events into log. A LocalEventLog logs information in-memory, and does so on each worker. The default log object is a NoOpLog, which doesn't store events at all. The FilterLog exists to allow writing events to a user-defined location (such as a database, file, or network socket).","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Once sufficient data has been accumulated into a LocalEventLog, it can be gathered to a single host via Dagger.get_logs!(log). The result is a Vector of Dagger.Timespan objects, which describe some metadata about an operation that occured and the scheduler logged. These events may be introspected directly, or may also be rendered to a DOT-format string:","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"logs = Dagger.get_logs!(log)\nstr = Dagger.show_plan(logs)","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"Dagger.show_plan can also be called as Dagger.show_plan(io::IO, logs) to write the graph to a file or other IO object. The string generated by this function may be passed to an external tool like Graphviz for rendering. Note that this method doesn't display input arguments to the DAG (non-Thunks); you can call Dagger.show_plan(logs, thunk), where thunk is the output Thunk of the DAG, to render argument nodes.","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"note: Note\nDagger.get_logs! clears out the event logs, so that old events don't mix with new ones from future DAGs.","category":"page"},{"location":"logging/","page":"Logging and Graphing","title":"Logging and Graphing","text":"As a convenience, it's possible to set ctx.log_file to the path to an output file, and then calls to compute(ctx, ...)/collect(ctx, ...) will automatically write the graph in DOT format to that path. There is also a benefit to this approach over manual calls to get_logs! and show_plan: DAGs which aren't Thunks (such as operations on the Dagger.DArray) will be properly rendered with input arguments (which normally aren't rendered because a Thunk is dynamically generated from such operations by Dagger before scheduling).","category":"page"},{"location":"#A-framework-for-out-of-core-and-parallel-execution","page":"Home","title":"A framework for out-of-core and parallel execution","text":"","category":"section"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The main function for using Dagger is delayed","category":"page"},{"location":"","page":"Home","title":"Home","text":"delayed(f; options...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"It returns a function which when called creates a Thunk object representing a call to function f with the given arguments. If it is called with other thunks as input, then they form a graph with input nodes directed at the output. The function f gets the result of the input Thunks. Thunks don't pass keyword argument to the function f. Options kwargs... to delayed are passed to the scheduler to control its behavior:","category":"page"},{"location":"","page":"Home","title":"Home","text":"get_result::Bool – return the actual result to the scheduler instead of Chunk objects. Used when f explicitly constructs a Chunk or when return value is small (e.g. in case of reduce)\nmeta::Bool – pass the input “Chunk” objects themselves to f and not the value contained in them - this is always run on the master process\npersist::Bool – the result of this Thunk should not be released after it becomes unused in the DAG\ncache::Bool – cache the result of this Thunk such that if the thunk is evaluated again, one can just reuse the cached value. If it’s been removed from cache, recompute the value.","category":"page"},{"location":"#DAG-creation-interface","page":"Home","title":"DAG creation interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here is a very simple example DAG:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Dagger\n\nadd1(value) = value + 1\nadd2(value) = value + 2\ncombine(a...) = sum(a)\n\np = delayed(add1)(4)\nq = delayed(add2)(p)\nr = delayed(add1)(3)\ns = delayed(combine)(p, q, r)\n\n@assert collect(s) == 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above computation can also be written in a more Julia-idiomatic syntax with @par:","category":"page"},{"location":"","page":"Home","title":"Home","text":"p = @par add1(4)\nq = @par add2(p)\nr = @par add1(3)\ns = @par combine(p, q, r)\n\n@assert collect(s) == 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"or similarly:","category":"page"},{"location":"","page":"Home","title":"Home","text":"s = @par begin\n    p = add1(4)\n    q = add2(p)\n    r = add1(3)\n    combine(p, q, r)\nend\n\n@assert collect(s) == 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"The connections between nodes p, q, r and s is represented by this dependency graph:","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: graph)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The final result is the obvious consequence of the operation","category":"page"},{"location":"","page":"Home","title":"Home","text":"add1(4) + add2(add1(4)) + add1(3)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(4 + 1) + ((4 + 1) + 2) + (3 + 1) = 16","category":"page"},{"location":"","page":"Home","title":"Home","text":"To compute and fetch the result of a thunk (say s), you can call collect(s). collect will fetch the result of the computation to the master process. Alternatively, if you want to compute but not fetch the result you can call compute on the thunk. This will return a Chunk object which references the result. If you pass in a Chunk objects as an input to a delayed function, then the function will get executed with the value of the Chunk – this evaluation will likely happen where the input chunks are, to reduce communication.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The key point is that, for each argument to a node, if the argument is a Thunk, it'll be executed before this node and its result will be passed into the function f provided. If the argument is not a Thunk (just some regular Julia object), it'll be passed as-is to the function f.","category":"page"},{"location":"#Polytree","page":"Home","title":"Polytree","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Polytrees are easily supported by Dagger. To make this work, pass all the head nodes Thunks into a call to delayed as arguments, which will act as the top node for the graph.","category":"page"},{"location":"","page":"Home","title":"Home","text":"group(x...) = [x...]\ntop_node = delayed(group)(head_nodes...)\ncompute(top_node)","category":"page"},{"location":"#Scheduler-and-Thunk-options","page":"Home","title":"Scheduler and Thunk options","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"While Dagger generally \"just works\", sometimes one needs to exert some more fine-grained control over how the scheduler allocates work. There are two parallel mechanisms to achieve this: Scheduler options (from Dagger.Sch.SchedulerOptions) and Thunk options (from Dagger.Sch.ThunkOptions). These two options structs generally contain the same options, with the difference being that Scheduler options operate globally across an entire DAG, and Thunk options operate on a thunk-by-thunk basis. Scheduler options can be constructed and passed to collect() or compute() as the keyword argument options, and Thunk options can be passed to Dagger's delayed function similarly: delayed(myfunc)(arg1, arg2, ...; options=opts). Check the docstring for the two options structs to see what options are available.","category":"page"},{"location":"#Processors-and-Resource-control","page":"Home","title":"Processors and Resource control","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"By default, Dagger uses the CPU to process work, typically single-threaded per cluster node. However, Dagger allows access to a wider range of hardware and software acceleration techniques, such as multithreading and GPUs. These more advanced (but performant) accelerators are disabled by default, but can easily be enabled by using Scheduler/Thunk options in the proctypes field. If non-empty, only the processor types contained in options.proctypes will be used to compute all or a given thunk.","category":"page"},{"location":"#GPU-Processors","page":"Home","title":"GPU Processors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The DaggerGPU.jl package can be imported to enable GPU acceleration for NVIDIA and AMD GPUs, when available. The processors provided by that package are not enabled by default, but may be enabled via options.proctypes as usual.","category":"page"},{"location":"#Rough-high-level-description-of-scheduling","page":"Home","title":"Rough high level description of scheduling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First picks the leaf Thunks and distributes them to available workers. Each worker is given at most 1 task at a time. If input to the node is a Chunk, then workers which already have the chunk are preferred.\nWhen a worker finishes a thunk it will return a Chunk object to the scheduler.\nOnce the worker has returned a Chunk, the scheduler picks the next task for the worker – this is usually the task the worker immediately made available (if possible). In the small example above, if worker 2 finished p it will be given q since it will already have the result of p which is input to q.\nThe scheduler also issues \"release\" Commands to chunks that are no longer required by nodes in the DAG: for example, when s is computed all of p, q, r are released to free up memory. This can be prevented by passing persist or cache options to delayed.","category":"page"},{"location":"#Modeling-of-Dagger-programs","page":"Home","title":"Modeling of Dagger programs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The key API for parallel and heterogeneous execution is Dagger.delayed. The call signature of Dagger.delayed is the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"thunk = Dagger.delayed(func)(args...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This invocation serves to construct a single node in a computational graph. func is a Julia function, which normally takes some number of arguments, of length N and of types Targs. The set of arguments args... is specified with ellipses to indicate that many arguments may be passed between the parentheses. When correctly invoked, args... is of length N and of types Targs (or suitable subtypes of Targs, for each respective argument in args...).  thunk is an instance of a Dagger Thunk, which is the value used internally by Dagger to represent a node in the graph.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Thunk may be \"computed\":","category":"page"},{"location":"","page":"Home","title":"Home","text":"chunk = Dagger.compute(thunk)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Computing a Thunk performs roughly the same logic as the following Julia function invocation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = func(args...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Such an invocation invokes func on args..., returning result. Computing the above thunk would produce a value with the same type as result, with the caveat that the result will be wrapped by a Dagger.Chunk (chunk in the above example). A Chunk is a reference to a value stored on a compute process within the Distributed cluster that Dagger is operating within. A Chunk may be \"collected\", which will return the wrapped value to the collecting process, which in the above example will be result:","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = collect(chunk)","category":"page"},{"location":"","page":"Home","title":"Home","text":"In order to create a graph with more than a single node, arguments to delayed may themselves be Thunks or Chunks. For example, the sum of the elements of vector [1,2,3,4] may be represented in Dagger as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"thunk1 = Dagger.delayed(+)(1, 2)\nthunk2 = Dagger.delayed(+)(3, 4)\nthunk3 = Dagger.delayed(+)(thunk1, thunk2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A graph has now been constructed, where thunk1 and thunk2 are dependencies (\"inputs\") to thunk3. Computing thunk3 and then collecting its resulting Chunk would result in the answer that is expected from the operation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"chunk = compute(thunk3)\nresult = collect(chunk)","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> result == 10\ntrue","category":"page"},{"location":"","page":"Home","title":"Home","text":"result now has the Int64 value 10, which is the result of summing the elements of the vector [1,2,3,4]. For convenience, computation may be performed together with collection, like so:","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = collect(thunk3)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above summation example is equivalent to the following invocation in plain Julia:","category":"page"},{"location":"","page":"Home","title":"Home","text":"x1 = 1 + 2\nx2 = 3 + 4\nresult = x1 + x2\nresult == 10","category":"page"},{"location":"","page":"Home","title":"Home","text":"However, there are key differences when using Dagger to perform this operation as compared to performing this operation without Dagger. In Dagger, the graph is constructed separately from computing the graph (\"lazily\"), whereas without Dagger the graph is executed immediately (\"eagerly\"). Dagger makes use of this lazy construction approach to allow modifying the actual execution of the overall operation in useful ways.","category":"page"},{"location":"","page":"Home","title":"Home","text":"By default, computing a Dagger graph creates an instance of a scheduler, which will be provided the graph to execute. The scheduler executes the individual nodes of the graph on their arguments in the order specified by the graph (ensuring dependencies to a node are satisfied before executing said node) on compute processes in the cluster; the scheduler process itself typically does not execute the nodes directly. Additionally, if a given set of nodes do not depend on each other (the value generated by a node is not an input to another node in the set), then those nodes may be executed in parallel, and the scheduler attempts to schedule such nodes in parallel when possible.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The scheduler also orchestrates data movement between compute processes, such that inputs to a given node are available on the compute process that is scheduled to execute said node. The scheduler attempts to minimize data movement between compute processes; it does so by trying to schedule nodes which depend on a given input on the same compute process that computed and retains that input.","category":"page"}]
}
