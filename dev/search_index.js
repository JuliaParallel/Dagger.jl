var documenterSearchIndex = {"docs":
[{"location":"dynamic/#Dynamic-Scheduler-Control","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"","category":"section"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Normally, Dagger executes static graphs defined with delayed and @par. However, it is possible for thunks to dynamically modify the graph at runtime, and to generally exert direct control over the scheduler's internal state. The Dagger.sch_handle function provides this functionality within a thunk:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"function mythunk(x)\n    h = Dagger.sch_handle()\n    Dagger.halt!(h)\n    return x\nend","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"The above example prematurely halts a running scheduler at the next opportunity using Dagger.halt!:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Dagger.halt!","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"There are a variety of other built-in functions available for various uses:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Dagger.get_dag_ids Dagger.add_thunk!","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"When working with thunks acquired from get_dag_ids or add_thunk!, you will have ThunkID objects which refer to a thunk by ID. Scheduler control functions which work with thunks accept or return ThunkIDs. For example, one can create a new thunk and get its result with Base.fetch:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"function mythunk(x)\n    h = Dagger.sch_handle()\n    id = Dagger.add_thunk!(h, x) do y\n        y + 1\n    end\n    return fetch(h, id)\nend","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Alternatively, Base.wait can be used when one does not wish to retrieve the returned value of the thunk.","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Users with needs not covered by the built-in functions should use the Dagger.exec! function to pass a user-defined function, closure, or callable struct to the scheduler, along with a payload which will be provided to that function:","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Dagger.exec!","category":"page"},{"location":"dynamic/","page":"Dynamic Scheduler Control","title":"Dynamic Scheduler Control","text":"Note that all functions called by Dagger.exec! take the scheduler's internal lock, so it's safe to manipulate the internal ComputeState object within the user-provided function.","category":"page"},{"location":"logging-visualization/#Logs-Visualization","page":"Logging: Visualization","title":"Logs Visualization","text":"","category":"section"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"To make Dagger's logging facilities useful without having to write custom code, Dagger has built-in and easily accessible logs visualization capabilities. Currently, there are two general mechanisms to visualize logs: show_logs/render_logs, and MultiEventLog consumers.","category":"page"},{"location":"logging-visualization/#Logs-visualization-with-show*logs/render*logs","page":"Logging: Visualization","title":"Logs visualization with showlogs/renderlogs","text":"","category":"section"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"The former (show_logs/render_logs) renders a logs Dict (acquired from fetch_logs!) either to an IO (via show_logs) or by returning a renderable object (via render_logs). This system is designed for rendering a single snapshot of logs into one or a few renderable objects, and is easily extensible by libraries or directly by the user, using multiple dispatch on show_logs(io::IO, logs::Dict, ::Val{mode}) and render_logs(logs::Dict, ::Val{mode}), where mode is a unique Symbol identifying the rendering mode to use. From the user's perspective, show_logs and render_logs take not a Val but a raw Symbol, which will be internally converted to a Val for dispatch purposes (i.e. render_logs(logs::Dict, :myrenderer) -> render_logs(logs, Val{:myrenderer}())).","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"Built-in IO support exists for:","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"show_logs(io, logs, :graphviz) to write a Graphviz dot graph of executed tasks and their dependencies (requires GraphViz.jl to be loaded)\nshow_logs(io, logs, :chrome_trace) to write a task execution timeline in the chrome-trace format (view in perfetto web UI or about:tracing in a chrome-based browser) (requires JSON3.jl to be loaded)","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"Built-in rendering support exists for:","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"render_logs(logs, :graphviz) to generate a graph diagram of executed tasks and their dependencies (requires GraphViz.jl to be loaded)\nrender_logs(logs, :plots_gantt) to generate a Gantt chart of task execution across all processors (requires Plots.jl and DataFrames.jl to be loaded)","category":"page"},{"location":"logging-visualization/#Continuous-visualization-with-MultiEventLog","page":"Logging: Visualization","title":"Continuous visualization with MultiEventLog","text":"","category":"section"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"The MultiEventLog mechanism is designed for continuous rendering of logs as they are generated, which permits real-time visualization of Dagger's operations. This logic is utilized in DaggerWebDash, which provides a web-based dashboard for visualizing Dagger's operations as a real-time Gantt chart and set of plots for various system metrics (CPU usage, memory usage, worker utilization, etc.).","category":"page"},{"location":"logging-visualization/#Visualization-with-DaggerWebDash","page":"Logging: Visualization","title":"Visualization with DaggerWebDash","text":"","category":"section"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"When working with Dagger, especially when working with its scheduler, it can be helpful to visualize what Dagger is doing internally in near-real-time. To assist with this, a web dashboard is available in the DaggerWebDash.jl package. This web dashboard uses a web server running within each Dagger worker, along with event logging information, to expose details about the scheduler. Information like worker and processor saturation, memory allocations, profiling traces, and much more are available in easy-to-interpret plots.","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"Using the dashboard is relatively simple and straightforward; if you run Dagger's benchmarking script, it's enabled for you automatically if the BENCHMARK_RENDER environment variable is set to webdash. This is the easiest way to get started with the web dashboard for new users.","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"For manual usage, the following snippet of code will suffice:","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"using Dagger, DaggerWebDash, TimespanLogging\n\nctx = Context() # or `ctx = Dagger.Sch.eager_context()` for eager API usage\nml = TimespanLogging.MultiEventLog()\n\n## Add some logging events of interest\n\nml[:core] = TimespanLogging.Events.CoreMetrics()\nml[:id] = TimespanLogging.Events.IDMetrics()\nml[:timeline] = TimespanLogging.Events.TimelineMetrics()\n# ...\n\n# (Optional) Enable profile flamegraph generation with ProfileSVG\nml[:profile] = DaggerWebDash.ProfileMetrics()\nctx.profile = true\n\n# Create a LogWindow; necessary for real-time event updates\nlw = TimespanLogging.Events.LogWindow(20*10^9, :core)\nml.aggregators[:logwindow] = lw\n\n# Create the D3Renderer server on port 8080\nd3r = DaggerWebDash.D3Renderer(8080)\n\n## Add some plots! Rendered top-down in order\n\n# Show an overview of all generated events as a Gantt chart\npush!(d3r, DaggerWebDash.GanttPlot(:core, :id, :esat, :psat; title=\"Overview\"))\n\n# Show various numerical events as line plots over time\npush!(d3r, DaggerWebDash.LinePlot(:core, :wsat, \"Worker Saturation\", \"Running Tasks\"))\npush!(d3r, DaggerWebDash.LinePlot(:core, :loadavg, \"CPU Load Average\", \"Average Running Threads\"))\npush!(d3r, DaggerWebDash.LinePlot(:core, :bytes, \"Allocated Bytes\", \"Bytes\"))\npush!(d3r, DaggerWebDash.LinePlot(:core, :mem, \"Available Memory\", \"% Free\"))\n\n# Show a graph rendering of compute tasks and data movement between them\n# Note: Profile events are ignored if absent from the log\npush!(d3r, DaggerWebDash.GraphPlot(:core, :id, :timeline, :profile, \"DAG\"))\n\n# TODO: Not yet functional\n#push!(d3r, DaggerWebDash.ProfileViewer(:core, :profile, \"Profile Viewer\"))\n\n# Add the D3Renderer as a consumer of special events generated by LogWindow\npush!(lw.creation_handlers, d3r)\npush!(lw.deletion_handlers, d3r)\n\n# D3Renderer is also an aggregator\nml.aggregators[:d3r] = d3r\n\nctx.log_sink = ml\n# ... use `ctx`","category":"page"},{"location":"logging-visualization/","page":"Logging: Visualization","title":"Logging: Visualization","text":"Once the server has started, you can browse to http://localhost:8080/ (if running on your local machine) to view the plots in real time. The dashboard also provides options at the top of the page to control the drawing speed, enable and disable reading updates from the server (disabling freezes the display at the current instant), and a selector for which worker to look at. If the connection to the server is lost for any reason, the dashboard will attempt to reconnect at 5 second intervals. The dashboard can usually survive restarts of the server perfectly well, although refreshing the page is usually a good idea. Informational messages are also logged to the browser console for debugging.","category":"page"},{"location":"scheduler-internals/#Scheduler-Internals","page":"Scheduler Internals","title":"Scheduler Internals","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Dagger's scheduler can be found primarily in the Dagger.Sch module. It performs a variety of functions to support tasks and data, and as such is a complex system. This documentation attempts to shed light on how the scheduler works internally (from a somewhat high level), with the hope that it will help users and contributors understand how to improve the scheduler or fix any bugs that may arise from it.","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"warn: Warn\nDagger's scheduler is evolving at a rapid pace, and is a complex mix of interacting parts. As such, this documentation may become out of date very quickly, and may not reflect the current state of the scheduler. Please feel free to file PRs to correct or improve this document, but also beware that the true functionality is defined in Dagger's source!","category":"page"},{"location":"scheduler-internals/#Core-vs.-Worker-Schedulers","page":"Scheduler Internals","title":"Core vs. Worker Schedulers","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Dagger's scheduler is really two kinds of entities: the \"core\" scheduler, and \"worker\" schedulers:","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The core scheduler runs on worker 1, thread 1, and is the entrypoint to tasks which have been submitted. The core scheduler manages all task dependencies, notifies calls to wait and fetch of task completion, and generally performs initial task placement. The core scheduler has cached information about each worker and their processors, and uses that information (together with metrics about previous tasks and other aspects of the Dagger runtime) to generate a near-optimal just-in-time task schedule.","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The worker schedulers each run as a set of tasks across all workers and all processors, and handles data movement and task execution. Once the core scheduler has scheduled and launched a task, it arrives at the worker scheduler for handling. The worker scheduler will pass the task to a queue for the assigned processor, where it will wait until the processor has a sufficient amount of \"occupancy\" for the task. Once the processor is ready for the task, it will first fetch all of the task's arguments from other workers, and then it will execute the task, package the task's result into a Chunk, and pass that back to the core scheduler.","category":"page"},{"location":"scheduler-internals/#Core:-Basics","page":"Scheduler Internals","title":"Core: Basics","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The core scheduler contains a single internal instance of type ComputeState, which maintains (among many other things) all necessary state to represent the set of waiting, ready, and running tasks, cached task results, and maps of interdependencies between tasks. It uses Julia's task infrastructure to asynchronously send work requests to remote Julia processes, and uses a RemoteChannel as an inbound queue for completed work.","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"There is an outer loop which drives the scheduler, which continues executing either eternally (excepting any internal scheduler errors or Julia exiting), or until all tasks in the graph have completed executing and the final task in the graph is ready to be returned to the user. This outer loop continuously performs two main operations: the first is to launch the execution of nodes which have become \"ready\" to execute; the second is to \"finish\" nodes which have been completed.","category":"page"},{"location":"scheduler-internals/#Core:-Initialization","page":"Scheduler Internals","title":"Core: Initialization","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"At the very beginning of a scheduler's lifecycle, a ComputeState object is allocated, workers are asynchronously initialized, and the outer loop is started. Additionally, the scheduler is passed one or more tasks to start scheduling, and so it will also fill out the ComputeState with the computed sets of dependencies between tasks, initially placing all tasks are placed in the \"waiting\" state. If any of the tasks are found to only have non-task input arguments, then they are considered ready to execute and moved from the \"waiting\" state to \"ready\".","category":"page"},{"location":"scheduler-internals/#Core:-Outer-Loop","page":"Scheduler Internals","title":"Core: Outer Loop","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"At each outer loop iteration, all tasks in the \"ready\" state will be scheduled, moved into the \"running\" state, and asynchronously sent to the workers for execution (called \"firing\"). Once all tasks are either waiting or running, the scheduler may sleep until actions need to be performed","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"When fired tasks have completed executing, an entry will exist in the inbound queue signalling the task's result and other metadata. At this point, the most recently-queued task is removed from the queue, \"finished\", and placed in the \"finished\" state. Finishing usually unlocks downstream tasks from the waiting state and allows them to transition to the ready state.","category":"page"},{"location":"scheduler-internals/#Core:-Task-Scheduling","page":"Scheduler Internals","title":"Core: Task Scheduling","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Once one or more tasks are ready to be scheduled, the scheduler will begin assigning them to the processors within each available worker. This is a sequential operation consisting of:","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Selecting candidate processors based on the task's combined scope\nCalculating the cost to move needed data to each candidate processor\nAdding a \"wait time\" cost proportional to the estimated run time for all the tasks currently executing on each candidate processor\nSelecting the least costly candidate processor as the executor for this task","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"After these operations have been performed for each task, the tasks will be fired off to their appropriate worker for handling.","category":"page"},{"location":"scheduler-internals/#Worker:-Task-Execution","page":"Scheduler Internals","title":"Worker: Task Execution","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Once a worker receives one or more tasks to be executed, the tasks are immediately enqueued into the appropriate processor's queue, and the processors are notified that work is available to be executed. The processors will asynchronously look at their queues and pick the task with the lowest occupancy first; a task with zero occupancy will always be executed immediately, but most tasks have non-zero occupancy, and so will be executed in order of increasing occupancy (effectively prioritizing asynchronous tasks like I/O).","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Before a task begins executions, the processor will collect the task's arguments from other workers as needed, and convert them as needed to execute correctly according to the processor's semantics. This operation is called a \"move\".","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Once a task's arguments have been moved, the task's function will be called with the arguments, and assuming the task doesn't throw an error, the result will be wrapped in a Chunk object. This Chunk will then be sent back to the core scheduler along with information about which task generated it. If the task does throw an error, then the error is instead propagated to the core scheduler, along with a flag indicating that the task failed.","category":"page"},{"location":"scheduler-internals/#Worker:-Workload-Balancing","page":"Scheduler Internals","title":"Worker: Workload Balancing","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"In general, Dagger's core scheduler tries to balance workloads as much as possible across all the available processors, but it can fail to do so effectively when either its cached knowledge of each worker's status is outdated, or when its estimates about the task's behavior are inaccurate. To minimize the possibility of workload imbalance, the worker schedulers' processors will attempt to steal tasks from each other when they are under-occupied. Tasks will only be stolen if the task's scope is compatible with the processor attempting the steal, so tasks with wider scopes have better balancing potential.","category":"page"},{"location":"scheduler-internals/#Core:-Finishing","page":"Scheduler Internals","title":"Core: Finishing","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"Finishing a task which has completed executing is generally a simple set of operations:","category":"page"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"The task's result is registered in the ComputeState for any tasks or user code which will need it\nAny unneeded data is cleared from the scheduler (such as preserved Chunk arguments)\nDownstream dependencies will be moved from \"waiting\" to \"ready\" if this task was the last upstream dependency to them","category":"page"},{"location":"scheduler-internals/#Core:-Shutdown","page":"Scheduler Internals","title":"Core: Shutdown","text":"","category":"section"},{"location":"scheduler-internals/","page":"Scheduler Internals","title":"Scheduler Internals","text":"If the core scheduler needs to shutdown due to an error or Julia exiting, then all workers will be shutdown, and the scheduler will close any open channels. If shutdown was due to an error, then an error will be printed or thrown back to the caller.","category":"page"},{"location":"streaming/#Streaming","page":"Streaming Tasks","title":"Streaming","text":"","category":"section"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"Dagger tasks have a limited lifetime - they are created, execute, finish, and are eventually destroyed when they're no longer needed. Thus, if one wants to run the same kind of computations over and over, one might re-create a similar set of tasks for each unit of data that needs processing.","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"This might be fine for computations which take a long time to run (thus dwarfing the cost of task creation, which is quite small), or when working with a limited set of data, but this approach is not great for doing lots of small computations on a large (or endless) amount of data. For example, processing image frames from a webcam, reacting to messages from a message bus, reading samples from a software radio, etc. All of these tasks are better suited to a \"streaming\" model of data processing, where data is simply piped into a continuously-running task (or DAG of tasks) forever, or until the data runs out.","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"Thankfully, if you have a problem which is best modeled as a streaming system of tasks, Dagger has you covered! Building on its support for Task Queues, Dagger provides a means to convert an entire DAG of tasks into a streaming DAG, where data flows into and out of each task asynchronously, using the spawn_streaming function:","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"Dagger.spawn_streaming() do # enters a streaming region\n  vals = Dagger.@spawn rand()\n  print_vals = Dagger.@spawn println(vals)\nend # exits the streaming region, and starts the DAG running","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"In the above example, vals is a Dagger task which has been transformed to run in a streaming manner - instead of just calling rand() once and returning its result, it will re-run rand() endlessly, continuously producing new random values. In typical Dagger style, print_vals is a Dagger task which depends on vals, but in streaming form - it will continuously println the random values produced from vals. Both tasks will run forever, and will run efficiently, only doing the work necessary to generate, transfer, and consume values.","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"As the comments point out, spawn_streaming creates a streaming region, during which vals and print_vals are created and configured. Both tasks are halted until spawn_streaming returns, allowing large DAGs to be built all at once, without any task losing a single value. If desired, streaming regions can be connected, although some values might be lost while tasks are being connected:","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"vals = Dagger.spawn_streaming() do\n    Dagger.@spawn rand()\nend\n\n# Some values might be generated by `vals` but thrown away\n# before `print_vals` is fully setup and connected to it\n\nprint_vals = Dagger.spawn_streaming() do\n    Dagger.@spawn println(vals)\nend","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"More complicated streaming DAGs can be easily constructed, without doing anything different. For example, we can generate multiple streams of random numbers, write them all to their own files, and print the combined results:","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"Dagger.spawn_streaming() do\n    all_vals = [Dagger.spawn(rand) for i in 1:4]\n    all_vals_written = map(1:4) do i\n        Dagger.spawn(all_vals[i]) do val\n            open(\"results_$i.txt\"; write=true, create=true, append=true) do io\n                println(io, repr(val))\n            end\n            return val\n        end\n    end\n    Dagger.spawn(all_vals_written...) do all_vals_written...\n        vals_sum = sum(all_vals_written)\n        println(vals_sum)\n    end\nend","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"If you want to stop the streaming DAG and tear it all down, you can call Dagger.cancel!(all_vals[1]) (or with any other task in the streaming DAG) to terminate all streaming tasks.","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"Alternatively, tasks can stop themselves from the inside with finish_stream, optionally returning a value that can be fetch'd. Let's do this when our randomly-drawn number falls within some arbitrary range:","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"vals = Dagger.spawn_streaming() do\n    Dagger.spawn() do\n        x = rand()\n        if x < 0.001\n            # That's good enough, let's be done\n            return Dagger.finish_stream(\"Finished!\")\n        end\n        return x\n    end\nend\nfetch(vals)","category":"page"},{"location":"streaming/","page":"Streaming Tasks","title":"Streaming Tasks","text":"In this example, the call to fetch will hang (while random numbers continue to be drawn), until a drawn number is less than 0.001; at that point, fetch will return with \"Finished!\", and the task vals will have terminated.","category":"page"},{"location":"use-cases/parallel-nested-loops/#Use-Case:-Parallel-Nested-Loops","page":"Parallel Nested Loops","title":"Use Case: Parallel Nested Loops","text":"","category":"section"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"One of the many applications of Dagger is that it can be used as a drop-in replacement for nested multi-threaded loops that would otherwise be written with Threads.@threads.","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"Consider a simplified scenario where you want to calculate the maximum mean values of random samples of various lengths that have been generated by several distributions provided by the Distributions.jl package. The results should be collected into a DataFrame. We have the following function:","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"using Dagger, Random, Distributions, StatsBase, DataFrames\n\nfunction f(dist, len, reps, σ)\n    v = Vector{Float64}(undef, len) # avoiding allocations\n    maximum(mean(rand!(dist, v)) for _ in 1:reps)/σ\nend","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"Let us consider the following probability distributions for numerical experiments, all of which have expected values equal to zero, and the following lengths of vectors:","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"dists =  [Cosine, Epanechnikov, Laplace, Logistic, Normal, NormalCanon, PGeneralizedGaussian, SkewNormal, SkewedExponentialPower, SymTriangularDist]\nlens = [10, 20, 50, 100, 200, 500]","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"Using Threads.@threads those experiments could be parallelized as:","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"function experiments_threads(dists, lens, K=1000)\n    res = DataFrame()\n    lck = ReentrantLock()\n    Threads.@threads for T in dists\n        dist = T()\n        σ = std(dist)\n        for L in lens\n            z = f(dist, L, K, σ)\n            Threads.lock(lck) do\n                push!(res, (;T, σ, L, z))\n            end\n        end\n    end\n    res\nend","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"Note that DataFrames.push! is not a thread safe operation and hence we need to utilize a locking mechanism in order to avoid two threads appending the DataFrame at the same time.","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"The same code could be rewritten in Dagger as:","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"function experiments_dagger(dists, lens, K=1000)\n    res = DataFrame()\n    @sync for T in dists\n        dist = T()\n        σ = Dagger.@spawn std(dist)\n        for L in lens\n            z = Dagger.@spawn f(dist, L, K, σ)\n            push!(res, (;T, σ, L, z))\n        end\n    end\n    res.z = fetch.(res.z)\n    res.σ = fetch.(res.σ)\n    res\nend","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"In this code we have job interdependence. Firstly, we are calculating the standard deviation σ, and then we are using that value in the function f. Since Dagger.@spawn yields a DTask rather than actual values, we need to use the fetch function to obtain those values. In this example, the value fetching is performed once all computations are completed (note that @sync preceding the loop forces the loop to wait for all jobs to complete). Also, note that contrary to the previous example, we do not need to implement locking as we are just pushing the DTask results of Dagger.@spawn serially into the DataFrame (which is fast since Dagger.@spawn doesn't block).","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"The above use case scenario has been tested by running julia -t 8 (or with JULIA_NUM_THREADS=8 as environment variable). The Threads.@threads code takes 1.8 seconds to run, while the Dagger code, which is also one line shorter, runs around 0.9 seconds, resulting in a 2x speedup.","category":"page"},{"location":"use-cases/parallel-nested-loops/","page":"Parallel Nested Loops","title":"Parallel Nested Loops","text":"warning: Warning\nAnnotating an inner loop with @sync will block the outer loop from iterating until the inner @sync loop is fully completed, negating some potential parallelism. @sync should only be applied to the outermost loop before a fetch.","category":"page"},{"location":"propagation/#Option-Propagation","page":"Option Propagation","title":"Option Propagation","text":"","category":"section"},{"location":"propagation/","page":"Option Propagation","title":"Option Propagation","text":"Most options passed to Dagger are passed via @spawn/spawn or delayed directly. This works well when an option only needs to be set for a single thunk, but is cumbersome when the same option needs to be set on multiple thunks, or set recursively on thunks spawned within other thunks. Thankfully, Dagger provides the with_options function to make this easier. This function is very powerful, by nature of using \"context variables\"; let's first see some example code to help explain it:","category":"page"},{"location":"propagation/","page":"Option Propagation","title":"Option Propagation","text":"function f(x)\n    m = Dagger.@spawn myid()\n    return Dagger.@spawn x+m\nend\nDagger.with_options(;scope=ProcessScope(2)) do\n    @sync begin\n        @async @assert fetch(Dagger.@spawn f(1)) == 3\n        @async @assert fetch(Dagger.@spawn f(2)) == 4\n    end\nend","category":"page"},{"location":"propagation/","page":"Option Propagation","title":"Option Propagation","text":"In the above example, with_options sets the scope for both Dagger.@spawn f(1) and Dagger.@spawn f(2) to ProcessScope(2) (locking Dagger tasks to worker 2). This is of course very useful for ensuring that a set of operations use a certain scope. What it also does, however, is propagates this scope through calls to @async, Threads.@spawn, and Dagger.@spawn; this means that the task spawned by f(x) also inherits this scope! This works thanks to the magic of context variables, which are inherited recursively through child tasks, and thanks to Dagger intentionally propagating the scope (and other options passed to with_options) across the cluster, ensuring that no matter how deep the recursive task spawning goes, the options are maintained.","category":"page"},{"location":"propagation/","page":"Option Propagation","title":"Option Propagation","text":"It's also possible to retrieve the options currently set by with_options, using Dagger.get_options:","category":"page"},{"location":"propagation/","page":"Option Propagation","title":"Option Propagation","text":"Dagger.with_options(;scope=ProcessScope(2)) do\n    fetch(@async @assert Dagger.get_options().scope == ProcessScope(2))\n    # Or:\n    fetch(@async @assert Dagger.get_options(:scope) == ProcessScope(2))\n    # Or, if `scope` might not have been propagated as an option, we can give\n    # it a default value:\n    fetch(@async @assert Dagger.get_options(:scope, AnyScope()) == ProcessScope(2))\nend","category":"page"},{"location":"propagation/","page":"Option Propagation","title":"Option Propagation","text":"This is a very powerful concept: with a single call to with_options, we can apply any set of options to any nested set of operations. This is great for isolating large workloads to different workers or processors, defining global checkpoint/restore behavior, and more.","category":"page"},{"location":"checkpointing/#Checkpointing","page":"Checkpointing","title":"Checkpointing","text":"","category":"section"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"If at some point during a Dagger computation a thunk throws an error, or if the entire computation dies because the head node hit an OOM or other unexpected error, the entire computation is lost and needs to be started from scratch. This can be unacceptable for scheduling very large/expensive/mission-critical graphs, and for interactive development where errors are common and easily fixable.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Robust applications often support \"checkpointing\", where intermediate results are periodically written out to persistent media, or sharded to the rest of the cluster, to allow resuming an interrupted computation from a point later than the original start. Dagger provides infrastructure to perform user-driven checkpointing of intermediate results once they're generated.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"As a concrete example, imagine that you're developing a numerical algorithm, and distributing it with Dagger. The idea is to sum all the values in a very big matrix, and then get the square root of the absolute value of the sum of sums. Here is what that might look like:","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"X = compute(randn(Blocks(128,128), 1024, 1024))\nY = [delayed(sum)(chunk) for chunk in X.chunks]\ninner(x...) = sqrt(sum(x))\nZ = delayed(inner)(Y...)\nz = collect(Z)","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Let's pretend that the above calculation of each element in Y takes a full day to run. If you run this, you might realize that if the final sum call returns a negative number, sqrt will throw a DomainError (because sqrt can't accept negative Real inputs). Of course, you forgot to add a call to abs before the call to sqrt! Now, you know how to fix this, but once you do, you'll have to spend another entire day waiting for it to finish! And maybe you fix this one bug and wait a full day for it to finish, and begin adding more very computationally-heavy code (which inevitably has bugs). Those later computations might fail, and if you're running this as a script (maybe under a cluster scheduler like Slurm), you have to restart everything from the very beginning. This is starting to sound pretty wasteful...","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Thankfully, Dagger has a simple solution to this: checkpointing. With checkpointing, Dagger can be instructed to save intermediate results (maybe the results of computing Y) to a persistent storage medium of your choice. Probably a file on disk, but maybe a database, or even just stored in RAM in a space-efficient form. You also tell Dagger how to restore this data: how to take the result stored in its persistent form, and turn it back into something identical to the original intermediate data that Dagger computed. Then, when the worst happens and a piece of your algorithm throws an error (as above), Dagger will call the restore function and try to materialize those intermediate results that you painstakingly computed, so that you don't need to re-compute them.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Let's see how we'd modify the above example to use checkpointing:","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"using Serialization\n\nX = compute(randn(Blocks(128,128), 1024, 1024))\nY = [delayed(sum; checkpoint=(thunk,result)->begin\n    open(\"checkpoint-$idx.bin\", \"w\") do io\n        serialize(io, collect(result))\n    end\nend, restore=(thunk)->begin\n    open(\"checkpoint-$idx.bin\", \"r\") do io\n        Dagger.tochunk(deserialize(io))\n    end\nend)(chunk) for (idx,chunk) in enumerate(X.chunks)]\ninner(x...) = sqrt(sum(x))\nZ = delayed(inner)(Y...)\nz = collect(Z)","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Two changes were made: first, we enumerate(X.chunks) so that we can get a unique index to identify each chunk; second, we specify options to delayed with a checkpoint and restore function that is specialized to write or read the given chunk to or from a file on disk, respectively. Notice the usage of collect in the checkpoint function, and the use of Dagger.tochunk in the restore function; Dagger represents intermediate results as Dagger.Chunk objects, so we need to convert between Chunks and the actual data to keep Dagger happy. Performance-sensitive users might consider modifying these methods to store the checkpoint files on the filesystem of the server that currently owns the Chunk, to minimize data transfer times during checkpoint and restore operations.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"If we run the above code once, we'll still end up waiting a day for Y to be computed, and we'll still get the DomainError from sqrt. However, when we fix the inner function to include that call to abs that was missing, and we re-run this code starting from the creation of Y, we'll find that we don't actually spend a day waiting; we probably spend a few seconds waiting, and end up with our final result! This is because Dagger called the restore function for each element of Y, and was provided a result by the user-specified function, so it skipped re-computing those sums entirely.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"You might also notice that when you ran this code the first time, you received errors about \"No such file or directory\", or some similar error; this occurs because Dagger always calls the restore function when it exists. In the first run, the checkpoint files don't yet exist, so there's nothing to restore; Dagger reports the thrown error, but keeps moving along, merrily computing the sums of Y. You're welcome to explicitly check if the file exists, and if not, return nothing; then Dagger won't report an annoying error, and will skip the restoration quietly.","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"Of course, you might have a lot of code that looks like this, and may want to also checkpoint the final result of the z = collect(...) call as well. This is just as easy to do:","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"# compute X, Y, Z above ...\nz = collect(Z; options=Dagger.Sch.SchedulerOptions(;\ncheckpoint=(result)->begin\n    open(\"checkpoint-final.bin\", \"w\") do io\n        serialize(io, collect(result))\n    end\nend, restore=()->begin\n    open(\"checkpoint-final.bin\", \"r\") do io\n        Dagger.tochunk(deserialize(io))\n    end\nend))","category":"page"},{"location":"checkpointing/","page":"Checkpointing","title":"Checkpointing","text":"In this case, the entire computation will be skipped if checkpoint-final.bin exists!","category":"page"},{"location":"api-dagger/types/#Dagger-Types","page":"Types","title":"Dagger Types","text":"","category":"section"},{"location":"api-dagger/types/","page":"Types","title":"Types","text":"Pages = [\"types.md\"]","category":"page"},{"location":"api-dagger/types/#Task-Types","page":"Types","title":"Task Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.Thunk","page":"Types","title":"Dagger.Thunk","text":"Thunk\n\nWraps a callable object to be run with Dagger. A Thunk is typically created through a call to delayed or its macro equivalent @par.\n\nConstructors\n\ndelayed(f; kwargs...)(args...)\n@par [option=value]... f(args...)\n\nExamples\n\njulia> t = delayed(sin)(π)  # creates a Thunk to be computed later\nThunk(sin, (π,))\n\njulia> collect(t)  # computes the result and returns it to the current process\n1.2246467991473532e-16\n\nArguments\n\nfargs: The function and arguments to be called upon execution of the Thunk.\nkwargs: The properties describing unique behavior of this Thunk. Details\n\nfor each property are described in the next section.\n\noption=value: The same as passing kwargs to delayed.\n\nOptions\n\noptions: An Options struct providing the options for the Thunk.\n\nIf omitted, options can also be specified by passing key-value pairs as kwargs.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.DTask","page":"Types","title":"Dagger.DTask","text":"DTask\n\nReturned from Dagger.@spawn/Dagger.spawn calls. Represents a task that is in the scheduler, potentially ready to execute, executing, or finished executing. May be fetch'd or wait'd on at any time. See Dagger.@spawn for more details.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Task-Options-Types","page":"Types","title":"Task Options Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.Options","page":"Types","title":"Dagger.Options","text":"Options\n\nStores per-task options to be passed to the scheduler.\n\nArguments\n\npropagates::Vector{Symbol}: The set of option names that will be propagated by this task to tasks that it spawns.\nprocessor::Processor: The processor associated with this task's function. Generally ignored by the scheduler.\ncompute_scope::AbstractScope: The execution scope of the task, which determines where the task can be scheduled and executed. scope is another name for this option.\nresult_scope::AbstractScope: The data scope of the task's result, which determines where the task's result can be accessed from.\nsingle::Int=0: (Deprecated) Force task onto worker with specified id. 0 disables this option.\nproclist=nothing: (Deprecated) Force task to use one or more processors that are instances/subtypes of a contained type. Alternatively, a function can be supplied, and the function will be called with a processor as the sole argument and should return a Bool result to indicate whether or not to use the given processor. nothing enables all default processors.\nget_result::Bool=false: Whether the worker should store the result directly (true) or as a Chunk (false)\nmeta::Bool=false: When true, values are not moved, and are passed directly as Chunk, if they are not immediate values\nsyncdeps::Set{Any}: Contains any additional tasks to synchronize with\ntime_util::Dict{Type,Any}: Indicates the maximum expected time utilization for this task. Each keypair maps a processor type to the utilization, where the value can be a real (approximately the number of nanoseconds taken), or MaxUtilization() (utilizes all processors of this type). By default, the scheduler assumes that this task only uses one processor.\nalloc_util::Dict{Type,UInt64}: Indicates the maximum expected memory utilization for this task. Each keypair maps a processor type to the utilization, where the value is an integer representing approximately the maximum number of bytes allocated at any one time.\noccupancy::Dict{Type,Real}: Indicates the maximum expected processor occupancy for this task. Each keypair maps a processor type to the utilization, where the value can be a real between 0 and 1 (the occupancy ratio, where 1 is full occupancy). By default, the scheduler assumes that this task has full occupancy.\ncheckpoint=nothing: If not nothing, uses the provided function to save the result of the task to persistent storage, for later retrieval by restore.\nrestore=nothing: If not nothing, uses the provided function to return the (cached) result of this task, were it to execute.  If this returns a Chunk, this task will be skipped, and its result will be set to the Chunk.  If nothing is returned, restoring is skipped, and the task will execute as usual. If this function throws an error, restoring will be skipped, and the error will be displayed.\nstorage::Union{Chunk,Nothing}=nothing: If not nothing, references a MemPool.StorageDevice which will be passed to MemPool.poolset internally when constructing Chunks (such as when constructing the return value). The device must support MemPool.CPURAMResource. When nothing, uses MemPool.GLOBAL_DEVICE[].\nstorage_root_tag::Any=nothing: If not nothing, specifies the MemPool storage leaf tag to associate with the task's result. This tag can be used by MemPool's storage devices to manipulate their behavior, such as the file name used to store data on disk.\"\nstorage_leaf_tag::Union{MemPool.Tag,Nothing}=nothing: If not nothing, specifies the MemPool storage leaf tag to associate with the task's result. This tag can be used by MemPool's storage devices to manipulate their behavior, such as the file name used to store data on disk.\"\nstorage_retain::Union{Bool,Nothing}=nothing: The value of retain to pass to MemPool.poolset when constructing the result Chunk. nothing defaults to false.\nname::Union{String,Nothing}=nothing: If not nothing, annotates the task with a name for logging purposes.\nstream_input_buffer_amount::Union{Int,Nothing}=nothing: (Streaming only) Specifies the amount of slots to allocate for the input buffer of the task. Defaults to 1.\nstream_output_buffer_amount::Union{Int,Nothing}=nothing: (Streaming only) Specifies the amount of slots to allocate for the output buffer of the task. Defaults to 1.\nstream_buffer_type::Union{Type,Nothing}=nothing: (Streaming only) Specifies the type of buffer to use for the input and output buffers of the task. Defaults to Dagger.ProcessRingBuffer.\nstream_max_evals::Union{Int,Nothing}=nothing: (Streaming only) Specifies the maximum number of times the task will be evaluated before returning a result. Defaults to infinite evaluations.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.SchedulerOptions","page":"Types","title":"Dagger.SchedulerOptions","text":"SchedulerOptions\n\nStores DAG-global options to be passed to the Dagger.Sch scheduler.\n\nArguments\n\nsingle::Int=0: (Deprecated) Force all work onto worker with specified id. 0 disables this option.\nproclist=nothing: (Deprecated) Force scheduler to use one or more processors that are instances/subtypes of a contained type. Alternatively, a function can be supplied, and the function will be called with a processor as the sole argument and should return a Bool result to indicate whether or not to use the given processor. nothing enables all default processors.\nallow_errors::Bool=false: Allow thunks to error without affecting non-dependent thunks.\ncheckpoint=nothing: If not nothing, uses the provided function to save the final result of the current scheduler invocation to persistent storage, for later retrieval by restore.\nrestore=nothing: If not nothing, uses the provided function to return the (cached) final result of the current scheduler invocation, were it to execute. If this returns a Chunk, all thunks will be skipped, and the Chunk will be returned.  If nothing is returned, restoring is skipped, and the scheduler will execute as usual. If this function throws an error, restoring will be skipped, and the error will be displayed.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Data-Management-Types","page":"Types","title":"Data Management Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.Chunk","page":"Types","title":"Dagger.Chunk","text":"Chunk\n\nA reference to a piece of data located on a remote worker. Chunks are typically created with Dagger.tochunk(data), and the data can then be accessed from any worker with collect(::Chunk). Chunks are serialization-safe, and use distributed refcounting (provided by MemPool.DRef) to ensure that the data referenced by a Chunk won't be GC'd, as long as a reference exists on some worker.\n\nEach Chunk is associated with a given Dagger.Processor, which is (in a sense) the processor that \"owns\" or contains the data. Calling collect(::Chunk) will perform data movement and conversions defined by that processor to safely serialize the data to the calling worker.\n\nConstructors\n\nSee tochunk.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.Shard","page":"Types","title":"Dagger.Shard","text":"Maps a value to one of multiple distributed \"mirror\" values automatically when used as a thunk argument. Construct using @shard or shard.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Data-Dependencies-Types","page":"Types","title":"Data Dependencies Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.In","page":"Types","title":"Dagger.In","text":"Specifies a read-only dependency.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.Out","page":"Types","title":"Dagger.Out","text":"Specifies a write-only dependency.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.InOut","page":"Types","title":"Dagger.InOut","text":"Specifies a read-write dependency.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.Deps","page":"Types","title":"Dagger.Deps","text":"Specifies one or more dependencies.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Processor-Types","page":"Types","title":"Processor Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.Processor","page":"Types","title":"Dagger.Processor","text":"Processor\n\nAn abstract type representing a processing device and associated memory, where data can be stored and operated on. Subtypes should be immutable, and instances should compare equal if they represent the same logical processing device/memory. Subtype instances should be serializable between different nodes. Subtype instances may contain a \"parent\" Processor to make it easy to transfer data to/from other types of Processor at runtime.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.OSProc","page":"Types","title":"Dagger.OSProc","text":"OSProc <: Processor\n\nJulia CPU (OS) process, identified by Distributed pid. The logical parent of all processors on a given node, but otherwise does not participate in computations.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.ThreadProc","page":"Types","title":"Dagger.ThreadProc","text":"ThreadProc <: Processor\n\nJulia CPU (OS) thread, identified by Julia thread ID.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Scope-Types","page":"Types","title":"Scope Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.AnyScope","page":"Types","title":"Dagger.AnyScope","text":"Widest scope that contains all processors.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.NodeScope","page":"Types","title":"Dagger.NodeScope","text":"Scoped to the same physical node.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.ProcessScope","page":"Types","title":"Dagger.ProcessScope","text":"Scoped to the same OS process.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.ProcessorTypeScope","page":"Types","title":"Dagger.ProcessorTypeScope","text":"Scoped to any processor with a given supertype.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/types/#Dagger.TaintScope","page":"Types","title":"Dagger.TaintScope","text":"Taints a scope for later evaluation.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.UnionScope","page":"Types","title":"Dagger.UnionScope","text":"Union of two or more scopes.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.ExactScope","page":"Types","title":"Dagger.ExactScope","text":"Scoped to a specific processor.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Context-Types","page":"Types","title":"Context Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.Context","page":"Types","title":"Dagger.Context","text":"Context(xs::Vector{OSProc}) -> Context\nContext(xs::Vector{Int}) -> Context\n\nCreate a Context, by default adding each available worker.\n\nIt is also possible to create a Context from a vector of OSProc, or equivalently the underlying process ids can also be passed directly as a Vector{Int}.\n\nSpecial fields include:\n\n'log_sink': A log sink object to use, if any.\nprofile::Bool: Whether or not to perform profiling with Profile stdlib.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Array-Types","page":"Types","title":"Array Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.DArray","page":"Types","title":"Dagger.DArray","text":"DArray{T,N,F}(domain, subdomains, chunks, concat)\nDArray(T, domain, subdomains, chunks, [concat=cat])\n\nAn N-dimensional distributed array of element type T, with a concatenation function of type F.\n\nArguments\n\nT: element type\ndomain::ArrayDomain{N}: the whole ArrayDomain of the array\nsubdomains::AbstractArray{ArrayDomain{N}, N}: a DomainBlocks of the same dimensions as the array\nchunks::AbstractArray{Union{Chunk,Thunk}, N}: an array of chunks of dimension N\nconcat::F: a function of type F. concat(x, y; dims=d) takes two chunks x and y and concatenates them along dimension d. cat is used by default.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.Blocks","page":"Types","title":"Dagger.Blocks","text":"Blocks(xs...)\n\nIndicates the size of an array operation, specified as xs, whose length indicates the number of dimensions in the resulting array.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.ArrayDomain","page":"Types","title":"Dagger.ArrayDomain","text":"ArrayDomain{N}\n\nAn N-dimensional domain over an array.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.UnitDomain","page":"Types","title":"Dagger.UnitDomain","text":"UnitDomain\n\nDefault domain – has no information about the value\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Logging-Event-Types","page":"Types","title":"Logging Event Types","text":"","category":"section"},{"location":"api-dagger/types/#Dagger.Events.BytesAllocd","page":"Types","title":"Dagger.Events.BytesAllocd","text":"BytesAllocd\n\nTracks memory allocated for Chunks.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.Events.ProcessorSaturation","page":"Types","title":"Dagger.Events.ProcessorSaturation","text":"ProcessorSaturation\n\nTracks the compute saturation (running tasks) per-processor.\n\n\n\n\n\n","category":"type"},{"location":"api-dagger/types/#Dagger.Events.WorkerSaturation","page":"Types","title":"Dagger.Events.WorkerSaturation","text":"WorkerSaturation\n\nTracks the compute saturation (running tasks).\n\n\n\n\n\n","category":"type"},{"location":"task-spawning/#Task-Spawning","page":"Task Spawning","title":"Task Spawning","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"The main entrypoint to Dagger is @spawn:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Dagger.@spawn [option=value]... f(args...; kwargs...)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"or spawn if it's more convenient:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Dagger.spawn(f, Dagger.Options(options), args...; kwargs...)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"When called, it creates an DTask (also known as a \"task\" or \"thunk\") object representing a call to function f with the arguments args and keyword arguments kwargs. If it is called with other tasks as args/kwargs, such as in Dagger.@spawn f(Dagger.@spawn g()), then, in this example, the function f gets passed the results of executing g(), once that result is available. If g() isn't yet finished executing, then the execution of f waits on g() to complete before executing.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"An important observation to make is that, for each argument to @spawn/spawn, if the argument is the result of another @spawn/spawn call (thus it's an DTask), the argument will be computed first, and then its result will be passed into the function receiving the argument. If the argument is not an DTask (instead, some other type of Julia object), it'll be passed as-is to the function f (with some exceptions).","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"note: Task / thread occupancy\nBy default, Dagger assumes that tasks saturate the thread they are running on and does not try to schedule other tasks on the thread. This default can be controlled by specifying Options (more details can be found under Task and Scheduler options). The section Changing the thread occupancy shows a runnable example of how to achieve this.","category":"page"},{"location":"task-spawning/#Options","page":"Task Spawning","title":"Options","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"The Options struct in the second argument position is optional; if provided, it is passed to the scheduler to control its behavior. Options contains option key-value pairs, which can be any field in Options (see Task and Scheduler options).","category":"page"},{"location":"task-spawning/#Simple-example","page":"Task Spawning","title":"Simple example","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Let's see a very simple directed acyclic graph (or DAG) constructed with Dagger:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\n\nadd1(value) = value + 1\nadd2(value) = value + 2\ncombine(a...) = sum(a)\n\np = Dagger.@spawn add1(4)\nq = Dagger.@spawn add2(p)\nr = Dagger.@spawn add1(3)\ns = Dagger.@spawn combine(p, q, r)\n\n@assert fetch(s) == 16","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"The tasks p, q, r, and s have the following structure:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"(Image: graph)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"The final result (from fetch(s)) is the obvious consequence of the operation:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"add1(4) + add2(add1(4)) + add1(3)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"(4 + 1) + ((4 + 1) + 2) + (3 + 1) == 16","category":"page"},{"location":"task-spawning/#Eager-Execution","page":"Task Spawning","title":"Eager Execution","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Dagger's @spawn macro works similarly to @async and Threads.@spawn: when called, it wraps the function call specified by the user in an DTask object, and immediately places it onto a running scheduler, to be executed once its dependencies are fulfilled.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"x = rand(400,400)\ny = rand(400,400)\nzt = Dagger.@spawn x * y\nz = fetch(zt)\n@assert isapprox(z, x * y)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"One can also wait on the result of @spawn and check completion status with isready:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"x = Dagger.@spawn sleep(10)\n@assert !isready(x)\nwait(x)\n@assert isready(x)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Like @async and Threads.@spawn, Dagger.@spawn synchronizes with locally-scoped @sync blocks:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"function sleep_and_print(delay, str)\n    sleep(delay)\n    println(str)\nend\n@sync begin\n    Dagger.@spawn sleep_and_print(3, \"I print first\")\nend\nwait(Dagger.@spawn sleep_and_print(1, \"I print second\"))","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"One can also safely call @spawn from another worker (not ID 1), and it will be executed correctly:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"x = fetch(Distributed.@spawnat 2 Dagger.@spawn 1+2) # fetches the result of `@spawnat`\nx::DTask\n@assert fetch(x) == 3 # fetch the result of `@spawn`","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"This is useful for nested execution, where an @spawn'd task calls @spawn. This is detailed further in Dynamic Scheduler Control.","category":"page"},{"location":"task-spawning/#Options-2","page":"Task Spawning","title":"Options","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"The Options struct in the second argument position is optional; if provided, it is passed to the scheduler to control its behavior. Options contains a NamedTuple of option key-value pairs, which can be any of:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Any field in Options (see Task and Scheduler options)\nmeta::Bool – Pass the input Chunk objects themselves to f and not the value contained in them.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"There are also some extra options that can be passed, although they're considered advanced options to be used only by developers or library authors:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"get_result::Bool – return the actual result to the scheduler instead of Chunk objects. Used when f explicitly constructs a Chunk or when return value is small (e.g. in case of reduce)\npersist::Bool – the result of this Thunk should not be released after it becomes unused in the DAG\ncache::Bool – cache the result of this Thunk such that if the thunk is evaluated again, one can just reuse the cached value. If it’s been removed from cache, recompute the value.","category":"page"},{"location":"task-spawning/#Errors","page":"Task Spawning","title":"Errors","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"If a task errors while running under the eager scheduler, it will be marked as having failed, all dependent (downstream) tasks will be marked as failed, and any future tasks that use a failed task as input will fail. Failure can be determined with fetch, which will re-throw the error that the originally-failing task threw. wait and isready will not check whether a task or its upstream failed; they only check if the task has completed, error or not.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"This failure behavior is not the default for lazy scheduling (Lazy API), but can be enabled by setting the scheduler/task option (Task and Scheduler options) allow_error to true.  However, this option isn't terribly useful for non-dynamic usecases, since any task failure will propagate down to the output task regardless of where it occurs.","category":"page"},{"location":"task-spawning/#Cancellation","page":"Task Spawning","title":"Cancellation","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Sometimes a task runs longer than expected (maybe it's hanging due to a bug), or the user decides that they don't want to wait on a task to run to completion. In these cases, Dagger provides the Dagger.cancel! function, which allows for stopping a task while it's running, or terminating it before it gets the chance to start running.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"t = Dagger.@spawn sleep(1000)\n# We're bored, let's cancel `t`\nDagger.cancel!(t)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Dagger.cancel! is generally safe to call, as it will not actually force a task to stop; instead, Dagger will simply \"abandon\" the task and allow it to finish on its own in the background, and it will not block the execution of other DTasks that are queued to run. It is possible to force-cancel a task by doing Dagger.cancel!(t; force=true), but this is generally discouraged, as it can cause memory leaks, hangs, and segfaults.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"If it's desired to cancel all tasks that are scheduled or running, one can call Dagger.cancel!(), and all tasks will be abandoned (or force-cancelled, if specified). Additionally, if Dagger's scheduler needs to be restarted for any reason, one can call Dagger.cancel!(;halt_sch=true) to stop the scheduler and all tasks. The scheduler will be automatically restarted on the next @spawn/spawn call.","category":"page"},{"location":"task-spawning/#Lazy-API","page":"Task Spawning","title":"Lazy API","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Alongside the modern eager API, Dagger also has a legacy lazy API, accessible via @par or delayed. The above computation can be executed with the lazy API by substituting @spawn with @par and fetch with collect:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"p = Dagger.@par add1(4)\nq = Dagger.@par add2(p)\nr = Dagger.@par add1(3)\ns = Dagger.@par combine(p, q, r)\n\n@assert collect(s) == 16","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"or similarly, in block form:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"s = Dagger.@par begin\n    p = add1(4)\n    q = add2(p)\n    r = add1(3)\n    combine(p, q, r)\nend\n\n@assert collect(s) == 16","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Alternatively, if you want to compute but not fetch the result of a lazy operation, you can call compute on the task. This will return a Chunk object which references the result (see Chunks for more details):","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"x = Dagger.@par 1+2\ncx = compute(x)\ncx::Chunk\n@assert collect(cx) == 3","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Note that, as a legacy API, usage of the lazy API is generally discouraged for modern usage of Dagger. The reasons for this are numerous:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Nothing useful is happening while the DAG is being constructed, adding extra latency\nDynamically expanding the DAG can't be done with @par and delayed, making recursive nesting annoying to write\nEach call to compute/collect starts a new scheduler, and destroys it at the end of the computation, wasting valuable time on setup and teardown\nDistinct schedulers don't share runtime metrics or learned parameters, thus causing the scheduler to act less intelligently\nDistinct schedulers can't share work or data directly","category":"page"},{"location":"task-spawning/#Task-and-Scheduler-options","page":"Task Spawning","title":"Task and Scheduler options","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"While Dagger generally \"just works\", sometimes one needs to exert some more fine-grained control over how the scheduler allocates work. There are two parallel mechanisms to achieve this: Task options (from Options) and Scheduler options (from Sch.SchedulerOptions). Scheduler options operate globally across an entire DAG, and Task options operate on a task-by-task basis.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Scheduler options can be constructed and passed to collect() or compute() as the keyword argument options for lazy API usage:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"t = Dagger.@par 1+2\nopts = Dagger.Sch.SchedulerOptions(;single=1) # Execute on worker 1\n\ncompute(t; options=opts)\n\ncollect(t; options=opts)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Task options can be passed to @spawn/spawn, @par, and delayed similarly:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"# Execute on worker 1\n\nDagger.@spawn single=1 1+2\nDagger.spawn(+, Dagger.Options(;single=1), 1, 2)\n\ndelayed(+; single=1)(1, 2)","category":"page"},{"location":"task-spawning/#Changing-the-thread-occupancy","page":"Task Spawning","title":"Changing the thread occupancy","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"One of the supported Options is the occupancy keyword. This keyword can be used to communicate that a task is not expected to fully saturate a CPU core (e.g. due to being IO-bound). The basic usage looks like this:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Dagger.@spawn occupancy=Dict(Dagger.ThreadProc=>0) fn","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Consider the following function definitions:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\n\nfunction inner()\n    sleep(0.1)\nend\n\nfunction outer_full_occupancy()\n    @sync for _ in 1:2\n        # By default, full occupancy is assumed\n        Dagger.@spawn inner()\n    end\nend\n\nfunction outer_low_occupancy()\n    @sync for _ in 1:2\n        # Here, we're explicitly telling the scheduler to assume low occupancy\n        Dagger.@spawn occupancy=Dict(Dagger.ThreadProc => 0) inner()\n    end\nend","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"When running the first outer function N times in parallel, you should see parallelization until all threads are blocked:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"for N in [1, 2, 4, 8, 16]\n    @time fetch.([Dagger.@spawn outer_full_occupancy() for _ in 1:N])\nend","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"The results from the above code snippet should look similar to this (the timings will be influenced by your specific machine):","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"  0.124829 seconds (44.27 k allocations: 3.055 MiB, 12.61% compilation time)\n  0.104652 seconds (14.80 k allocations: 1.081 MiB)\n  0.110588 seconds (28.94 k allocations: 2.138 MiB, 4.91% compilation time)\n  0.208937 seconds (47.53 k allocations: 2.932 MiB)\n  0.527545 seconds (79.35 k allocations: 4.384 MiB, 0.64% compilation time)","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Whereas running the outer function that communicates a low occupancy (outer_low_occupancy) should run fully in parallel:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"for N in [1, 2, 4, 8, 16]\n    @time fetch.([Dagger.@spawn outer_low_occupancy() for _ in 1:N])\nend","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"In comparison, the outer_low_occupancy snippet should show results like this:","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"  0.120686 seconds (44.38 k allocations: 3.070 MiB, 13.00% compilation time)\n  0.105665 seconds (15.40 k allocations: 1.072 MiB)\n  0.107495 seconds (28.56 k allocations: 1.940 MiB)\n  0.109904 seconds (55.03 k allocations: 3.631 MiB)\n  0.117239 seconds (87.95 k allocations: 5.372 MiB)","category":"page"},{"location":"task-spawning/#Different-ways-to-spawn-tasks","page":"Task Spawning","title":"Different ways to spawn tasks","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Beyond the standard function call syntax Dagger.@spawn f(args...), Dagger also supports several other convenient ways to spawn tasks, mirroring Julia's own syntax variations.","category":"page"},{"location":"task-spawning/#Broadcast","page":"Task Spawning","title":"Broadcast","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Tasks can be spawned using Julia's broadcast syntax. This is useful for applying an operation element-wise to collections.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\nA = rand(4)\nB = rand(4)\n\n# Spawn a task to compute A .+ B\nadd_task = Dagger.@spawn A .+ B\n@assert fetch(add_task) ≈ A .+ B\n\nx = randn(100)\nabs_task = Dagger.@spawn abs.(x)\n@assert fetch(abs_task) == abs.(x)","category":"page"},{"location":"task-spawning/#Do-block","page":"Task Spawning","title":"Do block","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Dagger supports spawning tasks using Julia's do block syntax, which is often used for functions that take another function as an argument, especially anonymous functions.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\nA = rand(4)\n\n# Spawn a task using a do block with sum\nsum_do_task = Dagger.@spawn sum(A) do a\n    a + 1\nend\n@assert fetch(sum_do_task) ≈ sum(a -> a + 1, A)\n\n# Spawn a task with a function that accepts a do block\ndo_f = f -> f(42)\ndo_task = Dagger.@spawn do_f() do x\n    x + 1\nend\n@assert fetch(do_task) == 43","category":"page"},{"location":"task-spawning/#Anonymous-direct-call","page":"Task Spawning","title":"Anonymous direct call","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Tasks can be spawned directly from anonymous function definitions.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\nA = rand(4)\n\n# Spawn a task from an anonymous function\nanon_task = Dagger.@spawn A -> sum(A)\n@assert fetch(anon_task) == sum(A)\n\n# Anonymous function with closed-over arguments\ndims = 1\nanon_kwargs_task = Dagger.@spawn A -> sum(A; dims=dims)\n@assert fetch(anon_kwargs_task) == sum(A; dims=dims)","category":"page"},{"location":"task-spawning/#Getindex","page":"Task Spawning","title":"Getindex","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Spawning tasks that retrieve elements from indexable collections, such as arrays, using index notation is supported.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\nA = rand(4, 4)\n\n# Spawn a task to get A[1, 2]\ngetindex_task1 = Dagger.@spawn A[1, 2]\n@assert fetch(getindex_task1) == A[1, 2]\n\n# Spawn a task to get A[2] (linear indexing)\ngetindex_task2 = Dagger.@spawn A[2]\n@assert fetch(getindex_task2) == A[2]\n\n# Getindex from a DTask result\nB_task = Dagger.@spawn rand(4, 4)\ngetindex_task_from_dtask = Dagger.@spawn B_task[1, 2]\n@assert fetch(getindex_task_from_dtask) == fetch(B_task)[1, 2]\n\nR = Ref(42)\n# Spawn a task to get R[]\nref_getindex_task = Dagger.@spawn R[]\n@assert fetch(ref_getindex_task) == 42","category":"page"},{"location":"task-spawning/#Setindex!","page":"Task Spawning","title":"Setindex!","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Similarly, tasks can be spawned to modify elements of mutable collections (such as arrays). The object being modified must be running under Datadeps, or wrapped with Dagger.@mutable, to ensure that its contents can be mutated correctly.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\nA = Dagger.@mutable rand(4, 4)\n\n# Spawn a task to set A[1, 2] = 3.0\nsetindex_task1 = Dagger.@spawn A[1, 2] = 3.0\nfetch(setindex_task1) # Wait for the setindex! to complete\n@assert fetch(Dagger.@spawn A[1, 2]) == 3.0\n\n# Spawn a task to set A[2] = 4.0 (linear indexing)\nsetindex_task2 = Dagger.@spawn A[2] = 4.0\nfetch(setindex_task2)\n@assert fetch(Dagger.@spawn A[2]) == 4.0\n\nR = Dagger.@mutable Ref(42)\n# Spawn a task to set R[] = 43\nref_setindex_task = Dagger.@spawn R[] = 43\nfetch(ref_setindex_task)\n@assert fetch(Dagger.@spawn R[]) == 43","category":"page"},{"location":"task-spawning/#NamedTuple","page":"Task Spawning","title":"NamedTuple","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Tasks can be spawned to conveniently create NamedTuples.","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\n\n# Spawn a task to create a NamedTuple\nnt_task = Dagger.@spawn (;a=1, b=2)\n@assert fetch(nt_task) == (;a=1, b=2)\n\n# Spawn a task to create an empty NamedTuple\nempty_nt_task = Dagger.@spawn (;)\n@assert fetch(empty_nt_task) == (;)","category":"page"},{"location":"task-spawning/#Getproperty","page":"Task Spawning","title":"Getproperty","text":"","category":"section"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"Tasks can be spawned to access properties of NamedTuples (or other objects supporting getproperty).","category":"page"},{"location":"task-spawning/","page":"Task Spawning","title":"Task Spawning","text":"using Dagger\nnt = (;a=1, b=2)\n\n# Spawn a task to get nt.b\ngetprop_task = Dagger.@spawn nt.b\n@assert fetch(getprop_task) == nt.b\n\n# Getproperty from a DTask result\nnt2_task = Dagger.@spawn (;a=1, b=3)\ngetprop_task_from_dtask = Dagger.@spawn nt2_task.b\n@assert fetch(getprop_task_from_dtask) == fetch(nt2_task).b","category":"page"},{"location":"task-queues/#Task-Queues","page":"Task Queues","title":"Task Queues","text":"","category":"section"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"By default, @spawn/spawn submit tasks immediately and directly into Dagger's scheduler without modifications. However, sometimes you want to be able to tweak this behavior for a region of code; for example, when working with GPUs or other operations which operate in-place, you might want to emulate CUDA's stream semantics by ensuring that tasks execute sequentially (to avoid one kernel reading from an array while another kernel is actively writing to it). Or, you might want to ensure that a set of Dagger tasks are submitted into the scheduler all at once for benchmarking purposes or to emulate the behavior of delayed. This and more is possible through a mechanism called \"task queues\".","category":"page"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"A task queue in Dagger is an object that can be configured to accept unlaunched tasks from @spawn/spawn and either modify them or delay their launching arbitrarily. By default, Dagger tasks are enqueued through the DefaultTaskQueue, which submits tasks directly into the scheduler before @spawn/spawn returns. However, Dagger also has an InOrderTaskQueue, which ensures that tasks enqueued through it execute sequentially with respect to each other. This queue can be allocated with Dagger.spawn_sequential:","category":"page"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"A = rand(16)\nB = zeros(16)\nC = zeros(16)\nfunction vcopy!(B, A)\n    B .= A .+ 1.0\n    return\nend\nfunction vadd!(C, A, B)\n    C .+= A .+ B\n    return\nend\nwait(Dagger.spawn_sequential() do\n    Dagger.@spawn vcopy!(B, A)\n    Dagger.@spawn vadd!(C, A, B)\nend)","category":"page"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"In the above example, vadd! is guaranteed to wait until vcopy! is completed, even though vadd! isn't taking the result of vcopy! as an argument (which is how tasks are normally ordered).","category":"page"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"What if we wanted to launch multiple vcopy! calls within a spawn_sequential region and allow them to execute in parallel, but still ensure that the vadd! happens after they all finish? In this case, we want to switch to another kind of task queue: the LazyTaskQueue. This task queue batches up task submissions into groups, so that all tasks enqueued with it are placed in the scheduler all at once. But what would happen if we used this task queue (via spawn_bulk) within a region using spawn_sequential:","category":"page"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"A = rand(16)\nB1 = zeros(16)\nB2 = zeros(16)\nC = zeros(16)\nwait(Dagger.spawn_sequential() do\n    Dagger.spawn_bulk() do\n        Dagger.@spawn vcopy!(B1, A)\n        Dagger.@spawn vcopy!(B2, A)\n    end\n    Dagger.@spawn vadd!(C, B1, B2)\nend)","category":"page"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"Conveniently, Dagger's task queues can be nested to get the expected behavior; the above example will submit the two vcopy! tasks as a group (and they can execute concurrently), while still ensuring that those two tasks finish before the vadd! task executes.","category":"page"},{"location":"task-queues/","page":"Task Queues","title":"Task Queues","text":"warn: Warn\nTask queues do not propagate to nested tasks; if a Dagger task launches another task internally, the child task doesn't inherit the task queue that the parent task was enqueued in.","category":"page"},{"location":"benchmarking/#Benchmarking-Dagger","page":"Benchmarking","title":"Benchmarking Dagger","text":"","category":"section"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"For ease of benchmarking changes to Dagger's scheduler and the DArray, a benchmarking script exists at benchmarks/benchmark.jl. This script currently allows benchmarking a non-negative matrix factorization (NNMF) algorithm, which we've found to be a good evaluator of scheduling performance. The benchmark script can test with and without Dagger, and also has support for using CUDA or AMD GPUs to accelerate the NNMF via DaggerGPU.jl.","category":"page"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"The script checks for a number of environment variables, which are used to control the benchmarks that are performed (all of which are optional):","category":"page"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"BENCHMARK_PROCS: Selects the number of Julia processes and threads to start-up. Specified as 8:4, this option would start 8 extra Julia processes, with 4 threads each. Defaults to 2 processors and 1 thread each.\nBENCHMARK_REMOTES: Specifies a colon-separated list of remote servers to connect to and start Julia processes on, using BENCHMARK_PROCS to indicate the processor/thread configuration of those remotes. Disabled by default (uses the local machine).\nBENCHMARK_OUTPUT_FORMAT: Selects the output format for benchmark results. Defaults to jls, which uses Julia's Serialization stdlib, and can also be jld to use JLD.jl.\nBENCHMARK_RENDER: Configures rendering, which is disabled by default. Can be \"live\" or \"offline\", which are explained below.\nBENCHMARK: Specifies the set of benchmarks to run as a comma-separated list, where each entry can be one of cpu, cuda, or amdgpu, and may optionally append +dagger (like cuda+dagger) to indicate whether or not to use Dagger. Defaults to cpu,cpu+dagger, which runs CPU benchmarks with and without Dagger.\nBENCHMARK_SCALE: Determines how much to scale the benchmark sizing by, typically specified as a UnitRange{Int}. Defaults to 1:5:50, which runs each scale from 1 to 50, in steps of 5.","category":"page"},{"location":"benchmarking/#Rendering-with-BENCHMARK_RENDER","page":"Benchmarking","title":"Rendering with BENCHMARK_RENDER","text":"","category":"section"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Dagger contains visualization code for the scheduler (as a Gantt chart) and thunk execution profiling (flamechart), which can be enabled with BENCHMARK_RENDER. Additionally, rendering can be done \"live\", served via a Mux.jl webserver run locally, or \"offline\", where the visualization will be embedded into the results output file. By default, rendering is disabled. If BENCHMARK_RENDER is set to live, a Mux webserver is started at localhost:8000 (the address is not yet configurable), and the Gantt chart and profiling flamechart will be rendered once the benchmarks start. If set to offline, data visualization will happen in the background, and will be passed in the results file.","category":"page"},{"location":"benchmarking/","page":"Benchmarking","title":"Benchmarking","text":"Note that Gantt chart and flamechart output is only generated and relevant during Dagger execution.","category":"page"},{"location":"benchmarking/#TODO:-Plotting","page":"Benchmarking","title":"TODO: Plotting","text":"","category":"section"},{"location":"api-timespanlogging/types/#TimespanLogging-Types","page":"Types","title":"TimespanLogging Types","text":"","category":"section"},{"location":"api-timespanlogging/types/","page":"Types","title":"Types","text":"Pages = [\"types.md\"]","category":"page"},{"location":"api-timespanlogging/types/#Log-Sink-Types","page":"Types","title":"Log Sink Types","text":"","category":"section"},{"location":"api-timespanlogging/types/#TimespanLogging.MultiEventLog","page":"Types","title":"TimespanLogging.MultiEventLog","text":"MultiEventLog\n\nProcesses events immediately, generating multiple log streams. Multiple consumers may register themselves in the MultiEventLog, and when accessed, log events will be provided to all consumers. A consumer is simply a function or callable struct which will be called with an event when it's generated. The return value of the consumer will be pushed into a log stream dedicated to that consumer. Errors thrown by consumers will be caught and rendered, but will not otherwise interrupt consumption by other consumers, or future consumption cycles. An error will result in nothing being appended to that consumer's log.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.LocalEventLog","page":"Types","title":"TimespanLogging.LocalEventLog","text":"LocalEventLog\n\nStores events in a process-local array. Accessing the logs is all-or-nothing; if multiple consumers call get_logs!, they will get different sets of logs.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.NoOpLog","page":"Types","title":"TimespanLogging.NoOpLog","text":"NoOpLog\n\nDisables event logging entirely.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#Built-in-Event-Types","page":"Types","title":"Built-in Event Types","text":"","category":"section"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.CoreMetrics","page":"Types","title":"TimespanLogging.Events.CoreMetrics","text":"CoreMetrics\n\nTracks the timestamp, category, and kind of the Event object generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.IDMetrics","page":"Types","title":"TimespanLogging.Events.IDMetrics","text":"IDMetrics\n\nTracks the ID of Event objects generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.TimelineMetrics","page":"Types","title":"TimespanLogging.Events.TimelineMetrics","text":"TimelineMetrics\n\nTracks the timeline of Event objects generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.FullMetrics","page":"Types","title":"TimespanLogging.Events.FullMetrics","text":"FullMetrics\n\nTracks the full Event object generated by log events.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.CPULoadAverages","page":"Types","title":"TimespanLogging.Events.CPULoadAverages","text":"CPULoadAverages\n\nMonitors the CPU load averages.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.MemoryFree","page":"Types","title":"TimespanLogging.Events.MemoryFree","text":"MemoryFree\n\nMonitors the percentage of free system memory.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.EventSaturation","page":"Types","title":"TimespanLogging.Events.EventSaturation","text":"EventSaturation\n\nTracks the compute saturation (running tasks) per-processor.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.DebugMetrics","page":"Types","title":"TimespanLogging.Events.DebugMetrics","text":"Debugging metric, used to log event start/finish via @debug.\n\n\n\n\n\n","category":"type"},{"location":"api-timespanlogging/types/#TimespanLogging.Events.LogWindow","page":"Types","title":"TimespanLogging.Events.LogWindow","text":"LogWindow\n\nAggregator that prunes events to within a given time window.\n\n\n\n\n\n","category":"type"},{"location":"logging/#Logging","page":"Logging: Basics","title":"Logging","text":"","category":"section"},{"location":"logging/","page":"Logging: Basics","title":"Logging: Basics","text":"Dagger provides mechanisms to log and visualize scheduler events. This can be useful for debugging and performance analysis.","category":"page"},{"location":"logging/#Basic-Logging-Functions","page":"Logging: Basics","title":"Basic Logging Functions","text":"","category":"section"},{"location":"logging/","page":"Logging: Basics","title":"Logging: Basics","text":"The primary functions for controlling logging are:","category":"page"},{"location":"logging/","page":"Logging: Basics","title":"Logging: Basics","text":"Dagger.enable_logging!: Enables logging. This function uses the MultiEventLog by default, which is flexible and performant. You can customize its behavior with keyword arguments.\nDagger.disable_logging!: Disables logging.\nDagger.fetch_logs!: Fetches the logs from all workers. This returns a Dict where keys are worker IDs and values are the logs.","category":"page"},{"location":"logging/#Example-Usage","page":"Logging: Basics","title":"Example Usage","text":"","category":"section"},{"location":"logging/","page":"Logging: Basics","title":"Logging: Basics","text":"using Dagger\n\n# Enable logging\nDagger.enable_logging!()\n\n# Run some Dagger computations\nwait(Dagger.@spawn sum([1, 2, 3]))\n\n# Fetch logs\nlogs = Dagger.fetch_logs!()\n\n# Disable logging\nDagger.disable_logging!()\n\n# You can now inspect the `logs` Dict or use visualization tools\n# like `show_logs` and `render_logs` (see [Logging: Visualization](@ref logging-visualization.md)).","category":"page"},{"location":"logging/","page":"Logging: Basics","title":"Logging: Basics","text":"For more advanced logging configurations, such as custom log sinks and consumers, see Logging: Advanced.","category":"page"},{"location":"external-languages/python/#Python-Integration","page":"Python","title":"Python Integration","text":"","category":"section"},{"location":"external-languages/python/","page":"Python","title":"Python","text":"If you're using Python as your main programming language, Dagger can be easily integrated into your workflow. Dagger has built-in support for Python, which you can easily access through the pydaggerjl library (accessible through pip). This library provides a Pythonic interface to Dagger, allowing you to spawn Dagger tasks that run Python functions on Python arguments.","category":"page"},{"location":"external-languages/python/","page":"Python","title":"Python","text":"Here's a simple example of interfacing between Python's numpy library and Dagger:","category":"page"},{"location":"external-languages/python/","page":"Python","title":"Python","text":"import numpy as np\nfrom pydaggerjl import daggerjl\n\n# Create a Dagger DTask to sum the elements of an array\ntask = daggerjl.spawn(np.sum, np.array([1, 2, 3]))\n\n# Wait on task to finish\n# This is purely educational, as fetch will wait for the task to finish\ndaggerjl.wait(task)\n\n# Fetch the result\nresult = daggerjl.fetch(task)\n\nprint(f\"The sum is: {result}\")\n\n# Create two numpy arrays\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Element-wise sum of the two arrays\ntask = daggerjl.spawn(np.add, a, b)\n\n# Fetch the result\nresult = daggerjl.fetch(task)\n\nprint(f\"The element-wise sum is: {result}\")\n\n# Element-wise sum of last result with itself\ntask2 = daggerjl.spawn(np.add, task, task)\n\n# Fetch the result\nresult2 = daggerjl.fetch(task2)\n\nprint(f\"The element-wise sum of the last result with itself is: {result2}\")","category":"page"},{"location":"external-languages/python/","page":"Python","title":"Python","text":"Keep an eye on Dagger and pydaggerjl - new features are soon to come!","category":"page"},{"location":"task-affinity/#Task-Affinity","page":"Task Affinity","title":"Task Affinity","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Dagger's allows for precise control over task placement and result availability using scopes. Tasks are assigned based on the combination of multiple scopes: scope/compute_scope, and result_scope (which can all be specified with @spawn), and additionally the scopes of any arguments to the task (in the form of a scope attached to a Chunk argument). Let's take a look at how to configure these scopes, and how they work together to direct task placement.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"For more information on how scopes work, see Scopes.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Task-Scopes","page":"Task Affinity","title":"Task Scopes","text":"","category":"section"},{"location":"task-affinity/#Scope","page":"Task Affinity","title":"Scope","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"scope defines the general set of locations where a Dagger task can execute. If scope is not specified, the task falls back to DefaultScope(), allowing it to run wherever execution is possible. Execution occurs on any worker within the defined scope.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Example:","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"g = Dagger.@spawn scope=Dagger.scope(worker=3) f(x,y)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Task g executes only on worker 3. Its result can be accessed by any worker.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Compute-Scope","page":"Task Affinity","title":"Compute Scope","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Like scope, compute_scope also specifies where a Dagger task can execute. The key difference is if both compute_scope and scope are provided, compute_scope takes precedence over scope for execution placement. If neither is specified, then they default to DefaultScope(). ","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Example:","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"g1 = Dagger.@spawn scope=Dagger.scope(worker=2,thread=3) compute_scope=Dagger.scope((worker=1, thread=2), (worker=3, thread=1))  f(x,y)\ng2 = Dagger.@spawn compute_scope=Dagger.scope((worker=1, thread=2), (worker=3, thread=1)) f(x,y)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Tasks g1 and g2 execute on either thread 2 of worker 1, or thread 1 of worker 3. The scope argument to g1 is ignored. Their result can be accessed by any worker.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Result-Scope","page":"Task Affinity","title":"Result Scope","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"The resultscope limits the processors from which a task's result can be accessed. This can be useful for managing data locality and minimizing transfers. If `resultscopeis not specified, it defaults toAnyScope()`, meaning the result can be accessed by any processor (including those not default enabled for task execution, such as GPUs).","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Example:","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"g = Dagger.@spawn result_scope=Dagger.scope(worker=3, threads=[1, 3, 4]) f(x,y)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"The result of g is accessible only from threads 1, 3 and 4 of worker process 3. The task's execution may happen anywhere on threads 1, 3 and 4 of worker 3.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Interaction-of-compute_scope-and-result_scope","page":"Task Affinity","title":"Interaction of compute_scope and result_scope","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"When scope/compute_scope and result_scope are specified, the scheduler executes the task on the intersection of the effective compute scope (which will be compute_scope if provided, otherwise scope) and the result_scope. If the intersection is empty, then the scheduler throws a Dagger.Sch.SchedulerException error.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Example:","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"g = Dagger.@spawn scope=Dagger.scope(worker=3,thread=2) compute_scope=Dagger.scope(worker=2) result_scope=Dagger.scope((worker=2, thread=2), (worker=4, thread=2)) f(x,y)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"The task g computes on thread 2 of worker 2 (as it's the intersection of compute and result scopes), but accessng its result is restricted to thread 2 of worker 2 and thread 2 of worker 4.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Function-as-a-Chunk","page":"Task Affinity","title":"Function as a Chunk","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"This section explains how scope/compute_scope and result_scope affect tasks when a Chunk is used to specify the function to be executed by @spawn (e.g. created via Dagger.tochunk(...) or by calling fetch(task; raw=true) on a task). This may seem strange (to use a Chunk to specify the function to be executed), but it can be useful with working with callable structs, such as closures or Flux.jl models.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Assume g is some function, e.g. g(x, y) = x * 2 + y * 3, and chunk_scope is its defined affinity.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"When Dagger.tochunk(...) is used to pass a Chunk as the function to be executed by @spawn:","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"The result is accessible only on processors in chunk_scope.\nDagger validates that there is an intersection between chunk_scope, the effective compute_scope (derived from @spawn's compute_scope or scope), and the result_scope. If no intersection exists, the scheduler throws an exception.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"!!! info While chunk_proc is currently required when constructing a chunk, it is only used to pick the most optimal processor for accessing the chunk; it does not affect which set of processors the task may execute on.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Usage:","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"chunk_scope = Dagger.scope(worker=3)\nchunk_proc = Dagger.OSProc(3) # not important, just needs to be a valid processor\ng(x, y) = x * 2 + y * 3\ng_chunk = Dagger.tochunk(g, chunk_proc, chunk_scope)\nh1 = Dagger.@spawn scope=Dagger.scope(worker=3) g_chunk(10, 11)\nh2 = Dagger.@spawn compute_scope=Dagger.scope((worker=1, thread=2), (worker=3, thread=1)) g_chunk(20, 21)\nh3 = Dagger.@spawn scope=Dagger.scope(worker=2,thread=3) compute_scope=Dagger.scope((worker=1, thread=2), (worker=3, thread=1)) g_chunk(30, 31)\nh4 = Dagger.@spawn result_scope=Dagger.scope(worker=3) g_chunk(40, 41)\nh5 = Dagger.@spawn scope=Dagger.scope(worker=3,thread=2) compute_scope=Dagger.scope(worker=3) result_scope=Dagger.scope(worker=3,threads=[2,3]) g_chunk(50, 51)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"In all these cases (h1 through h5), the tasks get executed on any processor within chunk_scope and its result is accessible only within chunk_scope.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Chunk-arguments","page":"Task Affinity","title":"Chunk arguments","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"This section details behavior when some or all of a task's arguments are Chunks.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Assume g(x, y) = x * 2 + y * 3, and arg = Dagger.tochunk(g(1, 2), arg_proc, arg_scope), where arg_scope is the argument's defined scope. Assume arg_scope = Dagger.scope(worker=2).","category":"page"},{"location":"task-affinity/#Scope-2","page":"Task Affinity","title":"Scope","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"If arg_scope and scope do not intersect, the scheduler throws an exception. Execution occurs on the intersection of scope and arg_scope.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"h = Dagger.@spawn scope=Dagger.scope(worker=2) g(arg, 11)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Task h executes on any worker within the intersection of scope and arg_scope. The result is accessible from any processor.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Compute-scope-and-Chunk-argument-scopes-interaction","page":"Task Affinity","title":"Compute scope and Chunk argument scopes interaction","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"If arg_scope and compute_scope do not intersect, the scheduler throws an exception. Otherwise, execution happens on the intersection of the effective compute scope (which will be compute_scope if provided, otherwise scope) and arg_scope.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"h1 = Dagger.@spawn compute_scope=Dagger.scope((worker=1, thread=2), (worker=2, thread=1)) g(arg, 11)\nh2 = Dagger.@spawn scope=Dagger.scope(worker=2,thread=3) compute_scope=Dagger.scope((worker=1, thread=2), (worker=2, thread=1)) g(arg, 21)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Tasks h1 and h2 execute on any processor within the intersection of the compute_scope and arg_scope. scope is ignored if compute_scope is specified. The result is accessible from any processor.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Result-scope-and-Chunk-argument-scopes-interaction","page":"Task Affinity","title":"Result scope and Chunk argument scopes interaction","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"If only result_scope is specified, computation happens on any processor within the intersection of arg_scope and result_scope, and the result is only accessible within result_scope.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"h = Dagger.@spawn result_scope=Dagger.scope(worker=2) g(arg, 11)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Task h executes on any processor within the intersection of arg_scope and result_scope. The result is accessible from only within result_scope.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"","category":"page"},{"location":"task-affinity/#Compute,-result,-and-chunk-argument-scopes-interaction","page":"Task Affinity","title":"Compute, result, and chunk argument scopes interaction","text":"","category":"section"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"When scope/compute_scope, result_scope, and Chunk argument scopes are all used, the scheduler executes the task on the intersection of arg_scope, the effective compute scope (which is compute_scope if provided, otherwise scope), and result_scope. If no intersection exists, the scheduler throws an exception.","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"h = Dagger.@spawn scope=Dagger.scope(worker=3,thread=2) compute_scope=Dagger.scope(worker=2) result_scope=Dagger.scope((worker=2, thread=2), (worker=4, thread=2)) g(arg, 31)","category":"page"},{"location":"task-affinity/","page":"Task Affinity","title":"Task Affinity","text":"Task h computes on thread 2 of worker 2 (as it's the intersection of arg_scope, compute_scope, and result_scope), and its result access is restricted to thread 2 of worker 2 or thread 2 of worker 4.","category":"page"},{"location":"api-daggerwebdash/functions/#DaggerWebDash-Functions","page":"Functions and Macros","title":"DaggerWebDash Functions","text":"","category":"section"},{"location":"api-daggerwebdash/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"datadeps/#Datadeps-(Data-Dependencies)","page":"Basics","title":"Datadeps (Data Dependencies)","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"For many programs, the restriction that tasks cannot write to their arguments feels overly restrictive and makes certain kinds of programs (such as in-place linear algebra) hard to express efficiently in Dagger. Thankfully, there is a solution called \"Datadeps\" (short for \"data dependencies\"), accessible through the spawn_datadeps function. This function constructs a \"datadeps region\", within which tasks are allowed to write to their arguments, with parallelism controlled via dependencies specified via argument annotations. At the end of the \"datadeps region\" the spawn_datadeps will wait for the completion of all the tasks launched within it. Let's look at a simple example to make things concrete:","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"A = rand(1000)\nB = rand(1000)\nC = zeros(1000)\n@everywhere add!(X, Y) = X .+= Y\nDagger.spawn_datadeps() do\n    Dagger.@spawn add!(InOut(B), In(A))\n    Dagger.@spawn copyto!(Out(C), In(B))\nend","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"In this example, we have two Dagger tasks being launched, one adding A into B, and the other copying B into C. The add! task is specifying that A is being only read from (In for \"input\"), and that B is being read from and written to (Out for \"output\", InOut for \"input and output\"). The copyto task, similarly, is specifying that B is being read from, and C is only being written to.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Without spawn_datadeps and In, Out, and InOut, the result of these tasks would be undefined; the two tasks could execute in parallel, or the copyto! could occur before the add!, resulting in all kinds of mayhem. However, spawn_datadeps changes things: because we have told Dagger how our tasks access their arguments, Dagger knows to control the parallelism and ordering, and ensure that add! executes and finishes before copyto! begins, ensuring that copyto! \"sees\" the changes to B before executing.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"There is another important aspect of spawn_datadeps that makes the above code work: if all of the Dagger.@spawn macros are removed, along with the dependency specifiers, the program would still produce the same results, without using Dagger. In other words, the parallel (Dagger) version of the program produces identical results to the serial (non-Dagger) version of the program. This is similar to using Dagger with purely functional tasks and without spawn_datadeps - removing Dagger.@spawn will still result in a correct (sequential and possibly slower) version of the program. Basically, spawn_datadeps will ensure that Dagger respects the ordering and dependencies of a program, while still providing parallelism, where possible.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"But where is the parallelism? The above example doesn't actually have any parallelism to exploit! Let's take a look at another example to see the datadeps model truly shine:","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"# Tree reduction of multiple arrays into the first array\nfunction tree_reduce!(op::Base.Callable, As::Vector{<:Array})\n    Dagger.spawn_datadeps() do\n        to_reduce = Vector[]\n        push!(to_reduce, As)\n        while !isempty(to_reduce)\n            As = pop!(to_reduce)\n            n = length(As)\n            if n == 2\n                Dagger.@spawn Base.mapreducedim!(identity, op, InOut(As[1]), In(As[2]))\n            elseif n > 2\n                push!(to_reduce, [As[1], As[div(n,2)+1]])\n                push!(to_reduce, As[1:div(n,2)])\n                push!(to_reduce, As[div(n,2)+1:end])\n            end\n        end\n    end\n    return As[1]\nend\n\nAs = [rand(1000) for _ in 1:1000]\nBs = copy.(As)\ntree_reduce!(+, As)\n@assert isapprox(As[1], reduce((x,y)->x .+ y, Bs))","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"In the above implementation of tree_reduce! (which is designed to perform an elementwise reduction across a vector of arrays), we have a tree reduction operation where pairs of arrays are reduced, starting with neighboring pairs, and then reducing pairs of reduction results, etc. until the final result is in As[1]. We can see that the application of Dagger to this algorithm is simple - only the single Base.mapreducedim! call is passed to Dagger - yet due to the data dependencies and the algorithm's structure, there should be plenty of parallelism to be exploited across each of the parallel reductions at each \"level\" of the reduction tree. Specifically, any two Dagger.@spawn calls which access completely different pairs of arrays can execute in parallel, while any call which has an In on an array will wait for any previous call which has an InOut on that same array.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Additionally, we can notice a powerful feature of this model - if the Dagger.@spawn macro is removed, the code still remains correct, but simply runs sequentially. This means that the structure of the program doesn't have to change in order to use Dagger for parallelization, which can make applying Dagger to existing algorithms quite effortless.","category":"page"},{"location":"datadeps/#Limitations","page":"Basics","title":"Limitations","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"It's important to be aware of a key limitation when working with Dagger.spawn_datadeps. Operations that involve explicit synchronization or fetching results of other Dagger tasks, such as fetch, wait, or @sync, cannot be used directly inside a spawn_datadeps block.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"The spawn_datadeps region is designed to manage data dependencies automatically based on the In, Out, and InOut annotations. Introducing explicit synchronization primitives can interfere with this mechanism and lead to unexpected behavior or errors.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Example of what NOT to do:","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Dagger.spawn_datadeps() do\n    # Incorrect: Using fetch inside spawn_datadeps\n    task1 = Dagger.@spawn my_func1!(InOut(A))\n    result1 = fetch(task1) # This will not work as expected\n\n    # Incorrect: Using wait inside spawn_datadeps\n    task2 = Dagger.@spawn my_func2!(InOut(B))\n    wait(task2) # This will also lead to issues\n\n    # Incorrect: Using @sync inside spawn_datadeps\n    @sync begin\n        Dagger.@spawn my_func3!(InOut(C))\n        Dagger.@spawn my_func4!(InOut(D))\n    end\nend","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"If you need to synchronize or fetch results, these operations should be performed outside the spawn_datadeps block. The primary purpose of spawn_datadeps is to define a region where data dependencies for mutable operations are automatically managed.","category":"page"},{"location":"datadeps/#Aliasing-Support","page":"Basics","title":"Aliasing Support","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Datadeps is smart enough to detect when two arguments from different tasks actually access the same memory (we say that these arguments \"alias\"). There's the obvious case where the two arguments are exactly the same object, but Datadeps is also aware of more subtle cases, such as when two arguments are different views into the same array, or where two arrays point to the same underlying memory. In these cases, Datadeps will ensure that the tasks are executed in the correct order - if one task writes to an argument which aliases with an argument read by another task, those two tasks will be executed in sequence, rather than in parallel.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"There are two ways to specify aliasing to Datadeps. The simplest way is the most straightforward: if the argument passed to a task is a view or another supported object (such as an UpperTriangular-wrapped array), Datadeps will compare it with all other task's arguments to determine if they alias. This works great when you want to pass that view or UpperTriangular object directly to the called function. For example:","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"A = rand(1000)\nA_l = view(A, 1:500)\nA_r = view(A, 501:1000)\n\n# inc! supports views, so we can pass A_l and A_r directly\n@everywhere inc!(X) = X .+= 1\n\nDagger.spawn_datadeps() do\n    # These two tasks don't alias, so they can run in parallel\n    Dagger.@spawn inc!(InOut(A_l))\n    Dagger.@spawn inc!(InOut(A_r))\n\n    # This task aliases with the previous two, so it will run after them\n    Dagger.@spawn inc!(InOut(A))\nend","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"The other way allows you to separate what argument is passed to the function, from how that argument is accessed within the function. This is done with the Deps wrapper, which is used like so:","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"A = rand(1000, 1000)\n\n@everywhere inc_upper!(X) = UpperTriangular(X) .+= 1\n@everywhere inc_ulower!(X) = UnitLowerTriangular(X) .+= 1\n@everywhere inc_diag!(X) = X[diagind(X)] .+= 1\n\nDagger.spawn_datadeps() do\n    # These two tasks don't alias, so they can run in parallel\n    Dagger.@spawn inc_upper!(Deps(A, InOut(UpperTriangular)))\n    Dagger.@spawn inc_ulower!(Deps(A, InOut(UnitLowerTriangular)))\n\n    # This task aliases with the `inc_upper!` task (`UpperTriangular` accesses the diagonal of the array)\n    Dagger.@spawn inc_diag!(Deps(A, InOut(Diagonal)))\nend","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"We call InOut(Diagonal) an \"aliasing modifier\". The purpose of Deps is to pass an argument (here, A) as-is, while specifying to Datadeps what portions of the argument will be accessed (in this case, the diagonal elements) and how (read/write/both). You can pass any number of aliasing modifiers to Deps.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Deps is particularly useful for declaring aliasing with Diagonal, Bidiagonal, Tridiagonal, and SymTridiagonal access, as these \"wrappers\" make a copy of their parent array and thus can't be used to \"mask\" access to the parent like UpperTriangular and UnitLowerTriangular can (which is valuable for writing memory-efficient, generic algorithms in Julia).","category":"page"},{"location":"datadeps/#Supported-Aliasing-Modifiers","page":"Basics","title":"Supported Aliasing Modifiers","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Any function that returns the original object or a view of the original object\nUpperTriangular/LowerTriangular/UnitUpperTriangular/UnitLowerTriangular\nDiagonal/Bidiagonal/Tridiagonal/SymTridiagonal (via Deps, e.g. to read from the diagonal of X: Dagger.@spawn sum(Deps(X, In(Diagonal))))\nSymbol for field access (via Deps, e.g. to write to X.value: Dagger.@spawn setindex!(Deps(X, InOut(:value)), :value, 42)","category":"page"},{"location":"datadeps/#In-place-data-movement-rules","page":"Basics","title":"In-place data movement rules","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Datadeps uses a specialized 5-argument function, Dagger.move!(dep_mod, from_space::Dagger.MemorySpace, to_space::Dagger.MemorySpace, from, to), for managing in-place data movement. This function is an in-place variant of the more general move function (see Data movement rules) and is exclusively used within the Datadeps system. The dep_mod argument is usually just identity, but it can also be an access modifier function like UpperTriangular, which limits what portion of the data should be read from and written to.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"The core responsibility of move! is to read data from the from argument and write it directly into the to argument. This is crucial for operations that modify data in place, as often encountered in numerical computing and linear algebra.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"The default implementation of move! handles Chunk objects by unwrapping them and then recursively calling move! on the underlying values. This ensures that the in-place operation is performed on the actual data.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Users have the option to define their own move! implementations for custom data types. However, this is typically not necessary for types that are subtypes of AbstractArray, provided that these types support the standard Base.copyto!(to, from) function. The default move! will leverage copyto! for such array types, enabling efficient in-place updates.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"Here's an example of a custom move! implementation:","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"struct MyCustomArrayWrapper{T,N}\n    data::Array{T,N}\nend\n\n# Custom move! function for MyCustomArrayWrapper\nfunction Dagger.move!(dep_mod::Any, from_space::Dagger.MemorySpace, to_space::Dagger.MemorySpace, from::MyCustomArrayWrapper, to::MyCustomArrayWrapper)\n    copyto!(dep_mod(to.data), dep_mod(from.data))\n    return\nend","category":"page"},{"location":"datadeps/#Chunk-and-DTask-slicing-with-view","page":"Basics","title":"Chunk and DTask slicing with view","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"The view function allows you to efficiently create a \"view\" of a Chunk or DTask that contains an array. This enables operations on specific parts of your distributed data using standard Julia array slicing, without needing to materialize the entire array.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"    view(c::Chunk, slices...) -> ChunkView\n    view(c::DTask, slices...) -> ChunkView","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"These methods create a ChunkView of a Chunk or DTask, which may be used as an argument to a Dagger.@spawn call in a Datadeps region. You specify the desired view using standard Julia array slicing syntax, identical to how you would slice a regular array.","category":"page"},{"location":"datadeps/#Examples","page":"Basics","title":"Examples","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"julia> A = rand(64, 64)\n64×64 Matrix{Float64}:\n[...]\n\njulia> DA = DArray(A, Blocks(8,8)) \n64x64 DMatrix{Float64} with 8x8 partitions of size 8x8:\n[...]\n\njulia> chunk = DA.chunks[1,1] \nDTask (finished)\n\njulia> view(chunk, :, :) # View the entire 8x8 chunk\nChunkSlice{2}(Dagger.Chunk(...), (Colon(), Colon()))\n\njulia> view(chunk, 1:4, 1:4) # View the top-left 4x4 sub-region of the chunk\nChunkSlice{2}(Dagger.Chunk(...), (1:4, 1:4))\n\njulia> view(chunk, 1, :) # View the first row of the chunk\nChunkSlice{2}(Dagger.Chunk(...), (1, Colon()))\n\njulia> view(chunk, :, 5) # View the fifth column of the chunk\nChunkSlice{2}(Dagger.Chunk(...), (Colon(), 5))\n\njulia> view(chunk, 1:2:7, 2:2:8) # View with stepped ranges\nChunkSlice{2}(Dagger.Chunk(...), (1:2:7, 2:2:8))","category":"page"},{"location":"datadeps/#Example-Usage:-Parallel-Row-Summation-of-a-DArray-using-view","page":"Basics","title":"Example Usage: Parallel Row Summation of a DArray using view","text":"","category":"section"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"This example demonstrates how to sum multiple rows of a DArray by using view to process individual rows within chunks to get a vector of row sums.","category":"page"},{"location":"datadeps/","page":"Basics","title":"Basics","text":"julia> A = DArray(rand(10, 1000), Blocks(2, 1000))\n10x1000 DMatrix{Float64} with 5x1 partitions of size 2x1000: \n[...]\n\n# Helper function to sum a single row and store it in a provided array view\njulia> @everywhere function sum_array_row!(row_sum::AbstractArray{Float64}, x::AbstractArray{Float64})\n    row_sum[1] = sum(x)\nend\n\n# Number of rows\njulia> nrows = size(A,1)\n\n# Initialize a zero array in the final row sums\njulia> row_sums = zeros(nrows)\n\n# Spawn tasks to sum each row in parallel using views\njulia> Dagger.spawn_datadeps() do\n           sz = size(A.chunks,1) \n           nrows_per_chunk = nrows ÷ sz\n           for i in 1:sz\n               for j in 1:nrows_per_chunk\n                   Dagger.@spawn sum_array_row!(Out(view(row_sums, (nrows_per_chunk*(i-1)+j):(nrows_per_chunk*(i-1)+j))),\n                                                In(Dagger.view(A.chunks[i,1], j:j, :)))\n               end\n           end\n       end\n\n# Print the result\njulia> println(\"Row sums: \", row_sums)\nRow sums: [499.8765, 500.1234, ..., 499.9876]","category":"page"},{"location":"api-timespanlogging/functions/#TimespanLogging-Functions","page":"Functions and Macros","title":"TimespanLogging Functions","text":"","category":"section"},{"location":"api-timespanlogging/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"api-timespanlogging/functions/#Basic-Functions","page":"Functions and Macros","title":"Basic Functions","text":"","category":"section"},{"location":"api-timespanlogging/functions/#TimespanLogging.timespan_start","page":"Functions and Macros","title":"TimespanLogging.timespan_start","text":"timespan_start(ctx, category::Symbol, id, tl)\n\nGenerates an Event{:start} which denotes the start of an event. The event is categorized by category, and uniquely identified by id; these two must be the same passed to timespan_finish to close the event. tl is the \"timeline\" of the event, which is just an arbitrary payload attached to the event.\n\n\n\n\n\n","category":"function"},{"location":"api-timespanlogging/functions/#TimespanLogging.timespan_finish","page":"Functions and Macros","title":"TimespanLogging.timespan_finish","text":"timespan_finish(ctx, category::Symbol, id, tl)\n\nGenerates an Event{:finish} which denotes the end of an event. The event is categorized by category, and uniquely identified by id; these two must be the same as previously passed to timespan_start. tl is the \"timeline\" of the event, which is just an arbitrary payload attached to the event.\n\n\n\n\n\n","category":"function"},{"location":"api-timespanlogging/functions/#TimespanLogging.get_logs!","page":"Functions and Macros","title":"TimespanLogging.get_logs!","text":"get_logs!(::LocalEventLog, raw=false; only_local=false) -> Union{Vector{Timespan},Vector{Event}}\n\nGet the logs from each process' local event log, clearing it in the process. Set raw to true to get potentially unmatched Events; the default is to return only matched events as Timespans. If only_local is set true, only process-local logs will be fetched; the default is to fetch logs from all processes.\n\n\n\n\n\n","category":"function"},{"location":"api-timespanlogging/functions/#Logging-Metric-Functions","page":"Functions and Macros","title":"Logging Metric Functions","text":"","category":"section"},{"location":"api-timespanlogging/functions/#TimespanLogging.init_similar","page":"Functions and Macros","title":"TimespanLogging.init_similar","text":"Creates a copy of x with the same configuration, but fresh/empty data.\n\n\n\n\n\n","category":"function"},{"location":"scopes/#Scopes","page":"Scopes","title":"Scopes","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Sometimes you will have data that is only meaningful in a certain location, such as within a single Julia process, a given server, or even for a specific Dagger processor. We call this location a \"scope\" in Dagger, denoting the bounds within which the data is meaningful and valid. For example, C pointers are typically scoped to a process, file paths are scoped to one or more servers dependent on filesystem configuration, etc. By default, Dagger doesn't recognize this; it treats everything passed into a task, or generated from a task, as inherently safe to transfer anywhere else. When this is not the case, Dagger provides optional scopes to instruct the scheduler where data is considered valid.","category":"page"},{"location":"scopes/#Scope-Basics","page":"Scopes","title":"Scope Basics","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Let's take the example of a webcam handle generated by VideoIO.jl. This handle is a C pointer, and thus has process scope. We can open the handle on a given process, and set the scope of the resulting data to be locked to the current process with Dagger.scope to construct a ProcessScope:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"using VideoIO, Distributed\n\nfunction get_handle()\n    handle = VideoIO.opencamera()\n    proc = Dagger.task_processor()\n    scope = Dagger.scope(worker=myid()) # constructs a `ProcessScope`\n    return Dagger.tochunk(handle, proc, scope)\nend\n\ncam_handle = Dagger.@spawn get_handle()","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Now, wherever cam_handle is passed, Dagger will ensure that any computations on the handle only happen within its defined scope. For example, we can read from the camera:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"cam_frame = Dagger.@spawn read(cam_handle)","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"The cam_frame task is executed within any processor on the same process that the cam_handle task was executed on. Of course, the resulting camera frame is not scoped to anywhere specific (denoted as AnyScope), and thus computations on it may execute anywhere.","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"You may also encounter situations where you want to use a callable struct (such as a closure, or a Flux.jl layer) only within a certain scope; you can specify the scope of the function pretty easily:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"using Flux\nm = Chain(...)\n# If `m` is only safe to transfer to and execute on this process,\n# we can set a `ProcessScope` on it:\nresult = Dagger.@spawn scope=Dagger.scope(worker=myid()) m(rand(8,8))","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Setting a scope on the function treats it as a regular piece of data (like the arguments to the function), so it participates in the scoping rules described in the following sections all the same.","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Scope Functions","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Now, let's try out some other kinds of scopes, starting with NodeScope. This scope encompasses the server that one or more Julia processes may be running on. Say we want to use memory mapping (mmap) to more efficiently send arrays between two tasks. We can construct the mmap'd array in one task, attach a NodeScope() to it, and using the path of the mmap'd file to communicate its location, lock downstream tasks to the same server:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"using Mmap\n\nfunction generate()\n    path = \"myfile.bin\"\n    arr = Mmap.mmap(path, Matrix{Int}, (64,64))\n    fill!(arr, 1)\n    Mmap.sync!(arr)\n    # Note: Dagger.scope() does not yet support node scopes\n    Dagger.tochunk(path, Dagger.task_processor(), NodeScope())\nend\n\nfunction consume(path)\n    arr = Mmap.mmap(path, Matrix{Int}, (64,64))\n    sum(arr)\nend\n\na = Dagger.@spawn generate()\n@assert fetch(Dagger.@spawn consume(a)) == 64*64","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Whatever server a executed on, b will also execute on it!","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Finally, we come to the \"lowest\" scope on the scope hierarchy, the ExactScope. This scope specifies one exact processor as the bounding scope, and is typically useful in certain limited cases (such as data existing only on a specific GPU). We won't provide an example here, because you don't usually need to ever use this scope, but if you already understand the NodeScope and ProcessScope, the ExactScope should be easy to figure out.","category":"page"},{"location":"scopes/#Union-Scopes","page":"Scopes","title":"Union Scopes","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Sometimes one simple scope isn't enough! In that case, you can use the UnionScope to construct the union of two or more scopes. Say, for example, you have some sensitive data on your company's servers that you want to compute summaries of, but you'll be driving the computation from your laptop, and you aren't allowed to send the data itself outside of the company's network. You could accomplish this by constructing a UnionScope of ProcessScopes of each of the non-laptop Julia processes, and use that to ensure that the data in its original form always stays within the company network:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"addprocs(4) # some local processors\nprocs = addprocs([(\"server.company.com\", 4)]) # some company processors\n\nsecrets_scope = UnionScope(ProcessScope.(procs))\n\nfunction generate_secrets()\n    secrets = open(\"/shared/secret_results.txt\", \"r\") do io\n        String(read(io))\n    end\n    Dagger.tochunk(secrets, Dagger.task_processor(), secrets_scope)\nend\n\nsummarize(secrets) = occursin(\"QA Pass\", secrets)\n\n# Generate the data on the first company process\nsensitive_data = Dagger.@spawn single=first(procs) generate_secrets()\n\n# We can safely call this, knowing that it will be executed on a company server\nqa_passed = Dagger.@spawn summarize(sensitive_data)","category":"page"},{"location":"scopes/#Mismatched-Scopes","page":"Scopes","title":"Mismatched Scopes","text":"","category":"section"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"You might now be thinking, \"What if I want to run a task on multiple pieces of data whose scopes don't match up?\" In such a case, Dagger will throw an error, refusing to schedule that task, since the intersection of the data scopes is an empty set (there is no feasible processor which can satisfy the scoping constraints). For example:","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"ps2 = ProcessScope(2)\nps3 = ProcessScope(3)\n\ngenerate(scope) = Dagger.tochunk(rand(64), Dagger.task_processor(), scope)\n\nd2 = Dagger.@spawn generate(ps2) # Run on process 2\nd3 = Dagger.@spawn generate(ps3) # Run on process 3\nres = Dagger.@spawn d2 * d3 # An error!","category":"page"},{"location":"scopes/","page":"Scopes","title":"Scopes","text":"Moral of the story: only use scopes when you know you really need them, and if you aren't careful to arrange everything just right, be prepared for Dagger to refuse to schedule your tasks! Scopes should only be used to ensure correctness of your programs, and are not intended to be used to optimize the schedule that Dagger uses for your tasks, since restricting the scope of execution for tasks will necessarily reduce the optimizations that Dagger's scheduler can perform.","category":"page"},{"location":"logging-advanced/#Logging:-Advanced","page":"Logging: Advanced","title":"Logging: Advanced","text":"","category":"section"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"Dagger's scheduler keeps track of the important and potentially expensive actions it does, such as moving data between workers or executing thunks, and tracks how much time and memory allocations these operations consume, among other things. It does it through the TimespanLogging.jl package (which used to be directly integrated into Dagger). Saving this information somewhere accessible is disabled by default, but it's quite easy to turn it on, through two mechanisms.","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"The first is Dagger.enable_logging!, which provides an easy-to-use interface to both enable and configure logging. The defaults are usually sufficient for most users, but can be tweaked with keyword arguments.","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"The second is done by setting a \"log sink\" in the Dagger Context being used, as ctx.log_sink. These log sinks drive how Dagger's logging behaves, and are configurable by the user, without the need to tweak any of Dagger's internal code.","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"A variety of log sinks are built-in to TimespanLogging; the NoOpLog is the default log sink when one isn't explicitly specified, and disables logging entirely (to minimize overhead). There are currently two other log sinks of interest; the first and newer of the two is the MultiEventLog, which generates multiple independent log streams, one per \"consumer\" (details in the next section). This is the log sink that enable_logging! uses, as it's easily the most flexible. The second and older sink is the LocalEventLog, which is explained later in this document. Most users are recommended to use the MultiEventLog (ideally via enable_logging!) since it's far more flexible and extensible, and is more performant in general.","category":"page"},{"location":"logging-advanced/#MultiEventLog","page":"Logging: Advanced","title":"MultiEventLog","text":"","category":"section"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"The MultiEventLog is intended to be configurable to exclude unnecessary information, and to include any built-in or user-defined metrics. It stores a set of \"sub-log\" streams internally, appending a single element to each of them when an event is generated. This element can be called a \"sub-event\" (to distinguish it from the higher-level \"event\" that Dagger creates), and is created by a \"consumer\". A consumer is a function or callable struct that, when called with the Event object generated by TimespanLogging, returns a sub-event characterizing whatever information the consumer represents. For example, the Dagger.Events.BytesAllocd consumer calculates the total bytes allocated and live at any given time within Dagger, and returns the current value when called. Let's construct one:","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"ctx = Dagger.Sch.eager_context()\nml = TimespanLogging.MultiEventLog()\n\n# Add the BytesAllocd consumer to the log as `:bytes`\nml[:bytes] = Dagger.Events.BytesAllocd()\n\nctx.log_sink = ml","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"As we can see above, each consumer gets a unique name as a Symbol that identifies it. Now that the log sink is attached with a consumer, we can execute some Dagger tasks, and then collect the sub-events generated by BytesAllocd:","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"# Allocates memory for their results\nt1 = Dagger.@spawn 3*4\nfetch(Dagger.@spawn 1+t1)\nlog = Dagger.fetch_logs!(ctx)[1] # Get the logs for worker 1\n@show log[:bytes]","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"note: Note\nTimespanLogging.get_logs! clears out the event logs, so that old events don't mix with new ones from future DAGs.","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"You'll then see that some number of bytes are allocated and then freed during the process of executing and completing those tasks.","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"There are a variety of other consumers built-in to TimespanLogging and Dagger, under the TimespanLogging.Events and Dagger.Events modules, respectively; see Dagger Types and TimespanLogging Types for details.","category":"page"},{"location":"logging-advanced/","page":"Logging: Advanced","title":"Logging: Advanced","text":"The MultiEventLog also has a mechanism to call a set of functions, called \"aggregators\", after all consumers have been executed, and are passed the full set of log streams as a Dict{Symbol,Vector{Any}}. The only one currently shipped with TimespanLogging directly is the LogWindow, and DaggerWebDash.jl has the TableStorage which integrates with it; see DaggerWebDash Types for details.","category":"page"},{"location":"api-dagger/functions/#Dagger-Functions","page":"Functions and Macros","title":"Dagger Functions","text":"","category":"section"},{"location":"api-dagger/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"api-dagger/functions/#Task-Functions/Macros","page":"Functions and Macros","title":"Task Functions/Macros","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.@spawn","page":"Functions and Macros","title":"Dagger.@spawn","text":"Dagger.@spawn [option=value]... f(args...; kwargs...) -> DTask\n\nSpawns a Dagger DTask that will call f(args...; kwargs...). This DTask is like a Julia Task, and has many similarities:\n\nThe DTask can be wait'd on and fetch'd from to see its final result\nBy default, the DTask will be automatically run on the first available compute resource\nIf all dependencies are satisfied, the DTask will be run as soon as possible\nThe DTask may be run in parallel with other DTasks, and the scheduler will automatically manage dependencies\nIf a DTask throws an exception, it will be propagated to any calls to fetch, but not to calls to wait\n\nHowever, the DTask also has many key differences from a Task:\n\nThe DTask may run on any thread of any Julia process, and even on a remote machine, in your cluster (see Distributed.addprocs)\nThe DTask might automatically utilize GPUs or other accelerators, if available\nIf arguments to a DTask are also DTasks, then the scheduler will execute those arguments' DTasks first, before running the \"downstream\" task\nIf an argument to a DTask t2 is a DTask t1, then the result of t1 (gotten via fetch(t1)) will be passed to t2 (no need for t2 to call fetch!)\nDTasks are generally expected to be defined \"functionally\", meaning that they should not mutate global state, mutate their arguments, or have side effects\nDTasks are function call-focused, meaning that Dagger.@spawn expects a single function call, and not a block of code\nAll DTask arguments are expected to be safe to serialize and send to other Julia processes; if not, use the scope option or Dagger.@mutable to control execution location\n\nOptions to the DTask can be set before the call to f with key-value syntax, e.g. Dagger.@spawn myopt=2 do_something(1, 3.0), which would set the option myopt to 2 for this task. Multiple options may be provided, which are specified like Dagger.@spawn myopt=2 otheropt=4 do_something(1, 3.0).\n\nThese options control a variety of properties of the resulting DTask:\n\nscope: The execution \"scope\" of the task, which determines where the task will run. By default, the task will run on the first available compute resource. If you have multiple compute resources, you can specify a scope to run the task on a specific resource. For example, Dagger.@spawn scope=Dagger.scope(worker=2) do_something(1, 3.0) would run do_something(1, 3.0) on worker 2.\nmeta: If true, instead of the scheduler automatically fetching values from other tasks, the raw Chunk objects will be passed to f. Useful for doing manual fetching or manipulation of Chunk references. Non-Chunk arguments are still passed as-is.\n\nOther options exist; see Dagger.Options for the full list.\n\nThis macro is a semi-thin wrapper around Dagger.spawn - it creates a call to Dagger.spawn on f with arguments args and keyword arguments kwargs, and also passes along any options in an Options struct. For example, Dagger.@spawn myopt=2 do_something(1, 3.0) would essentially become Dagger.spawn(do_something, Dagger.Options(;myopt=2), 1, 3.0).\n\n\n\n\n\n","category":"macro"},{"location":"api-dagger/functions/#Dagger.spawn","page":"Functions and Macros","title":"Dagger.spawn","text":"Dagger.spawn(f, args...; kwargs...) -> DTask\n\nSpawns a DTask that will call f(args...; kwargs...). Also supports passing a Dagger.Options struct as the first argument to set task options. See Dagger.@spawn for more details on DTasks.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Task-Options-Functions/Macros","page":"Functions and Macros","title":"Task Options Functions/Macros","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.with_options","page":"Functions and Macros","title":"Dagger.with_options","text":"with_options(f, options::NamedTuple) -> Any\nwith_options(f; options...) -> Any\n\nSets one or more scoped options to the given values, executes f(), resets the options to their previous values, and returns the result of f(). This is the recommended way to set scoped options, as it only affects tasks spawned within its scope. Note that setting an option here will propagate its value across Julia or Dagger tasks spawned by f() or its callees (i.e. the options propagate).\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.get_options","page":"Functions and Macros","title":"Dagger.get_options","text":"get_options(key::Symbol, default) -> Any\nget_options(key::Symbol) -> Any\n\nReturns the value of the scoped option named key. If option does not have a value set, then an error will be thrown, unless default is set, in which case it will be returned instead of erroring.\n\nget_options() -> NamedTuple\n\nReturns a NamedTuple of all scoped option key-value pairs.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.@option","page":"Functions and Macros","title":"Dagger.@option","text":"@option name myfunc(A, B, C) = value\n\nA convenience macro for defining default_option. For example:\n\nDagger.@option single mylocalfunc(Int) = 1\n\nThe above call will set the single option to 1 for any Dagger task calling mylocalfunc(Int) with an Int argument.\n\n\n\n\n\n","category":"macro"},{"location":"api-dagger/functions/#Dagger.default_option","page":"Functions and Macros","title":"Dagger.default_option","text":"default_option(::Val{name}, Tf, Targs...) where name = value\n\nDefines the default value for option name to value when Dagger is preparing to execute a function with type Tf with the argument types Targs. Users and libraries may override this to set default values for tasks.\n\nAn easier way to define these defaults is with @option.\n\nNote that the actual task's argument values are not passed, as it may not always be possible or efficient to gather all Dagger task arguments on one worker.\n\nThis function may be executed within the scheduler, so it should generally be made very cheap to execute. If the function throws an error, the scheduler will use whatever the global default value is for that option instead.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Data-Management-Functions","page":"Functions and Macros","title":"Data Management Functions","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.tochunk","page":"Functions and Macros","title":"Dagger.tochunk","text":"tochunk(x, proc::Processor, scope::AbstractScope; device=nothing, rewrap=false, kwargs...) -> Chunk\n\nCreate a chunk from data x which resides on proc and which has scope scope.\n\ndevice specifies a MemPool.StorageDevice (which is itself wrapped in a Chunk) which will be used to manage the reference contained in the Chunk generated by this function. If device is nothing (the default), the data will be inspected to determine if it's safe to serialize; if so, the default MemPool storage device will be used; if not, then a MemPool.CPURAMDevice will be used.\n\nIf rewrap==true and x isa Chunk, then the Chunk will be rewrapped in a new Chunk.\n\nAll other kwargs are passed directly to MemPool.poolset.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.mutable","page":"Functions and Macros","title":"Dagger.mutable","text":"mutable(f::Base.Callable; worker, processor, scope) -> Chunk\n\nCalls f() on the specified worker or processor, returning a Chunk referencing the result with the specified scope scope.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.@mutable","page":"Functions and Macros","title":"Dagger.@mutable","text":"@mutable [worker=1] [processor=OSProc()] [scope=ProcessorScope()] f()\n\nHelper macro for mutable().\n\n\n\n\n\n","category":"macro"},{"location":"api-dagger/functions/#Dagger.@shard","page":"Functions and Macros","title":"Dagger.@shard","text":"Creates a Shard. See Dagger.shard for details.\n\n\n\n\n\n","category":"macro"},{"location":"api-dagger/functions/#Dagger.shard","page":"Functions and Macros","title":"Dagger.shard","text":"shard(f; kwargs...) -> Chunk{Shard}\n\nExecutes f on all workers in workers, wrapping the result in a process-scoped Chunk, and constructs a Chunk{Shard} containing all of these Chunks on the current worker.\n\nKeyword arguments:\n\nprocs – The list of processors to create pieces on. May be any iterable container of Processors.\nworkers – The list of workers to create pieces on. May be any iterable container of Integers.\nper_thread::Bool=false – If true, creates a piece per each thread, rather than a piece per each worker.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Data-Dependencies-Functions","page":"Functions and Macros","title":"Data Dependencies Functions","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.spawn_datadeps","page":"Functions and Macros","title":"Dagger.spawn_datadeps","text":"spawn_datadeps(f::Base.Callable; traversal::Symbol=:inorder)\n\nConstructs a \"datadeps\" (data dependencies) region and calls f within it. Dagger tasks launched within f may wrap their arguments with In, Out, or InOut to indicate whether the task will read, write, or read+write that argument, respectively. These argument dependencies will be used to specify which tasks depend on each other based on the following rules:\n\nDependencies across unrelated arguments are independent; only dependencies on arguments which overlap in memory synchronize with each other\nInOut is the same as In and Out applied simultaneously, and synchronizes with the union of the In and Out effects\nAny two or more In dependencies do not synchronize with each other, and may execute in parallel\nAn Out dependency synchronizes with any previous In and Out dependencies\nAn In dependency synchronizes with any previous Out dependencies\nIf unspecified, an In dependency is assumed\n\nIn general, the result of executing tasks following the above rules will be equivalent to simply executing tasks sequentially and in order of submission. Of course, if dependencies are incorrectly specified, undefined behavior (and unexpected results) may occur.\n\nUnlike other Dagger tasks, tasks executed within a datadeps region are allowed to write to their arguments when annotated with Out or InOut appropriately.\n\nAt the end of executing f, spawn_datadeps will wait for all launched tasks to complete, rethrowing the first error, if any. The result of f will be returned from spawn_datadeps.\n\nThe keyword argument traversal controls the order that tasks are launched by the scheduler, and may be set to :bfs or :dfs for Breadth-First Scheduling or Depth-First Scheduling, respectively. All traversal orders respect the dependencies and ordering of the launched tasks, but may provide better or worse performance for a given set of datadeps tasks. This argument is experimental and subject to change.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Scope-Functions","page":"Functions and Macros","title":"Scope Functions","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.scope","page":"Functions and Macros","title":"Dagger.scope","text":"scope(scs...) -> AbstractScope\nscope(;scs...) -> AbstractScope\n\nConstructs an AbstractScope from a set of scope specifiers. Each element in scs is a separate specifier; if scs is empty, an empty UnionScope() is produced; if scs has one element, then exactly one specifier is constructed; if scs has more than one element, a UnionScope of the scopes specified by scs is constructed. A variety of specifiers can be passed to construct a scope:\n\n:any - Constructs an AnyScope()\n:default - Constructs a DefaultScope()\n(scs...,) - Constructs a UnionScope of scopes, each specified by scs\nthread=tid or threads=[tids...] - Constructs an ExactScope or UnionScope containing all Dagger.ThreadProcs with thread ID tid/tids across all workers.\nworker=wid or workers=[wids...] - Constructs a ProcessScope or UnionScope containing all Dagger.ThreadProcs with worker ID wid/wids across all threads.\nthread=tid/threads=tids and worker=wid/workers=wids - Constructs an ExactScope, ProcessScope, or UnionScope containing all Dagger.ThreadProcs with worker ID wid/wids and threads tid/tids.\n\nAside from the worker and thread specifiers, it's possible to add custom specifiers for scoping to other kinds of processors (like GPUs) or providing different ways to specify a scope. Specifier selection is determined by a precedence ordering: by default, all specifiers have precedence 0, which can be changed by defining scope_key_precedence(::Val{spec}) = precedence (where spec is the specifier as a Symbol). The specifier with the highest precedence in a set of specifiers is used to determine the scope by calling to_scope(::Val{spec}, sc::NamedTuple) (where sc is the full set of specifiers), which should be overriden for each custom specifier, and which returns an AbstractScope. For example:\n\n# Setup a GPU specifier\nDagger.scope_key_precedence(::Val{:gpu}) = 1\nDagger.to_scope(::Val{:gpu}, sc::NamedTuple) = ExactScope(MyGPUDevice(sc.worker, sc.gpu))\n\n# Generate an `ExactScope` for `MyGPUDevice` on worker 2, device 3\nDagger.scope(gpu=3, worker=2)\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.constrain","page":"Functions and Macros","title":"Dagger.constrain","text":"constraint(x::AbstractScope, y::AbstractScope) -> ::AbstractScope\n\nConstructs a scope that is the intersection of scopes x and y.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Processor-Functions","page":"Functions and Macros","title":"Processor Functions","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.execute!","page":"Functions and Macros","title":"Dagger.execute!","text":"execute!(proc::Processor, f, args...; kwargs...) -> Any\n\nExecutes the function f with arguments args and keyword arguments kwargs on processor proc. This function can be overloaded by Processor subtypes to allow executing function calls differently than normal Julia.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.iscompatible","page":"Functions and Macros","title":"Dagger.iscompatible","text":"iscompatible(proc::Processor, opts, f, Targs...) -> Bool\n\nIndicates whether proc can execute f over Targs given opts. Processor subtypes should overload this function to return true if and only if it is essentially guaranteed that f(::Targs...) is supported. Additionally, iscompatible_func and iscompatible_arg can be overriden to determine compatibility of f and Targs individually. The default implementation returns false.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.default_enabled","page":"Functions and Macros","title":"Dagger.default_enabled","text":"default_enabled(proc::Processor) -> Bool\n\nReturns whether processor proc is enabled by default. The default value is false, which is an opt-out of the processor from execution when not specifically requested by the user, and true implies opt-in, which causes the processor to always participate in execution when possible.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.get_processors","page":"Functions and Macros","title":"Dagger.get_processors","text":"get_processors(proc::Processor) -> Set{<:Processor}\n\nReturns the set of processors contained in proc, if any. Processor subtypes should overload this function if they can contain sub-processors. The default method will return a Set containing proc itself.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.get_parent","page":"Functions and Macros","title":"Dagger.get_parent","text":"get_parent(proc::Processor) -> Processor\n\nReturns the parent processor for proc. The ultimate parent processor is an OSProc. Processor subtypes should overload this to return their most direct parent.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.move","page":"Functions and Macros","title":"Dagger.move","text":"move(from_proc::Processor, to_proc::Processor, x)\n\nMoves and/or converts x such that it's available and suitable for usage on the to_proc processor. This function can be overloaded by Processor subtypes to transport arguments and convert them to an appropriate form before being used for exection. Subtypes of Processor wishing to implement efficient data movement should provide implementations where x::Chunk.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.get_tls","page":"Functions and Macros","title":"Dagger.get_tls","text":"get_tls() -> DTaskTLS\n\nGets all Dagger TLS variable as a DTaskTLS.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.set_tls!","page":"Functions and Macros","title":"Dagger.set_tls!","text":"set_tls!(tls::NamedTuple)\n\nSets all Dagger TLS variables from tls, which may be a DTaskTLS or a NamedTuple.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Context-Functions","page":"Functions and Macros","title":"Context Functions","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.addprocs!","page":"Functions and Macros","title":"Dagger.addprocs!","text":"addprocs!(ctx::Context, xs)\n\nAdd new workers xs to ctx.\n\nWorkers will typically be assigned new tasks in the next scheduling iteration if scheduling is ongoing.\n\nWorkers can be either Processors or the underlying process IDs as Integers.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.rmprocs!","page":"Functions and Macros","title":"Dagger.rmprocs!","text":"rmprocs!(ctx::Context, xs)\n\nRemove the specified workers xs from ctx.\n\nWorkers will typically finish all their assigned tasks if scheduling is ongoing but will not be assigned new tasks after removal.\n\nWorkers can be either Processors or the underlying process IDs as Integers.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Distributed-Package-Selection-Functions","page":"Functions and Macros","title":"Distributed Package Selection Functions","text":"","category":"section"},{"location":"api-dagger/functions/#Dagger.set_distributed_package!","page":"Functions and Macros","title":"Dagger.set_distributed_package!","text":"set_distributed_package!(value[=\"Distributed|DistributedNext\"])\n\nSet a preference for using either the Distributed.jl stdlib or DistributedNext.jl. You will need to restart Julia after setting a new preference.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#DTask-Execution-Environment-Functions","page":"Functions and Macros","title":"DTask Execution Environment Functions","text":"","category":"section"},{"location":"api-dagger/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"These functions are used within the function called by a DTask.","category":"page"},{"location":"api-dagger/functions/#Dagger.in_task","page":"Functions and Macros","title":"Dagger.in_task","text":"in_task() -> Bool\n\nReturns true if currently executing in a DTask, else false.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.task_processor","page":"Functions and Macros","title":"Dagger.task_processor","text":"task_processor() -> Processor\n\nGet the current processor executing the current DTask.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dynamic-Scheduler-Control-Functions","page":"Functions and Macros","title":"Dynamic Scheduler Control Functions","text":"","category":"section"},{"location":"api-dagger/functions/","page":"Functions and Macros","title":"Functions and Macros","text":"These functions query and control the scheduler remotely.","category":"page"},{"location":"api-dagger/functions/#Dagger.Sch.sch_handle","page":"Functions and Macros","title":"Dagger.Sch.sch_handle","text":"Gets the scheduler handle for the currently-executing thunk.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.Sch.add_thunk!","page":"Functions and Macros","title":"Dagger.Sch.add_thunk!","text":"Adds a new Thunk to the DAG.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Base.fetch","page":"Functions and Macros","title":"Base.fetch","text":"Base.fetch(c::DArray)\n\nIf a DArray tree has a Thunk in it, make the whole thing a big thunk.\n\n\n\n\n\nWaits on a thunk to complete, and fetches its result.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Base.wait","page":"Functions and Macros","title":"Base.wait","text":"Waits on a thunk to complete.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.Sch.exec!","page":"Functions and Macros","title":"Dagger.Sch.exec!","text":"Executes an arbitrary function within the scheduler, returning the result.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.Sch.halt!","page":"Functions and Macros","title":"Dagger.Sch.halt!","text":"Commands the scheduler to halt execution immediately.\n\n\n\n\n\n","category":"function"},{"location":"api-dagger/functions/#Dagger.Sch.get_dag_ids","page":"Functions and Macros","title":"Dagger.Sch.get_dag_ids","text":"Returns all Thunks IDs as a Dict, mapping a Thunk to its downstream dependents.\n\n\n\n\n\n","category":"function"},{"location":"data-management/#Data-Management","page":"Data Management","title":"Data Management","text":"","category":"section"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Dagger is not just a computing platform - it also has awareness of where each piece of data resides, and will move data between workers and perform conversions as necessary to satisfy the needs of your tasks.","category":"page"},{"location":"data-management/#Chunks","page":"Data Management","title":"Chunks","text":"","category":"section"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Dagger often needs to move data between workers to allow a task to execute. To make this efficient when communicating potentially large units of data, Dagger uses a remote reference, called a Chunk, to refer to objects which may exist on another worker. Chunks are backed by a distributed refcounting mechanism provided by MemPool.jl, which ensures that the referenced data is not garbage collected until all Chunks referencing that object are GC'd from all workers.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Conveniently, if you pass in a Chunk object as an input to a Dagger task, then the task's payload function will get executed with the value contained in the Chunk. The scheduler also understands Chunks, and will try to schedule tasks close to where their Chunk inputs reside, to reduce communication overhead.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Chunks also have a cached type, a \"processor\", and a \"scope\", which are important for identifying the type of the object, where in memory (CPU RAM, GPU VRAM, etc.) the value resides, and where the value is allowed to be transferred and dereferenced. See Processors and Scopes for more details on how these properties can be used to control scheduling behavior around Chunks.","category":"page"},{"location":"data-management/#Data-movement-rules","page":"Data Management","title":"Data movement rules","text":"","category":"section"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Dagger utilizes a 3-argument function Dagger.move(from_proc::Dagger.Processor, to_proc::Dagger.Processor, x) to manage data movement between processors. This function is invoked by the scheduler for every argument of a task, including the task's function itself, before the task is executed. The purpose of move is to transfer the argument x from its current processor (from_proc) to the target processor (to_proc) where the task will run, and to perform any necessary data conversion or unwrapping before execution.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"This move mechanism is fundamental to how Dagger handles Chunk objects. When a Chunk is passed as an argument to a task, the move function is responsible for unwrapping the Chunk and providing its underlying value to the task.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"While users can define custom move implementations for their specific data types if needed, the default fallback implementation of move is designed to handle most common use cases effectively. Therefore, custom implementations are generally unnecessary.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Here's an example of a custom move implementation:","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"struct MyCustomType\n    data::Vector{Float64}\nend\n\n# Custom move function for MyCustomType\nfunction Dagger.move(from_proc::Dagger.Processor, to_proc::Dagger.Processor, x::MyCustomType)\n    return x.data\nend\n\nA = MyCustomType(rand(100))\ns = fetch(Dagger.@spawn sum(A))\n@assert s == sum(A.data)","category":"page"},{"location":"data-management/#Mutation","page":"Data Management","title":"Mutation","text":"","category":"section"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Normally, Dagger tasks should be functional and \"pure\": never mutating their inputs, always producing identical outputs for a given set of inputs, and never producing side effects which might affect future program behavior. However, for certain codes, this restriction ends up costing the user performance and engineering time to work around.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Thankfully, Dagger provides the Dagger.@mutable macro for just this purpose. @mutable allows data to be marked such that it will never be copied or serialized by the scheduler (unless copied by the user). When used as an argument to a task, the task will be forced to execute on the same worker that @mutable was called on. For example:","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"x = Dagger.@mutable worker=2 Threads.Atomic{Int}(0)\nx::Dagger.Chunk # The result is always a `Chunk`\n\n# x is now considered mutable, and may only be accessed on worker 2:\nwait(Dagger.@spawn Threads.atomic_add!(x, 1)) # Always executed on worker 2\nwait(Dagger.@spawn scope=Dagger.scope(worker=1) Threads.atomic_add!(x, 1)) # SchedulingException","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"@mutable, when called as above, is constructed on worker 2, and the data gains a scope of ProcessScope(myid()), which means that any processor on that worker is allowed to execute tasks that use the object (subject to the usual scheduling rules).","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"@mutable also allows the scope to be manually supplied, if more specific restrictions are desirable:","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"x = @mutable scope=Dagger.scope(worker=1, threads=[3,4]) rand(100)\n# x is now scoped to threads 3 and 4 on worker `myid()`","category":"page"},{"location":"data-management/#Sharding","page":"Data Management","title":"Sharding","text":"","category":"section"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"@mutable is convenient for creating a single mutable object, but often one wants to have multiple mutable objects, with each object being scoped to their own worker or thread in the cluster, to be used as local counters, partial reduction containers, data caches, etc.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"The Shard object (constructed with Dagger.@shard/Dagger.shard) is a mechanism by which such a setup can be created with one invocation.  By default, each worker will have their own local object which will be used when a task that uses the shard as an argument is scheduled on that worker. Other shard pieces that aren't scoped to the processor being executed on will not be serialized or copied, keeping communication costs constant even with a very large shard.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"This mechanism makes it easy to construct a distributed set of mutable objects which are treated as \"mirrored shards\" by the scheduler, but require no further user input to access. For example, creating and using a local counter for each worker is trivial:","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"# Create a local atomic counter on each worker that Dagger knows about:\ncs = Dagger.@shard Threads.Atomic{Int}(0)\n\n# Let's add `1` to the local counter, not caring about which worker we're on:\nwait.([Dagger.@spawn Threads.atomic_add!(cs, 1) for i in 1:1000])\n\n# And let's fetch the total sum of all counters:\n@assert sum(map(ctr->fetch(ctr)[], cs)) == 1000","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Note that map, when used on a shard, will execute the provided function once per shard \"piece\", and each result is considered immutable. map is an easy way to make a copy of each piece of the shard, to be later reduced, scanned, etc.","category":"page"},{"location":"data-management/","page":"Data Management","title":"Data Management","text":"Further details about what arguments can be passed to @shard/shard can be found in Data Management Functions.","category":"page"},{"location":"processors/#Processors","page":"Processors","title":"Processors","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger contains a flexible mechanism to represent CPUs, GPUs, and other devices that the scheduler can place user work on. The individual devices that are capable of computing a user operation are called \"processors\", and are subtypes of Dagger.Processor. Processors are automatically detected by Dagger at scheduler initialization, and placed in a hierarchy reflecting the physical (network-, link-, or memory-based) boundaries between processors in the hierarchy. The scheduler uses the information in this hierarchy to efficiently schedule and partition user operations.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger's Chunk objects can have a processor associated with them that defines where the contained data \"resides\". Each processor has a set of functions that define the mechanisms and rules by which the data can be transferred between similar or different kinds of processors, and will be called by Dagger's scheduler automatically when fetching function arguments (or the function itself) for computation on a given processor.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Setting the processor on a function argument is done by wrapping it in a Chunk with Dagger.tochunk:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"a = 1\nb = 2\n# Let's say `b` \"resides\" on the second thread of the first worker:\nb_chunk = Dagger.tochunk(b, Dagger.ThreadProc(1, 2))::Dagger.Chunk\nc = Dagger.@spawn a + b_chunk\nfetch(c) == 3","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"It's also simple to set the processor of the function being passed; it will be automatically wrapped in a Chunk if necessary:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"# `+` is treated as existing on the second thread of the first worker:\nDagger.@spawn processor=Dagger.ThreadProc(1, 2) a + b","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"You can also tell Dagger about the processor type for the returned value of a task by making it a Chunk:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger.spawn(a) do a\n    c = a + 1\n    return Dagger.tochunk(c, Dagger.ThreadProc(1, 2))\nend","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Note that unless you know that your function, arguments, or return value are associated with a specific processor, you don't need to assign one to them. Dagger will treat them as being simple values with no processor association, and will serialize them to wherever they're used.","category":"page"},{"location":"processors/#Hardware-capabilities,-topology,-and-data-locality","page":"Processors","title":"Hardware capabilities, topology, and data locality","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"The processor hierarchy is modeled as a multi-root tree, where each root is an OSProc, which represents a Julia OS process, and the \"children\" of the root or some other branch in the tree represent the processors which reside on the same logical server as the \"parent\" branch. All roots are connected to each other directly, in the common case. The processor hierarchy's topology is automatically detected and elaborated by callbacks in Dagger, which users may manipulate to add detection of extra processors.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"A move between a given pair of processors is implemented as a Julia function dispatching on the types of each processor, as well as the type of the data being moved. Users are permitted to define custom move functions to improve data movement efficiency, perform automatic value conversions, or even make use of special IPC facilities. Custom processors may also be defined by the user to represent a processor type which is not automatically detected by Dagger, such as novel GPUs, special OS process abstractions, FPGAs, etc.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Movement of data between any two processors A and B (from A to B), if not defined by the user, is decomposed into 3 moves: processor A to OSProc parent of A, OSProc parent of A to OSProc parent of B, and OSProc parent of B to processor B. This mechanism uses Julia's Serialization library to serialize and deserialize data, so data must be serializable for this mechanism to work properly.","category":"page"},{"location":"processors/#Processor-Selection","page":"Processors","title":"Processor Selection","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"By default, Dagger uses the CPU to process work, typically single-threaded per cluster node. However, Dagger allows access to a wider range of hardware and software acceleration techniques, such as multithreading and GPUs. These more advanced (but performant) accelerators are disabled by default, but can easily be enabled by using scopes (see Scopes for details).","category":"page"},{"location":"processors/#Resource-Control","page":"Processors","title":"Resource Control","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger assumes that a thunk executing on a processor, fully utilizes that processor at 100%. When this is not the case, you can tell Dagger as much with options.procutil:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"procutil = Dict(\n    Dagger.ThreadProc => 4.0, # utilizes 4 CPU threads fully\n    DaggerGPU.CuArrayProc => 0.1 # utilizes 10% of a single CUDA GPU\n)","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger will use this information to execute only as many thunks on a given processor (or set of similar processors) as add up to less than or equal to 1.0 total utilization. If a thunk is scheduled onto a processor which the local worker deems as \"oversubscribed\", it will not execute the thunk until sufficient resources become available by thunks completing execution.","category":"page"},{"location":"processors/#GPU-Processors","page":"Processors","title":"GPU Processors","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"The DaggerGPU.jl package can be imported to enable GPU acceleration for NVIDIA and AMD GPUs, when available. The processors provided by that package are not enabled by default, but may be enabled via custom scopes (Scopes).","category":"page"},{"location":"processors/#Future:-Network-Devices-and-Topology","page":"Processors","title":"Future: Network Devices and Topology","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"In the future, users will be able to define network devices attached to a given processor, which provides a direct connection to a network device on another processor, and may be used to transfer data between said processors. Data movement rules will most likely be defined by a similar (or even identical) mechanism to the current processor move mechanism. The multi-root tree will be expanded to a graph to allow representing these network devices (as they may potentially span non-root nodes).","category":"page"},{"location":"processors/#Redundancy","page":"Processors","title":"Redundancy","text":"","category":"section"},{"location":"processors/#Fault-Tolerance","page":"Processors","title":"Fault Tolerance","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger has a single means for ensuring redundancy, which is currently called \"fault tolerance\". Said redundancy is only targeted at a specific failure mode, namely the unexpected exit or \"killing\" of a worker process in the cluster. This failure mode often presents itself when running on a Linux and generating large memory allocations, where the Out Of Memory (OOM) killer process can kill user processes to free their allocated memory for the Linux kernel to use. The fault tolerance system mitigates the damage caused by the OOM killer performing its duties on one or more worker processes by detecting the fault as a process exit exception (generated by Julia), and then moving any \"lost\" work to other worker processes for re-computation.","category":"page"},{"location":"processors/#Future:-Multi-master,-Network-Failure-Correction,-etc.","page":"Processors","title":"Future: Multi-master, Network Failure Correction, etc.","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"This single redundancy mechanism helps alleviate a common issue among HPC and scientific users, however it does little to help when, for example, the master node exits, or a network link goes down. Such failure modes require a more complicated detection and recovery process, including multiple master processes, a distributed and replicated database such as etcd, and checkpointing of the scheduler to ensure an efficient recovery. Such a system does not yet exist, but contributions for such a change are desired.","category":"page"},{"location":"processors/#Dynamic-worker-pools","page":"Processors","title":"Dynamic worker pools","text":"","category":"section"},{"location":"processors/","page":"Processors","title":"Processors","text":"Dagger's default scheduler supports modifying the worker pool while the scheduler is running. This is done by modifying the Processors of the Context supplied to the scheduler at initialization using addprocs!(ctx, ps) and rmprocs(ctx, ps) where ps can be Processors or just process ids.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"An example of when this is useful is in HPC environments where individual jobs to start up workers are queued so that not all workers are guaranteed to be available at the same time.","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"New workers will typically be assigned new tasks as soon as the scheduler sees them. Removed workers will finish all their assigned tasks but will not be assigned any new tasks. Note that this makes it difficult to determine when a worker is no longer in use by Dagger. Contributions to alleviate this uncertainty are welcome!","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"Example:","category":"page"},{"location":"processors/","page":"Processors","title":"Processors","text":"using Distributed\n\nps1 = addprocs(2, exeflags=\"--project\")\n@everywhere using Distributed, Dagger\n\n# Dummy task to wait for 0.5 seconds and then return the id of the worker\nts = delayed(vcat)((delayed(i -> (sleep(0.5); myid()))(i) for i in 1:20)...)\n\nctx = Context()\n# Scheduler is blocking, so we need a new task to add workers while it runs\njob = @async collect(ctx, ts)\n\n# Lets fire up some new workers\nps2 = addprocs(2, exeflags=\"--project\")\n@everywhere ps2 using Distributed, Dagger\n# New workers are not available until we do this\naddprocs!(ctx, ps2)\n\n# Lets hope the job didn't complete before workers were added :)\n@show fetch(job) |> unique\n\n# and cleanup after ourselves...\nworkers() |> rmprocs","category":"page"},{"location":"stencils/#Stencil-Operations","page":"Stencils","title":"Stencil Operations","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"The @stencil macro in Dagger.jl provides a convenient way to perform stencil computations on DArrays. It operates within a Dagger.spawn_datadeps() block and allows you to define operations that apply to each element of a DArray, potentially accessing values from each element's neighbors.","category":"page"},{"location":"stencils/#Basic-Usage","page":"Stencils","title":"Basic Usage","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"The fundamental structure of a @stencil block involves iterating over an implicit index, named idx in the following example , which represents the coordinates of an element in the processed DArrays.","category":"page"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"using Dagger\nimport Dagger: @stencil, Wrap, Pad\n\n# Initialize a DArray\nA = zeros(Blocks(2, 2), Int, 4, 4)\n\nDagger.spawn_datadeps() do\n    @stencil begin\n        A[idx] = 1 # Assign 1 to every element of A\n    end\nend\n\n@assert all(collect(A) .== 1)","category":"page"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"In this example, A[idx] = 1 is executed for each chunk of A. The idx variable corresponds to the indices within each chunk.","category":"page"},{"location":"stencils/#Neighborhood-Access-with-@neighbors","page":"Stencils","title":"Neighborhood Access with @neighbors","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"The true power of stencils comes from accessing neighboring elements. The @neighbors macro facilitates this.","category":"page"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"@neighbors(array[idx], distance, boundary_condition)","category":"page"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"array[idx]: The array and current index from which to find neighbors.\ndistance: An integer specifying the extent of the neighborhood (e.g., 1 for a 3x3 neighborhood in 2D).\nboundary_condition: Defines how to handle accesses beyond the array boundaries. Available conditions are:\nWrap(): Wraps around to the other side of the array.\nPad(value): Pads with a specified value.","category":"page"},{"location":"stencils/#Example:-Averaging-Neighbors-with-Wrap","page":"Stencils","title":"Example: Averaging Neighbors with Wrap","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"import Dagger: Wrap\n\n# Initialize a DArray\nA = ones(Blocks(1, 1), Int, 3, 3)\nA[2,2] = 10 # Central element has a different value\nB = zeros(Blocks(1, 1), Float64, 3, 3)\n\nDagger.spawn_datadeps() do\n    @stencil begin\n        # Calculate the average of the 3x3 neighborhood (including the center)\n        B[idx] = sum(@neighbors(A[idx], 1, Wrap())) / 9.0\n    end\nend\n\n# Manually calculate expected B for verification\nexpected_B = zeros(Float64, 3, 3)\nA_collected = collect(A)\nfor r in 1:3, c in 1:3\n    local_sum = 0.0\n    for dr in -1:1, dc in -1:1\n        nr, nc = mod1(r+dr, 3), mod1(c+dc, 3)\n        local_sum += A_collected[nr, nc]\n    end\n    expected_B[r,c] = local_sum / 9.0\nend\n\n@assert collect(B) ≈ expected_B","category":"page"},{"location":"stencils/#Example:-Convolution-with-Pad","page":"Stencils","title":"Example: Convolution with Pad","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"import Pad\n\n# Initialize a DArray\nA = ones(Blocks(2, 2), Int, 4, 4)\nB = zeros(Blocks(2, 2), Int, 4, 4)\n\nDagger.spawn_datadeps() do\n    @stencil begin\n        B[idx] = sum(@neighbors(A[idx], 1, Pad(0))) # Pad with 0\n    end\nend\n\n# Expected result for a 3x3 sum filter with zero padding\nexpected_B_padded = [\n    4 6 6 4;\n    6 9 9 6;\n    6 9 9 6;\n    4 6 6 4\n]\n@assert collect(B) == expected_B_padded","category":"page"},{"location":"stencils/#Sequential-Semantics","page":"Stencils","title":"Sequential Semantics","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"Expressions within a @stencil block are executed sequentially in terms of their effect on the data. This means that the result of one statement is visible to the subsequent statements, as if they were applied \"all at once\" across all indices before the next statement begins.","category":"page"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"A = zeros(Blocks(2, 2), Int, 4, 4)\nB = zeros(Blocks(2, 2), Int, 4, 4)\n\nDagger.spawn_datadeps() do\n    @stencil begin\n        A[idx] = 1  # First, A is initialized\n        B[idx] = A[idx] * 2       # Then, B is computed using the new values of A\n    end\nend\n\nexpected_A = [1 for r in 1:4, c in 1:4]\nexpected_B_seq = expected_A .* 2\n\n@assert collect(A) == expected_A\n@assert collect(B) == expected_B_seq","category":"page"},{"location":"stencils/#Operations-on-Multiple-DArrays","page":"Stencils","title":"Operations on Multiple DArrays","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"You can read from and write to multiple DArrays within a single @stencil block, provided they have compatible chunk structures.","category":"page"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"A = ones(Blocks(1, 1), Int, 2, 2)\nB = DArray(fill(3, 2, 2), Blocks(1, 1))\nC = zeros(Blocks(1, 1), Int, 2, 2)\n\nDagger.spawn_datadeps() do\n    @stencil begin\n        C[idx] = A[idx] + B[idx]\n    end\nend\n@assert all(collect(C) .== 4)","category":"page"},{"location":"stencils/#Example:-Game-of-Life","page":"Stencils","title":"Example: Game of Life","text":"","category":"section"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"The following demonstrates a more complex example: Conway's Game of Life.","category":"page"},{"location":"stencils/","page":"Stencils","title":"Stencils","text":"# Ensure Plots and other necessary packages are available for the example\nusing Plots\n\nN = 27 # Size of one dimension of a tile\nnt = 3 # Number of tiles in each dimension (results in nt x nt grid of tiles)\nniters = 10 # Number of iterations for the animation\n\ntiles = zeros(Blocks(N, N), Bool, N*nt, N*nt)\noutputs = zeros(Blocks(N, N), Bool, N*nt, N*nt)\n\n# Create a fun initial state (e.g., a glider and some random noise)\ntiles[13, 14] = true\ntiles[14, 14] = true\ntiles[15, 14] = true\ntiles[15, 15] = true\ntiles[14, 16] = true\n# Add some random noise in one of the tiles\n@view(tiles[(2N+1):3N, (2N+1):3N]) .= rand(Bool, N, N)\n\n\n\nanim = @animate for _ in 1:niters\n    Dagger.spawn_datadeps() do\n        @stencil begin\n            outputs[idx] = begin\n                nhood = @neighbors(tiles[idx], 1, Wrap())\n                neighs = sum(nhood) - tiles[idx] # Sum neighborhood, but subtract own value\n                if tiles[idx] && neighs < 2\n                    0 # Dies of underpopulation\n                elseif tiles[idx] && neighs > 3\n                    0 # Dies of overpopulation\n                elseif !tiles[idx] && neighs == 3\n                    1 # Becomes alive by reproduction\n                else\n                    tiles[idx] # Keeps its prior value\n                end\n            end\n            tiles[idx] = outputs[idx] # Update tiles for the next iteration\n        end\n    end\n    heatmap(Int.(collect(outputs))) # Generate a heatmap visualization\nend\npath = mp4(anim; fps=5, show_msg=true).filename # Create an animation of the heatmaps over time","category":"page"},{"location":"api-daggerwebdash/types/#DaggerWebDash-Types","page":"Types","title":"DaggerWebDash Types","text":"","category":"section"},{"location":"api-daggerwebdash/types/","page":"Types","title":"Types","text":"Pages = [\"types.md\"]","category":"page"},{"location":"api-daggerwebdash/types/#Logging-Event-Types","page":"Types","title":"Logging Event Types","text":"","category":"section"},{"location":"api-daggerwebdash/types/#DaggerWebDash.D3Renderer","page":"Types","title":"DaggerWebDash.D3Renderer","text":"D3Renderer(port::Int, port_range::UnitRange; seek_store=nothing) -> D3Renderer\n\nConstructs a D3Renderer, which is a TimespanLogging aggregator which renders the logs over HTTP using the d3.js library. port is the port that will be serving the HTTP website. port_range specifies a range of ports that will be used to listen for connections from other Dagger workers. seek_store, if specified, is a Tables.jl-compatible object that logs will be written to and read from. This table can be written to disk and then re-read later for offline log analysis.\n\n\n\n\n\n","category":"type"},{"location":"api-daggerwebdash/types/#DaggerWebDash.TableStorage","page":"Types","title":"DaggerWebDash.TableStorage","text":"TableStorage\n\nLogWindow-compatible aggregator which stores logs in a Tables.jl-compatible sink.\n\nUsing a TableStorage is reasonably simple:\n\nml = TimespanLogging.MultiEventLog()\n\n... # Add some events\n\nlw = TimespanLogging.LogWindow(5*10^9, :core)\n\n# Create a DataFrame with one Any[] for each event\ndf = DataFrame([key=>[] for key in keys(ml.consumers)]...)\n\n# Create the TableStorage and register its creation handler\nts = DaggerWebDash.TableStorage(df)\npush!(lw.creation_handlers, ts)\n\nml.aggregators[:lw] = lw\n\n# Logs will now be saved into `df` automatically, and packages like\n# DaggerWebDash.jl will automatically use it to retrieve subsets of the logs.\n\n\n\n\n\n","category":"type"},{"location":"api-daggerwebdash/types/#DaggerWebDash.ProfileMetrics","page":"Types","title":"DaggerWebDash.ProfileMetrics","text":"ProfileMetrics\n\nTracks compute profile traces.\n\n\n\n\n\n","category":"type"},{"location":"gpu/#GPU-Support","page":"GPUs","title":"GPU Support","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Dagger supports GPU acceleration for CUDA, ROCm (AMD), Intel oneAPI, Metal (Apple), and OpenCL devices. GPU support enables automatic data movement between CPU and GPU memory, distributed GPU computing across multiple devices, and seamless integration with Julia's GPU ecosystem.","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Dagger's GPU support is built on top of the KernelAbstractions.jl package, as well as the specific GPU-specific packages for each backend (e.g. CUDA.jl, AMDGPU.jl, oneAPI.jl, Metal.jl, and OpenCL.jl). Dagger's GPU support is designed to be fully interoperable with the Julia GPU ecosystem, allowing you to use Dagger to distribute your GPU computations across multiple devices.","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"There are a few ways to use Dagger's GPU support:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"KernelAbstractions: Use the KernelAbstractions.jl interface to write GPU kernels, and then use Dagger.Kernel and Dagger.@spawn to execute them.\nDArray: Use the DArray interface to create distributed GPU arrays, and then call regular array operations on them, which will be automatically executed on the GPU.\nDatadeps: Use the Datadeps.jl interface to create GPU-compatible algorithms, within which you can call kernels or array operations.\nManual: Use Dagger.gpu_kernel_backend() to get the appropriate backend for the current processor, and use that to execute kernels.","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"In all cases, you need to ensure that right GPU-specific package is loaded.","category":"page"},{"location":"gpu/#Package-Loading","page":"GPUs","title":"Package Loading","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Dagger's GPU support requires loading one of the following packages:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"CUDA.jl for NVIDIA GPUs\nAMDGPU.jl for AMD GPUs\noneAPI.jl for Intel GPUs\nMetal.jl for Apple GPUs\nOpenCL.jl for OpenCL devices","category":"page"},{"location":"gpu/#Backend-Detection","page":"GPUs","title":"Backend Detection","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"You can check if a given kind of GPU is supported by calling:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"CUDA: Dagger.gpu_can_compute(:CUDA)\nAMDGPU: Dagger.gpu_can_compute(:ROC)\noneAPI: Dagger.gpu_can_compute(:oneAPI)\nMetal: Dagger.gpu_can_compute(:Metal)\nOpenCL: Dagger.gpu_can_compute(:OpenCL)","category":"page"},{"location":"gpu/#Backend-Specific-Scopes","page":"GPUs","title":"Backend-Specific Scopes","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Once you've loaded the appropriate package, you can create a scope for that backend by calling:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"# First GPU of different GPU types\ncuda_scope = Dagger.scope(cuda_gpu=1)\nrocm_scope = Dagger.scope(rocm_gpu=1)  \nintel_scope = Dagger.scope(intel_gpu=1)\nmetal_scope = Dagger.scope(metal_gpu=1)\nopencl_scope = Dagger.scope(cl_device=1)","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"These kinds of scopes can be passed to Dagger.@spawn or Dagger.with_options to enable GPU acceleration on the given backend. Note that by default, Dagger will not use any GPU if a compatible scope isn't provided through one of these mechanisms.","category":"page"},{"location":"gpu/#KernelAbstractions","page":"GPUs","title":"KernelAbstractions","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"The most direct way to use GPU acceleration in Dagger is through the KernelAbstractions.jl interface. Dagger provides seamless integration with KernelAbstractions, automatically selecting the appropriate backend for the current processor.","category":"page"},{"location":"gpu/#Basic-Kernel-Usage","page":"GPUs","title":"Basic Kernel Usage","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Write your kernels using the standard KernelAbstractions syntax:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"using KernelAbstractions\n\n@kernel function vector_add!(c, a, b)\n    i = @index(Global, Linear)\n    c[i] = a[i] + b[i]\nend\n\n@kernel function fill_kernel!(arr, value)\n    i = @index(Global, Linear)\n    arr[i] = value\nend","category":"page"},{"location":"gpu/#Using-Dagger.Kernel-for-Automatic-Backend-Selection","page":"GPUs","title":"Using Dagger.Kernel for Automatic Backend Selection","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Dagger.Kernel wraps your kernel functions and automatically selects the correct backend based on the current processor:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"# Use in tasks - backend is selected automatically\ncpu_array = Dagger.@mutable zeros(1000)\ngpu_array = Dagger.@mutable CUDA.zeros(1000)\n\n# Runs on CPU\nfetch(Dagger.@spawn Dagger.Kernel(fill_kernel!)(cpu_array, 42.0; ndrange=length(cpu_array)))\n\n# Runs on GPU when scoped appropriately\nDagger.with_options(;scope=Dagger.scope(cuda_gpu=1)) do\n    fetch(Dagger.@spawn Dagger.Kernel(fill_kernel!)(gpu_array, 42.0; ndrange=length(gpu_array)))\n\n    # Synchronize the GPU\n    Dagger.gpu_synchronize(:CUDA)\nend","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Notice the usage of Dagger.@mutable to create mutable arrays on the GPU. This is required when mutating arrays in-place with Dagger-launched kernels.","category":"page"},{"location":"gpu/#Manual-Backend-Selection-with-gpu_kernel_backend","page":"GPUs","title":"Manual Backend Selection with gpu_kernel_backend","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"For more control, use Dagger.gpu_kernel_backend() to get the backend for the current processor:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"function manual_kernel_execution(arr, value)\n    # Get the backend for the current processor\n    backend = Dagger.gpu_kernel_backend()\n\n    # Create kernel with specific backend\n    kernel = fill_kernel!(backend)\n\n    # Execute kernel\n    kernel(arr, value; ndrange=length(arr))\n\n    return arr\nend\n\n# Use within a Dagger task\narr = Dagger.@mutable CUDA.zeros(1000)\nDagger.with_options(;scope=Dagger.scope(cuda_gpu=1)) do\n    fetch(Dagger.@spawn manual_kernel_execution(arr, 42.0))\n\n    Dagger.gpu_synchronize(:CUDA)\nend","category":"page"},{"location":"gpu/#Kernel-Synchronization","page":"GPUs","title":"Kernel Synchronization","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Dagger handles synchronization automatically within Dagger tasks, but if you mixed Dagger-launched and non-Dagger-launched kernels, you can synchronize the GPU manually:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"# Launch kernel as a task - Dagger.Kernel handles backend selection automatically\narr = Dagger.@mutable CUDA.zeros(1000)\nDagger.with_options(;scope=Dagger.scope(cuda_gpu=1)) do\n    result = fetch(Dagger.@spawn Dagger.Kernel(fill_kernel!)(arr, 42.0; ndrange=length(arr)))\n\n    # Synchronize kernels launched by Dagger tasks\n    Dagger.gpu_synchronize()\n\n    # Launch kernel as a task - Dagger.Kernel handles backend selection automatically\n    fill_kernel(CUDABackend())(arr, 42.0; ndrange=length(arr))\n\n    return result\nend","category":"page"},{"location":"gpu/#DArray:-Distributed-GPU-Arrays","page":"GPUs","title":"DArray: Distributed GPU Arrays","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Dagger's DArray type seamlessly supports GPU acceleration, allowing you to create distributed arrays that are automatically allocated in GPU memory when using appropriate scopes.","category":"page"},{"location":"gpu/#GPU-Array-Allocation","page":"GPUs","title":"GPU Array Allocation","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Allocate DArrays directly on GPU devices:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"using CUDA  # or AMDGPU, oneAPI, Metal\n\n# Single GPU allocation\ngpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    # All standard allocation functions work\n    DA_rand = rand(Blocks(32, 32), Float32, 128, 128)\n    DA_ones = ones(Blocks(32, 32), Float32, 128, 128)\n    DA_zeros = zeros(Blocks(32, 32), Float32, 128, 128)\n    DA_randn = randn(Blocks(32, 32), Float32, 128, 128)\nend","category":"page"},{"location":"gpu/#Multi-GPU-Distribution","page":"GPUs","title":"Multi-GPU Distribution","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Distribute arrays across multiple GPUs:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"# Use all available CUDA GPUs\nall_gpu_scope = Dagger.scope(cuda_gpus=:)\nDagger.with_options(;scope=all_gpu_scope) do\n    DA = rand(Blocks(64, 64), Float32, 256, 256)\n    # Each chunk may be allocated on a different GPU\nend\n\n# Use specific GPUs\nmulti_gpu_scope = Dagger.scope(cuda_gpus=[1, 2, 3])\nDagger.with_options(;scope=multi_gpu_scope) do\n    DA = ones(Blocks(32, 32), Float32, 192, 192)\nend","category":"page"},{"location":"gpu/#Converting-Between-CPU-and-GPU-Arrays","page":"GPUs","title":"Converting Between CPU and GPU Arrays","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Move existing arrays to GPU:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"# Create CPU DArray\ncpu_array = rand(Blocks(32, 32), 128, 128)\n\n# Convert to GPU\ngpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    gpu_array = similar(cpu_array)\n    # gpu_array now has the same structure but is allocated on GPU\nend\n\n# Convert back to CPU\ncpu_result = collect(gpu_array)  # Brings all data back to CPU","category":"page"},{"location":"gpu/#(Advanced)-Verifying-GPU-Allocation","page":"GPUs","title":"(Advanced) Verifying GPU Allocation","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"If necessary for testing or debugging, you can check that your DArray chunks are actually living on the GPU:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"gpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    DA = rand(Blocks(4, 4), Float32, 8, 8)\n\n    # Check each chunk\n    for chunk in DA.chunks\n        raw_chunk = fetch(chunk; raw=true)\n        @assert raw_chunk isa Dagger.Chunk{<:CuArray}\n\n        # Verify it's on the correct GPU device\n        @assert remotecall_fetch(raw_chunk.handle.owner, raw_chunk) do chunk\n            arr = Dagger.MemPool.poolget(chunk.handle)\n            return CUDA.device(arr) == CUDA.devices()[1]  # GPU 1\n        end\n    end\nend","category":"page"},{"location":"gpu/#Datadeps:-GPU-Compatible-Algorithms","page":"GPUs","title":"Datadeps: GPU-Compatible Algorithms","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Datadeps regions work seamlessly with GPU arrays, enabling complex GPU algorithms with automatic dependency management. Unlike without Datadeps, you don't need to use Dagger.@mutable, as Datadeps ensures that array mutation is performed correctly.","category":"page"},{"location":"gpu/#In-Place-GPU-Operations","page":"GPUs","title":"In-Place GPU Operations","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"using LinearAlgebra\n\n# Create GPU arrays\ngpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    DA = rand(Blocks(4, 4), Float32, 8, 8)\n    DB = rand(Blocks(4, 4), Float32, 8, 8)\n    DC = zeros(Blocks(4, 4), Float32, 8, 8)\n\n    # In-place matrix multiplication on GPU\n    Dagger.spawn_datadeps() do\n        Dagger.@spawn mul!(Out(DC), In(DA), In(DB))\n    end\n\n    # Verify result\n    @assert collect(DC) ≈ collect(DA) * collect(DB)\nend","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Notice that we didn't need to call Dagger.gpu_synchronize() here, because the DArray is automatically synchronized when the DArray is collected.","category":"page"},{"location":"gpu/#Out-of-Place-GPU-Operations","page":"GPUs","title":"Out-of-Place GPU Operations","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"Because Dagger options propagate into function calls, you can call algorithms that use Datadeps (such as DArray matrix multiplication) on GPUs, without having to do any extra work:","category":"page"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"gpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    DA = rand(Blocks(4, 4), Float32, 8, 8)\n    DB = rand(Blocks(4, 4), Float32, 8, 8)\n\n    # Out-of-place operations\n    DC = DA * DB  # Automatically runs on GPU\n\n    @assert collect(DC) ≈ collect(DA) * collect(DB)\nend","category":"page"},{"location":"gpu/#Complex-GPU-Algorithms","page":"GPUs","title":"Complex GPU Algorithms","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"using LinearAlgebra\n\ngpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    # Create a positive definite matrix for Cholesky decomposition\n    A = rand(Float32, 8, 8)\n    A = A * A'\n    A[diagind(A)] .+= size(A, 1)\n    DA = DArray(A, Blocks(4, 4))\n    \n    # Cholesky decomposition on GPU\n    chol_result = cholesky(DA)\n    @assert collect(chol_result.U) ≈ cholesky(collect(DA)).U\nend","category":"page"},{"location":"gpu/#Cross-Backend-Operations","page":"GPUs","title":"Cross-Backend Operations","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"# You can even mix different GPU types in a single computation\n# (though data movement between different GPU types goes through CPU)\n\ncuda_data = Dagger.with_options(;scope=Dagger.scope(cuda_gpu=1)) do\n    rand(Blocks(32, 32), Float32, 64, 64)\nend\n\nrocm_result = Dagger.with_options(;scope=Dagger.scope(rocm_gpu=1)) do\n    # Data automatically moved: CUDA GPU -> CPU -> ROCm GPU\n    fetch(Dagger.@spawn sum(cuda_data))\nend","category":"page"},{"location":"gpu/#Distributed-GPU-Computing","page":"GPUs","title":"Distributed GPU Computing","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"You can easily combine GPU acceleration with distributed computing across multiple workers.","category":"page"},{"location":"gpu/#Multi-Worker-GPU-Setup","page":"GPUs","title":"Multi-Worker GPU Setup","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"using Distributed\naddprocs(4)  # Add 4 workers\n\n@everywhere using Dagger, CUDA\n\n# Use GPU 1 on worker 2\ndistributed_gpu_scope = Dagger.scope(worker=2, cuda_gpu=1)\n\nDagger.with_options(;scope=distributed_gpu_scope) do\n    # Create a GPU array and sum it on worker 2, GPU 1\n    DA = rand(Blocks(32, 32), Float32, 128, 128)\n    result = fetch(Dagger.@spawn sum(DA))\nend","category":"page"},{"location":"gpu/#Load-Balancing-Across-GPUs-and-Workers","page":"GPUs","title":"Load Balancing Across GPUs and Workers","text":"","category":"section"},{"location":"gpu/","page":"GPUs","title":"GPUs","text":"# Distribute work across multiple workers and their GPUs\nworkers_with_gpus = [\n    Dagger.scope(worker=2, cuda_gpu=1),\n    Dagger.scope(worker=3, cuda_gpu=1),\n    Dagger.scope(worker=4, rocm_gpu=1)  # Mix of GPU types\n]\n\n# Dagger will automatically balance work across available resources\nresults = map(workers_with_gpus) do scope\n    Dagger.with_options(;scope) do\n        DA = rand(Blocks(16, 16), Float32, 64, 64)\n        fetch(Dagger.@spawn sum(DA))\n    end\nend","category":"page"},{"location":"#Dagger:-A-framework-for-out-of-core-and-parallel-execution","page":"Home","title":"Dagger: A framework for out-of-core and parallel execution","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger.jl is a framework for parallel computing across all kinds of resources, like CPUs and GPUs, and across multiple threads and multiple servers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: when using Dagger with multiple workers, make sure that the workers are initialized before importing Dagger and doing any distributed operations. This is good:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Distributed\n\njulia> addprocs(2)\n\njulia> using Dagger","category":"page"},{"location":"","page":"Home","title":"Home","text":"But this will cause errors when using Dagger:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Distributed, Dagger\n\njulia> addprocs(2)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The reason is because Distributed.jl requires packages to be loaded across all workers in order for the workers to use objects from the needed packages, and using Dagger will load Dagger on all existing workers. If you're executing custom functions then you will also need to define those on all workers with @everywhere, see the Distributed.jl documentation for more information.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quickstart:-Task-Spawning","page":"Home","title":"Quickstart: Task Spawning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For more details: Task Spawning","category":"page"},{"location":"#Launch-a-task","page":"Home","title":"Launch a task","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you want to call a function myfunc with arguments arg1, arg2, arg3, and keyword argument color=:red:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function myfunc(arg1, arg2, arg3; color=:blue)\n    arg_total = arg1 + arg2 * arg3\n    printstyled(arg_total; color)\n    return arg_total\nend\nt = Dagger.@spawn myfunc(arg1, arg2, arg3; color=:red)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This will run the function asynchronously; you can fetch its result with fetch(t), or just wait on it to complete with wait(t). If the call to myfunc throws an error, fetch(t) will rethrow it.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If running Dagger with multiple workers, make sure to define myfunc with @everywhere from the Distributed stdlib.","category":"page"},{"location":"#Launch-a-task-with-an-anonymous-function","page":"Home","title":"Launch a task with an anonymous function","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"It's more convenient to use Dagger.spawn for anonymous functions. Taking the previous example, but using an anonymous function instead of myfunc:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Dagger.spawn((arg1, arg2, arg3; color=:blue) -> begin\n    arg_total = arg1 + arg2 * arg3\n    printstyled(arg_total; color)\n    return arg_total\nend, arg1, arg2, arg3; color=:red)","category":"page"},{"location":"","page":"Home","title":"Home","text":"spawn is functionally identical to @spawn, but can be more or less convenient to use, depending on what you're trying to do.","category":"page"},{"location":"#Launch-many-tasks-and-wait-on-them-all-to-complete","page":"Home","title":"Launch many tasks and wait on them all to complete","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"@spawn participates in @sync blocks, just like @async and Threads.@spawn, and will cause @sync to wait until all the tasks have completed:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@sync for result in simulation_results\n    Dagger.@spawn send_result_to_database(result)\nend\nnresults = length(simulation_results)\nwait(Dagger.@spawn update_database_result_count(nresults))","category":"page"},{"location":"","page":"Home","title":"Home","text":"Above, update_database_result_count will only run once all send_result_to_database calls have completed.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that other APIs (including spawn) do not participate in @sync blocks.","category":"page"},{"location":"#Run-a-task-on-a-specific-Distributed-worker","page":"Home","title":"Run a task on a specific Distributed worker","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger uses Scopes to control where tasks can execute. There's a handy constructor, Dagger.scope, that makes defining scopes easy:","category":"page"},{"location":"","page":"Home","title":"Home","text":"w2_only = Dagger.scope(worker=2)\nDagger.@spawn scope=w2_only myfunc(arg1, arg2, arg3; color=:red)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now the launched task will definitely execute on worker 2 (or if it's not possible to run on worker 2, Dagger will throw an error when you try to fetch the result).","category":"page"},{"location":"#Parallelize-nested-loops","page":"Home","title":"Parallelize nested loops","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Nested loops are a very common pattern in Julia, yet it's often difficult to parallelize them efficiently with @threads or @distributed/pmap. Thankfully, this kind of problem is quite easy for Dagger to handle; here is an example of parallelizing a two-level nested loop, where the inner loop computations (g) depend on an outer loop computation (f):","category":"page"},{"location":"","page":"Home","title":"Home","text":"@everywhere begin\n    using Random\n    Random.seed!(0)\n\n    # Some \"expensive\" functions that complete at different speeds\n    const crn = abs.(randn(20, 7))\n    f(i) = sleep(crn[i, 7])\n    g(i, j, y) = sleep(crn[i, j])\nend\nfunction nested_dagger()\n    @sync for i in 1:20\n        y = Dagger.@spawn f(i)\n        for j in 1:6\n            z = Dagger.@spawn g(i, j, y)\n        end\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"And the equivalent (and less performant) example with Threads.@threads, either parallelizing the inner or outer loop:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function nested_threads_outer()\n    Threads.@threads for i in 1:20\n        y = f(i)\n        for j in 1:6\n            z = g(i, j, y)\n        end\n    end\nend\nfunction nested_threads_inner()\n    for i in 1:20\n        y = f(i)\n        Threads.@threads for j in 1:6\n            z = g(i, j, y)\n        end\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Unlike Threads.@threads (which is really only intended to be used for a single loop, unnested), Dagger.@spawn is capable of parallelizing across both loop levels seamlessly, using the dependencies between f and g to determine the correct order to execute tasks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quickstart:-Data-Management","page":"Home","title":"Quickstart: Data Management","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For more details: Data Management","category":"page"},{"location":"#Operate-on-mutable-data-in-place","page":"Home","title":"Operate on mutable data in-place","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger usually assumes that you won't be modifying the arguments passed to your functions, but you can tell Dagger you plan to mutate them with @mutable:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A = Dagger.@mutable rand(1000, 1000)\nDagger.@spawn accumulate!(+, A, A)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This will lock A (and any tasks that use it) to the current worker. You can also lock it to a different worker by creating the data within a task:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A = Dagger.spawn() do\n    Dagger.@mutable rand(1000, 1000)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"or by specifying the worker argument to @mutable:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A = Dagger.@mutable worker=2 rand(1000, 1000)","category":"page"},{"location":"#Operate-on-distributed-data","page":"Home","title":"Operate on distributed data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Often we want to work with more than one piece of data; the common case of wanting one piece of data per worker is easy to do by using @shard:","category":"page"},{"location":"","page":"Home","title":"Home","text":"X = Dagger.@shard myid()","category":"page"},{"location":"","page":"Home","title":"Home","text":"This will execute myid() independently on every worker in your Julia cluster, and place references to each within a Shard object called X. We can then use X in task spawning, but we'll only get the result of myid() that corresponds to the worker that the task is running on:","category":"page"},{"location":"","page":"Home","title":"Home","text":"for w in workers()\n    @show fetch(Dagger.@spawn scope=Dagger.scope(worker=w) identity(X))\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above should print the result of myid() for each worker in worker(), as identity(X) receives only the value of X specific to that worker.","category":"page"},{"location":"#Reducing-over-distributed-data","page":"Home","title":"Reducing over distributed data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Reductions are often parallelized by reducing a set of partitions on each worker, and then reducing those intermediate reductions on a single worker. Dagger supports this easily with @shard:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A = Dagger.@shard rand(1:20, 10000)\ntemp_bins = Dagger.@shard zeros(20)\nhist! = (bins, arr) -> for elem in arr\n    bins[elem] += 1\nend\nwait.([Dagger.@spawn scope=Dagger.scope(;worker) hist!(temp_bins, A) for worker in procs()])\nfinal_bins = sum(map(b->fetch(Dagger.@spawn copy(b)), temp_bins); dims=1)[1]","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here, A points to unique random arrays, one on each worker, and temp_bins points to a set of histogram bins on each worker. When we @spawn hist!, Dagger passes in the random array and bins for only the specific worker that the task is run on; i.e. a call to hist! that runs on worker 2 will get a different A and temp_bins from a call to hist! on worker 3. All of the calls to hist! may run in parallel.","category":"page"},{"location":"","page":"Home","title":"Home","text":"By using map on temp_bins, we then make a copy of each worker's bins that we can safely return back to our current worker, and sum them together to get our total histogram.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quickstart:-File-IO","page":"Home","title":"Quickstart: File IO","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger has support for loading and saving files that integrates seamlessly with its task system, in the form of Dagger.File and Dagger.tofile.","category":"page"},{"location":"","page":"Home","title":"Home","text":"warn: Warn\nThese functions are not yet fully tested, so please make sure to take backups of any files that you load with them.","category":"page"},{"location":"#Loading-files-from-disk","page":"Home","title":"Loading files from disk","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In order to load one or more files from disk, Dagger provides the File function, which creates a lazy reference to a file:","category":"page"},{"location":"","page":"Home","title":"Home","text":"f = Dagger.File(\"myfile.jls\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"f is now a lazy reference to \"myfile.jls\", and its contents can be loaded automatically by just passing the object to a task:","category":"page"},{"location":"","page":"Home","title":"Home","text":"wait(Dagger.@spawn println(f))\n# Prints the loaded contents of the file","category":"page"},{"location":"","page":"Home","title":"Home","text":"By default, File assumes that the file uses Julia's Serialization format; this can be easily changed to assume Arrow format, for example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Arrow\nf = Dagger.File(\"myfile.arrow\"; serialize=Arrow.write, deserialize=Arrow.Table)","category":"page"},{"location":"#Writing-data-to-disk","page":"Home","title":"Writing data to disk","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Saving data to disk is as easy as loading it; tofile provides this capability in a similar manner to File:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A = rand(1000)\nf = Dagger.tofile(A, \"mydata.jls\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Like File, f can still be used to reference the file's data in tasks. It is likely most useful to use tofile at the end of a task to save results:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function make_data()\n    A = rand(1000)\n    return Dagger.tofile(A, \"mydata.jls\")\nend\nfetch(Dagger.@spawn make_data())\n# Data was also written to \"mydata.jls\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"tofile takes the same keyword arguments as File, allowing the format of data on disk to be specified as desired.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quickstart:-Distributed-Package-Selection","page":"Home","title":"Quickstart: Distributed Package Selection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger.jl can use either the built-in Distributed package or the DistributedNext package for distributed operations. You can set your preference using the Dagger.set_distributed_package! function.","category":"page"},{"location":"#Setting-the-Distributed-Package","page":"Home","title":"Setting the Distributed Package","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To set your preferred distributed package (e.g., to DistributedNext):","category":"page"},{"location":"","page":"Home","title":"Home","text":"Dagger.set_distributed_package!(\"DistributedNext\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"This will set a preference that persists across Julia sessions. Remember that you need to restart your Julia session for this change to take effect.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more details, see Dagger.set_distributed_package!.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quickstart:-Distributed-Arrays","page":"Home","title":"Quickstart: Distributed Arrays","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger's DArray type represents a distributed array, where a single large array is implemented as a set of smaller array partitions, which may be distributed across a Julia cluster.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more details: Distributed Arrays","category":"page"},{"location":"#Distribute-an-existing-array","page":"Home","title":"Distribute an existing array","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Distributing any kind of array into a DArray is easy, just use distribute, and specify the partitioning you desire with Blocks. For example, to distribute a 16 x 16 matrix in 4 x 4 partitions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A = rand(16, 16)\nDA = distribute(A, Blocks(4, 4))","category":"page"},{"location":"#Allocate-a-distributed-array-directly","page":"Home","title":"Allocate a distributed array directly","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To allocate a DArray, just pass your Blocks partitioning object into the appropriate allocation function, such as rand, ones, or zeros:","category":"page"},{"location":"","page":"Home","title":"Home","text":"rand(Blocks(20, 20), 100, 100)\nones(Blocks(20, 100), 100, 2000)\nzeros(Blocks(50, 20), 300, 200)","category":"page"},{"location":"#Convert-a-DArray-back-into-an-Array","page":"Home","title":"Convert a DArray back into an Array","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To get back an Array from a DArray, just call collect:","category":"page"},{"location":"","page":"Home","title":"Home","text":"DA = rand(Blocks(32, 32), 256, 128)\ncollect(DA) # returns a `Matrix{Float64}`","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Quickstart:-Stencil-Operations","page":"Home","title":"Quickstart: Stencil Operations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger's @stencil macro allows for easy specification of stencil operations on DArrays, often used in simulations and image processing. These operations typically involve updating an element based on the values of its neighbors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more details: Stencil Operations","category":"page"},{"location":"#Applying-a-Simple-Stencil","page":"Home","title":"Applying a Simple Stencil","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's how to apply a stencil that averages each element with its immediate neighbors, using a Wrap boundary condition (where neighbor access at the array edges wrap around).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Dagger\nimport Dagger: @stencil, Wrap\n\n# Create a 5x5 DArray, partitioned into 2x2 blocks\nA = rand(Blocks(2, 2), 5, 5)\nB = zeros(Blocks(2,2), 5, 5)\n\nDagger.spawn_datadeps() do\n    @stencil begin\n        # For each element in A, calculate the sum of its 3x3 neighborhood\n        # (including itself) and store the average in B.\n        # Values outside the array bounds are determined by Wrap().\n        B[idx] = sum(@neighbors(A[idx], 1, Wrap())) / 9.0\n    end\nend\n\n# B now contains the averaged values.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In this example, idx refers to the coordinates of each element being processed. @neighbors(A[idx], 1, Wrap()) fetches the 3x3 neighborhood around A[idx]. The 1 indicates a neighborhood distance of 1 from the central element, and Wrap() specifies the boundary behavior.","category":"page"},{"location":"#Quickstart:-Datadeps","page":"Home","title":"Quickstart: Datadeps","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Datadeps is a feature in Dagger.jl that facilitates parallelism control within designated regions, allowing tasks to write to their arguments while ensuring dependencies are respected. For more details: Datadeps (Data Dependencies)","category":"page"},{"location":"#Syntax","page":"Home","title":"Syntax","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Dagger.spawn_datadeps() function is used to create a \"datadeps region\" where tasks are executed with parallelism controlled by specified dependencies:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Dagger.spawn_datadeps() do\n    Dagger.@spawn func!(Out(var_name_x), In(var_name_y))\nend","category":"page"},{"location":"#Argument-Annotation","page":"Home","title":"Argument Annotation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In(X): Indicates that the variable X is only read by the task (an \"input\").\nOut(X): Indicates that the variable X is only written to by the task (an \"output\").\nInOut(X): Indicates that the variable X is both read from and written to by the task (an \"input\" and \"output\" simultaneously).","category":"page"},{"location":"#Example-with-Datadeps","page":"Home","title":"Example with Datadeps","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"X = [4,5,6,7,1,2,3,9,8]\nC = zeros(10)\n\nDagger.spawn_datadeps() do\n    Dagger.@spawn sort!(InOut(X))\n    Dagger.@spawn copyto!(Out(C), In(X))\nend\n\n# C = [1,2,3,4,5,6,7,8,9]","category":"page"},{"location":"","page":"Home","title":"Home","text":"In this example, the sort! function operates on array X, while the copyto! task reads from array X and writes to array C. By specifying dependencies using argument annotations, the tasks are executed in a controlled parallel manner, resulting in a sorted C array.","category":"page"},{"location":"#Example-without-Datadeps","page":"Home","title":"Example without Datadeps","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"X = [4,5,6,7,1,2,3,9,8]\nC = zeros(10)\nDagger.@spawn sort!(X)\nDagger.@spawn copyto!(C, X)\n\n\n# C = [4,5,6,7,1,2,3,9,8]","category":"page"},{"location":"","page":"Home","title":"Home","text":"In contrast to the previous example, here, the tasks are executed without argument annotations. As a result, there is a possibility of the copyto! task being executed before the sort! task, leading to unexpected results in the output array C.","category":"page"},{"location":"#Quickstart:-Streaming","page":"Home","title":"Quickstart: Streaming","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger.jl provides a streaming API that allows you to process data in a streaming fashion, where data is processed as it becomes available, rather than waiting for the entire dataset to be loaded into memory.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more details: Streaming","category":"page"},{"location":"#Syntax-2","page":"Home","title":"Syntax","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Dagger.spawn_streaming() function is used to create a streaming region, where tasks are executed continuously, processing data as it becomes available:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Open a file to write to on this worker\nf = Dagger.@mutable open(\"output.txt\", \"w\")\nt = Dagger.spawn_streaming() do\n    # Generate random numbers continuously\n    val = Dagger.@spawn rand()\n    # Write each random number to a file\n    Dagger.@spawn (f, val) -> begin\n        if val < 0.01\n            # Finish streaming when the random number is less than 0.01\n            Dagger.finish_stream()\n        end\n        println(f, val)\n    end\nend\n# Wait for all values to be generated and written\nwait(t)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above example demonstrates a streaming region that generates random numbers continuously and writes each random number to a file. The streaming region is terminated when a random number less than 0.01 is generated, which is done by calling Dagger.finish_stream() (this terminates the current task, and will also terminate all streaming tasks launched by spawn_streaming).","category":"page"},{"location":"#Quickstart:-GPUs","page":"Home","title":"Quickstart: GPUs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger supports GPU acceleration for CUDA, ROCm (AMD), Intel oneAPI, Metal (Apple), and OpenCL devices. GPU support enables automatic data movement between CPU and GPU memory, distributed GPU computing across multiple devices, and seamless integration with Julia's GPU ecosystem.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more details: GPU Support","category":"page"},{"location":"#Allocate-distributed-arrays-on-GPUs","page":"Home","title":"Allocate distributed arrays on GPUs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can allocate DArrays directly on GPUs by using scopes to target specific GPU devices. The arrays will be automatically allocated in GPU memory:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using CUDA  # or AMDGPU, oneAPI, Metal\n\n# Allocate a DArray on the first CUDA GPU\ngpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    DA = rand(Blocks(32, 32), Float32, 128, 128)\n    # DA is now distributed across GPU memory\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can also target multiple GPUs or all available GPUs:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Use all available CUDA GPUs\nall_gpu_scope = Dagger.scope(cuda_gpus=:)\nDagger.with_options(;scope=all_gpu_scope) do\n    DA = ones(Blocks(64, 64), Float32, 256, 256)\nend\n\n# Use specific GPUs (e.g., GPUs 1 and 2)\nmulti_gpu_scope = Dagger.scope(cuda_gpus=[1, 2])\nDagger.with_options(;scope=multi_gpu_scope) do\n    DA = zeros(Blocks(32, 32), Float32, 128, 128)\nend","category":"page"},{"location":"#Convert-CPU-arrays-to-GPU-arrays","page":"Home","title":"Convert CPU arrays to GPU arrays","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can move existing CPU DArrays to GPU by using similar within a GPU scope:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Create a CPU DArray\ncpu_array = rand(Blocks(32, 32), 128, 128)\n\n# Move to GPU\ngpu_scope = Dagger.scope(cuda_gpu=1)\nDagger.with_options(;scope=gpu_scope) do\n    gpu_array = similar(cpu_array)\n    # gpu_array is now allocated on GPU with same structure as cpu_array\nend","category":"page"},{"location":"#Run-custom-GPU-kernels-with-Dagger.Kernel","page":"Home","title":"Run custom GPU kernels with Dagger.Kernel","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger integrates with KernelAbstractions.jl to run custom GPU kernels. Use Dagger.Kernel to wrap your kernel functions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using KernelAbstractions\n\n# Define a kernel function\n@kernel function vector_add!(c, a, b)\n    i = @index(Global, Linear)\n    c[i] = a[i] + b[i]\nend\n\n# Run on GPU\n# Note: GPU arrays must be marked @mutable or use Datadeps to ensure mutability\ngpu_scope = Dagger.scope(cuda_gpu=1)\na = Dagger.@mutable CUDA.rand(1000)\nb = Dagger.@mutable CUDA.rand(1000)\nc = Dagger.@mutable CUDA.zeros(1000)\nresult = Dagger.with_options(;scope=gpu_scope) do\n    fetch(Dagger.@spawn Dagger.Kernel(vector_add!)(c, a, b; ndrange=length(c)))\n    # Synchronize the GPU\n    Dagger.gpu_synchronize(:CUDA)\nend","category":"page"},{"location":"#Use-gpu_kernel_backend-within-tasks","page":"Home","title":"Use gpu_kernel_backend within tasks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"When writing functions that will run on different devices, use Dagger.gpu_kernel_backend() to get the appropriate KernelAbstractions backend for the current processor:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@kernel function fill_kernel!(arr, value)\n    i = @index(Global, Linear)\n    arr[i] = value\nend\n\nfunction fill_array_task!(arr, value)\n    # Get the backend for the current processor (CPU, CUDA, ROCm, etc.)\n    backend = Dagger.gpu_kernel_backend()\n    kernel = fill_kernel!(backend)\n    kernel(arr, value; ndrange=length(arr))\n    return arr\nend\n\n# This function works on both CPU and GPU\ncpu_array = Dagger.@mutable zeros(1000)\ngpu_array = Dagger.@mutable CUDA.zeros(1000)\n\n# Runs on CPU\nfetch(Dagger.@spawn fill_array_task!(cpu_array, 42.0))\n\n# Runs on GPU when scoped appropriately\nDagger.with_options(;scope=Dagger.scope(cuda_gpu=1)) do\n    fetch(Dagger.@spawn fill_array_task!(gpu_array, 42.0))\n\n    # Synchronize the GPU\n    Dagger.gpu_synchronize(:CUDA)\nend","category":"page"},{"location":"#Multi-GPU-and-multi-backend-support","page":"Home","title":"Multi-GPU and multi-backend support","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dagger supports multiple GPU backends simultaneously. You can specify different GPU types using their respective scope keywords:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# CUDA GPUs\ncuda_scope = Dagger.scope(cuda_gpu=1)\n\n# ROCm/AMD GPUs  \nrocm_scope = Dagger.scope(rocm_gpu=1)\n\n# Intel GPUs\nintel_scope = Dagger.scope(intel_gpu=1)\n\n# Metal GPUs (Apple)\nmetal_scope = Dagger.scope(metal_gpu=1)\n\n# OpenCL devices\nopencl_scope = Dagger.scope(cl_device=1)","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can also combine GPU scopes with worker scopes for distributed GPU computing:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Use CUDA GPU 1 on worker 2\ndistributed_gpu_scope = Dagger.scope(worker=2, cuda_gpu=1)\nDagger.with_options(;scope=distributed_gpu_scope) do\n    DA = rand(Blocks(32, 32), Float32, 128, 128)\n    result = fetch(Dagger.@spawn sum(DA))\nend","category":"page"},{"location":"darray/#Distributed-Arrays","page":"Distributed Arrays","title":"Distributed Arrays","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"The DArray, or \"distributed array\", is an abstraction layer on top of Dagger that allows loading array-like structures into a distributed environment. The DArray partitions a larger array into smaller \"blocks\" or \"chunks\", and those blocks may be located on any worker in the cluster. The DArray uses a Parallel Global Address Space (aka \"PGAS\") model for storing partitions, which means that a DArray instance contains a reference to every partition in the greater array; this provides great flexibility in allowing Dagger to choose the most efficient way to distribute the array's blocks and operate on them in a heterogeneous manner.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Aside: an alternative model, here termed the \"MPI\" model, is not yet supported, but would allow storing only a single partition of the array on each MPI rank in an MPI cluster. DArray support for this model is planned in the near future.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"This should not be confused with the DistributedArrays.jl package.","category":"page"},{"location":"darray/#Creating-DArrays","page":"Distributed Arrays","title":"Creating DArrays","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"A DArray can be created in two ways: through an API similar to the usual rand, ones, etc. calls, or by distributing an existing array with DArray, DVector, DMatrix, or distribute.","category":"page"},{"location":"darray/#Allocating-new-arrays","page":"Distributed Arrays","title":"Allocating new arrays","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"As an example, one can allocate a random DArray by calling rand with a Blocks object as the first argument - Blocks specifies the size of partitions to be constructed, and must be the same number of dimensions as the array being allocated.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"# Add some Julia workers\njulia> using Distributed; addprocs(6)\n6-element Vector{Int64}:\n 2\n 3\n 4\n 5\n 6\n 7\n\njulia> @everywhere using Dagger\n\njulia> DX = rand(Blocks(50, 50), 100, 100)\nDMatrix{Float64}(100, 100) with 2x2 partitions of size 50x50:\n 0.610404   0.0475367  0.809016   0.311305   0.0306211   0.689645   …  0.220267   0.678548   0.892062    0.0559988\n 0.680815   0.788349   0.758755   0.0594709  0.640167    0.652266      0.331429   0.798848   0.732432    0.579534\n 0.306898   0.0805607  0.498372   0.887971   0.244104    0.148825      0.340429   0.029274   0.140624    0.292354\n 0.0537622  0.844509   0.509145   0.561629   0.566584    0.498554      0.427503   0.835242   0.699405    0.0705192\n 0.587364   0.59933    0.0624318  0.3795     0.430398    0.0853735     0.379947   0.677105   0.0305861   0.748001\n 0.14129    0.635562   0.218739   0.0629501  0.373841    0.439933   …  0.308294   0.0966736  0.783333    0.00763648\n 0.14539    0.331767   0.912498   0.0649541  0.527064    0.249595      0.826705   0.826868   0.41398     0.80321\n 0.13926    0.353158   0.330615   0.438247   0.284794    0.238837      0.791249   0.415801   0.729545    0.88308\n 0.769242   0.136001   0.950214   0.171962   0.183646    0.78294       0.570442   0.321894   0.293101    0.911913\n 0.786168   0.513057   0.781712   0.0191752  0.512821    0.621239      0.50503    0.0472064  0.0368674   0.75981\n 0.493378   0.129937   0.758052   0.169508   0.0564534   0.846092   …  0.873186   0.396222   0.284       0.0242124\n 0.12689    0.194842   0.263186   0.213071   0.535613    0.246888      0.579931   0.699231   0.441449    0.882772\n 0.916144   0.21305    0.629293   0.329303   0.299889    0.127453      0.644012   0.311241   0.713782    0.0554386\n ⋮                                                       ⋮          ⋱\n 0.430369   0.597251   0.552528   0.795223   0.46431     0.777119      0.189266   0.499178   0.715808    0.797629\n 0.235668   0.902973   0.786537   0.951402   0.768312    0.633666      0.724196   0.866373   0.0679498   0.255039\n 0.605097   0.301349   0.758283   0.681568   0.677913    0.51507    …  0.654614   0.37841    0.86399     0.583924\n 0.824216   0.62188    0.369671   0.725758   0.735141    0.183666      0.0401394  0.522191   0.849429    0.839651\n 0.578047   0.775035   0.704695   0.203515   0.00267523  0.869083      0.0975535  0.824887   0.00787017  0.920944\n 0.805897   0.0275489  0.175715   0.135956   0.389958    0.856349      0.974141   0.586308   0.59695     0.906727\n 0.212875   0.509612   0.85531    0.266659   0.0695836   0.0551129     0.788085   0.401581   0.948216    0.00242077\n 0.512997   0.134833   0.895968   0.996953   0.422192    0.991526   …  0.838781   0.141053   0.747722    0.84489\n 0.283221   0.995152   0.61636    0.75955    0.072718    0.691665      0.151339   0.295759   0.795476    0.203072\n 0.0946639  0.496832   0.551496   0.848571   0.151074    0.625696      0.673817   0.273958   0.177998    0.563221\n 0.0900806  0.127274   0.394169   0.140403   0.232985    0.460306      0.536441   0.200297   0.970311    0.0292218\n 0.0698985  0.463532   0.934776   0.448393   0.606287    0.552196      0.883694   0.212222   0.888415    0.941097","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"The rand(Blocks(50, 50), 100, 100) call specifies that a DMatrix (a DArray matrix) should be allocated which is in total 100 x 100, split into 4 blocks of size 50 x 50, and initialized with random Float64s. Many other functions, like randn, sprand, ones, and zeros can be called in this same way.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Alternatively, instead of manually specifying the block size, one can call rand with an AutoBlocks object to have Dagger automatically choose a block size:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"julia> DX = rand(AutoBlocks(), 100, 100)\nDMatrix{Float64}(100, 100) with 1x7 partitions of size 100x15:\n 0.610404   0.0475367  0.809016   0.311305   0.0306211   0.689645   …  0.220267   0.678548   0.892062    0.0559988\n 0.680815   0.788349   0.758755   0.0594709  0.640167    0.652266      0.331429   0.798848   0.732432    0.579534\n 0.306898   0.0805607  0.498372   0.887971   0.244104    0.148825      0.340429   0.029274   0.140624    0.292354\n 0.0537622  0.844509   0.509145   0.561629   0.566584    0.498554      0.427503   0.835242   0.699405    0.0705192\n 0.587364   0.59933    0.0624318  0.3795     0.430398    0.0853735     0.379947   0.677105   0.0305861   0.748001\n 0.14129    0.635562   0.218739   0.0629501  0.373841    0.439933   …  0.308294   0.0966736  0.783333    0.00763648\n 0.14539    0.331767   0.912498   0.0649541  0.527064    0.249595      0.826705   0.826868   0.41398     0.80321\n 0.13926    0.353158   0.330615   0.438247   0.284794    0.238837      0.791249   0.415801   0.729545    0.88308\n 0.769242   0.136001   0.950214   0.171962   0.183646    0.78294       0.570442   0.321894   0.293101    0.911913\n 0.786168   0.513057   0.781712   0.0191752  0.512821    0.621239      0.50503    0.0472064  0.0368674   0.75981\n 0.493378   0.129937   0.758052   0.169508   0.0564534   0.846092   …  0.873186   0.396222   0.284       0.0242124\n 0.12689    0.194842   0.263186   0.213071   0.535613    0.246888      0.579931   0.699231   0.441449    0.882772\n 0.916144   0.21305    0.629293   0.329303   0.299889    0.127453      0.644012   0.311241   0.713782    0.0554386\n ⋮                                                       ⋮          ⋱\n 0.430369   0.597251   0.552528   0.795223   0.46431     0.777119      0.189266   0.499178   0.715808    0.797629\n 0.235668   0.902973   0.786537   0.951402   0.768312    0.633666      0.724196   0.866373   0.0679498   0.255039\n 0.605097   0.301349   0.758283   0.681568   0.677913    0.51507    …  0.654614   0.37841    0.86399     0.583924\n 0.824216   0.62188    0.369671   0.725758   0.735141    0.183666      0.0401394  0.522191   0.849429    0.839651\n 0.578047   0.775035   0.704695   0.203515   0.00267523  0.869083      0.0975535  0.824887   0.00787017  0.920944\n 0.805897   0.0275489  0.175715   0.135956   0.389958    0.856349      0.974141   0.586308   0.59695     0.906727\n 0.212875   0.509612   0.85531    0.266659   0.0695836   0.0551129     0.788085   0.401581   0.948216    0.00242077\n 0.512997   0.134833   0.895968   0.996953   0.422192    0.991526   …  0.838781   0.141053   0.747722    0.84489\n 0.283221   0.995152   0.61636    0.75955    0.072718    0.691665      0.151339   0.295759   0.795476    0.203072\n 0.0946639  0.496832   0.551496   0.848571   0.151074    0.625696      0.673817   0.273958   0.177998    0.563221\n 0.0900806  0.127274   0.394169   0.140403   0.232985    0.460306      0.536441   0.200297   0.970311    0.0292218\n 0.0698985  0.463532   0.934776   0.448393   0.606287    0.552196      0.883694   0.212222   0.888415    0.941097","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"We can see above that the DMatrix was partitioned into 7 partitions, each of a maximum size of 100 x 15. Dagger will automatically partition DArray objects into as many partitions as there are Dagger processors, to optimize for parallelism.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Note that the DArray is an asynchronous object (i.e. operations on it may execute in the background), so to force it to be materialized, fetch may need to be called:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"julia> fetch(DX)\nDMatrix{Float64}(100, 100) with 1x7 partitions of size 100x15:\n 0.610404   0.0475367  0.809016   0.311305   0.0306211   0.689645   …  0.220267   0.678548   0.892062    0.0559988\n 0.680815   0.788349   0.758755   0.0594709  0.640167    0.652266      0.331429   0.798848   0.732432    0.579534\n 0.306898   0.0805607  0.498372   0.887971   0.244104    0.148825      0.340429   0.029274   0.140624    0.292354\n 0.0537622  0.844509   0.509145   0.561629   0.566584    0.498554      0.427503   0.835242   0.699405    0.0705192\n 0.587364   0.59933    0.0624318  0.3795     0.430398    0.0853735     0.379947   0.677105   0.0305861   0.748001\n 0.14129    0.635562   0.218739   0.0629501  0.373841    0.439933   …  0.308294   0.0966736  0.783333    0.00763648\n 0.14539    0.331767   0.912498   0.0649541  0.527064    0.249595      0.826705   0.826868   0.41398     0.80321\n 0.13926    0.353158   0.330615   0.438247   0.284794    0.238837      0.791249   0.415801   0.729545    0.88308\n 0.769242   0.136001   0.950214   0.171962   0.183646    0.78294       0.570442   0.321894   0.293101    0.911913\n 0.786168   0.513057   0.781712   0.0191752  0.512821    0.621239      0.50503    0.0472064  0.0368674   0.75981\n 0.493378   0.129937   0.758052   0.169508   0.0564534   0.846092   …  0.873186   0.396222   0.284       0.0242124\n 0.12689    0.194842   0.263186   0.213071   0.535613    0.246888      0.579931   0.699231   0.441449    0.882772\n 0.916144   0.21305    0.629293   0.329303   0.299889    0.127453      0.644012   0.311241   0.713782    0.0554386\n ⋮                                                       ⋮          ⋱\n 0.430369   0.597251   0.552528   0.795223   0.46431     0.777119      0.189266   0.499178   0.715808    0.797629\n 0.235668   0.902973   0.786537   0.951402   0.768312    0.633666      0.724196   0.866373   0.0679498   0.255039\n 0.605097   0.301349   0.758283   0.681568   0.677913    0.51507    …  0.654614   0.37841    0.86399     0.583924\n 0.824216   0.62188    0.369671   0.725758   0.735141    0.183666      0.0401394  0.522191   0.849429    0.839651\n 0.578047   0.775035   0.704695   0.203515   0.00267523  0.869083      0.0975535  0.824887   0.00787017  0.920944\n 0.805897   0.0275489  0.175715   0.135956   0.389958    0.856349      0.974141   0.586308   0.59695     0.906727\n 0.212875   0.509612   0.85531    0.266659   0.0695836   0.0551129     0.788085   0.401581   0.948216    0.00242077\n 0.512997   0.134833   0.895968   0.996953   0.422192    0.991526   …  0.838781   0.141053   0.747722    0.84489\n 0.283221   0.995152   0.61636    0.75955    0.072718    0.691665      0.151339   0.295759   0.795476    0.203072\n 0.0946639  0.496832   0.551496   0.848571   0.151074    0.625696      0.673817   0.273958   0.177998    0.563221\n 0.0900806  0.127274   0.394169   0.140403   0.232985    0.460306      0.536441   0.200297   0.970311    0.0292218\n 0.0698985  0.463532   0.934776   0.448393   0.606287    0.552196      0.883694   0.212222   0.888415    0.941097","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"This doesn't change the type or values of the DArray, but it does make sure that any pending operations have completed. When shown in the REPL, Dagger will show all of the values of the DArray that have finished being computed, and otherwise shows a ... for values which are still be computed.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"To convert a DArray back into an Array, collect can be used to gather the data from all the Julia workers that they're on and combine them into a single Array on the worker calling collect:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"julia> collect(DX)\n100×100 Matrix{Float64}:\n 0.610404   0.0475367  0.809016   0.311305   0.0306211   0.689645   …  0.220267   0.678548   0.892062    0.0559988\n 0.680815   0.788349   0.758755   0.0594709  0.640167    0.652266      0.331429   0.798848   0.732432    0.579534\n 0.306898   0.0805607  0.498372   0.887971   0.244104    0.148825      0.340429   0.029274   0.140624    0.292354\n 0.0537622  0.844509   0.509145   0.561629   0.566584    0.498554      0.427503   0.835242   0.699405    0.0705192\n 0.587364   0.59933    0.0624318  0.3795     0.430398    0.0853735     0.379947   0.677105   0.0305861   0.748001\n 0.14129    0.635562   0.218739   0.0629501  0.373841    0.439933   …  0.308294   0.0966736  0.783333    0.00763648\n 0.14539    0.331767   0.912498   0.0649541  0.527064    0.249595      0.826705   0.826868   0.41398     0.80321\n 0.13926    0.353158   0.330615   0.438247   0.284794    0.238837      0.791249   0.415801   0.729545    0.88308\n 0.769242   0.136001   0.950214   0.171962   0.183646    0.78294       0.570442   0.321894   0.293101    0.911913\n 0.786168   0.513057   0.781712   0.0191752  0.512821    0.621239      0.50503    0.0472064  0.0368674   0.75981\n 0.493378   0.129937   0.758052   0.169508   0.0564534   0.846092   …  0.873186   0.396222   0.284       0.0242124\n 0.12689    0.194842   0.263186   0.213071   0.535613    0.246888      0.579931   0.699231   0.441449    0.882772\n 0.916144   0.21305    0.629293   0.329303   0.299889    0.127453      0.644012   0.311241   0.713782    0.0554386\n ⋮                                                       ⋮          ⋱\n 0.430369   0.597251   0.552528   0.795223   0.46431     0.777119      0.189266   0.499178   0.715808    0.797629\n 0.235668   0.902973   0.786537   0.951402   0.768312    0.633666      0.724196   0.866373   0.0679498   0.255039\n 0.605097   0.301349   0.758283   0.681568   0.677913    0.51507    …  0.654614   0.37841    0.86399     0.583924\n 0.824216   0.62188    0.369671   0.725758   0.735141    0.183666      0.0401394  0.522191   0.849429    0.839651\n 0.578047   0.775035   0.704695   0.203515   0.00267523  0.869083      0.0975535  0.824887   0.00787017  0.920944\n 0.805897   0.0275489  0.175715   0.135956   0.389958    0.856349      0.974141   0.586308   0.59695     0.906727\n 0.212875   0.509612   0.85531    0.266659   0.0695836   0.0551129     0.788085   0.401581   0.948216    0.00242077\n 0.512997   0.134833   0.895968   0.996953   0.422192    0.991526   …  0.838781   0.141053   0.747722    0.84489\n 0.283221   0.995152   0.61636    0.75955    0.072718    0.691665      0.151339   0.295759   0.795476    0.203072\n 0.0946639  0.496832   0.551496   0.848571   0.151074    0.625696      0.673817   0.273958   0.177998    0.563221\n 0.0900806  0.127274   0.394169   0.140403   0.232985    0.460306      0.536441   0.200297   0.970311    0.0292218\n 0.0698985  0.463532   0.934776   0.448393   0.606287    0.552196      0.883694   0.212222   0.888415    0.941097","category":"page"},{"location":"darray/#Distributing-existing-arrays","page":"Distributed Arrays","title":"Distributing existing arrays","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Now let's look at constructing a DArray from an existing array object; we can do this by calling the DArray constructor or distribute:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"julia> Z = zeros(100, 500);\n\njulia> Dzeros = DArray(Z, Blocks(10, 50))\nDMatrix{Float64}(100, 500) with 10x10 partitions of size 10x50:\n...","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"This will distribute the array partitions (in chunks of 10 x 50 matrices) across the workers in the Julia cluster in a relatively even distribution; future operations on a DArray may produce a different distribution from the one chosen by previous calls.","category":"page"},{"location":"darray/#Explicit-Processor-Mapping-of-DArray-Blocks","page":"Distributed Arrays","title":"Explicit Processor Mapping of DArray Blocks","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"This feature allows you to control how DArray blocks (chunks) are assigned to specific processors within the cluster. Controlling data locality is crucial for optimizing the performance of distributed algorithms.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"You can specify the mapping using the optional assignment argument in the DArray constructor functions (DArray, DVector, and DMatrix), the distribute function, and also directly within constructor-like functions such as rand, randn, sprand, ones, and zeros using the assignment optional keyword argument.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"The assignment argument accepts the following values:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":":arbitrary (Default):\nIf assignment is not provided or is set to symbol :arbitrary, Dagger's scheduler assigns blocks to processors automatically. This is the default behavior.\n:blockrow:\nDivides the matrix blocks row-wise (vertically in the terminal). Each processor gets a contiguous chunk of row blocks.\n:blockcol:\nDivides the matrix blocks column-wise (horizontally in the terminal). Each processor gets a contiguous chunk of column blocks.\n:cyclicrow:\nAssigns row-blocks to processors in a round-robin fashion. Blocks are distributed one row-block at a time. Useful for parallel row-wise tasks.\n:cycliccol:\nAssigns column-blocks to processors in a round-robin fashion. Blocks are distributed one column-block at a time. Useful for parallel column-wise tasks.\nAny other symbol used for assignment results in an error.\nAbstractArray{<:Int, N}:\nProvide an integer N-dimensional array of worker IDs. The dimension N must match the number of dimensions of the DArray.\nDagger maps blocks to worker IDs in a block-cyclic manner according to this processor-array. The block at index (i,j,...) is assigned to the first CPU thread of the worker with ID assignment[mod1(i, size(assignment,1)), mod1(j, size(assignment,2)), ...]. This pattern repeats block-cyclically across all dimensions.\nAbstractArray{<:Processor, N}:\nProvide an N-dimensional array of Processor objects. The dimension N must match the number of dimensions of the DArray blocks.\nBlocks are mapped in a block-cyclic manner according to the Processor objects in the assignment array. The block at index (i,j,...) is assigned to the processor at assignment[mod1(i, size(assignment,1)), mod1(j, size(assignment,2)), ...]. This pattern repeats block-cyclically across all dimensions.","category":"page"},{"location":"darray/#Examples-and-Usage","page":"Distributed Arrays","title":"Examples and Usage","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"The assignment argument works similarly for DArray, DVector, and DMatrix, as well as the distribute function. The key difference lies in the dimensionality of the resulting distributed array. For functions like rand, randn, sprand, ones, and zeros, assignment is an keyword argument.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"DArray: For N-dimensional distributed arrays.\nDVector: Specifically for 1-dimensional distributed arrays.\nDMatrix: Specifically for 2-dimensional distributed arrays.\ndistribute: General function to distribute arrays of any dimensionality.\nrand, randn, sprand, ones, zeros: Functions to create DArrays with initial values, also supporting assignment.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Here are some examples using a setup with one master process and three worker processes.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"First, let's create some sample arrays for distribute (and constructor functions):","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"A = rand(7, 11)   # 2D array\nv = ones(15)      # 1D array\nM = zeros(5, 5, 5) # 3D array","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Arbitrary Assignment:\n```julia  Ad = distribute(A, Blocks(2, 2), :arbitrary)\nDMatrix(A, Blocks(2, 2), :arbitrary)\nvd = distribute(v, Blocks(3), :arbitrary)\nDVector(v, Blocks(3), :arbitrary)\nMd = distribute(M, Blocks(2, 2, 2), :arbitrary)\nDArray(M, Blocks(2,2,2), :arbitrary)\nRd = rand(Blocks(2, 2), 7, 11; assignment=:arbitrary)\ndistribute(rand(7, 11), Blocks(2, 2), :arbitrary)\n```\nThis creates distributed arrays with the specified block sizes, and assigns the blocks to processors arbitrarily. For example, the assignment for Ad might look like this:\njulia  4×6 Matrix{Dagger.ThreadProc}:  ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(2, 1) ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(2, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)\nStructured Assignments:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":":blockrow Assignment:\nAd = distribute(A, Blocks(1, 2), :blockrow)\n# DMatrix(A, Blocks(1, 2), :blockrow)\nvd = distribute(v, Blocks(3), :blockrow)\n# DVector(v, Blocks(3), :blockrow)\nMd = distribute(M, Blocks(2, 2, 2), :blockrow)\n# DArray(M, Blocks(2,2,2), :blockrow)\nOd = ones(Blocks(1, 2), 7, 11; assignment=:blockrow)\n# distribute(ones(7, 11), Blocks(1, 2), :blockrow)\nThis creates distributed arrays with the specified block sizes, and assigns contiguous row-blocks to processors evenly. For example, the assignment for Ad (and Od) will look like this:\n7×6 Matrix{Dagger.ThreadProc}:\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1) ThreadProc(1, 1)\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1) ThreadProc(1, 1)\nThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1) ThreadProc(2, 1)\nThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1) ThreadProc(2, 1)\nThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1) ThreadProc(3, 1)\nThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1) ThreadProc(3, 1)\nThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1) ThreadProc(4, 1)\n:blockcol Assignment:\nAd = distribute(A, Blocks(2, 2), :blockcol)\n# DMatrix(A, Blocks(2, 2), :blockcol)\nvd = distribute(v, Blocks(3), :blockcol)\n# DVector(v, Blocks(3), :blockcol)\nMd = distribute(M, Blocks(2, 2, 2), :blockcol)\n# DArray(M, Blocks(2,2,2), :blockcol)\nRd = randn(Blocks(2, 2), 7, 11; assignment=:blockcol)\n# distribute(randn(7, 11), Blocks(2, 2), :blockcol)\nThis creates distributed arrays with the specified block sizes, and assigns contiguous column-blocks to processors evenly. For example, the assignment for Ad (and Rd) will look like this:\n4×6 Matrix{Dagger.ThreadProc}:\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)\n:cyclicrow Assignment:\nAd = distribute(A, Blocks(1, 2), :cyclicrow)\n# DMatrix(A, Blocks(1, 2), :cyclicrow)\nvd = distribute(v, Blocks(3), :cyclicrow)\n# DVector(v, Blocks(3), :cyclicrow)\nMd = distribute(M, Blocks(2, 2, 2), :cyclicrow)\n# DArray(M, Blocks(2,2,2), :cyclicrow)\nZd = zeros(Blocks(1, 2), 7, 11; assignment=:cyclicrow)\n# distribute(zeros(7, 11), Blocks(1, 2), :cyclicrow)\nThis creates distributed arrays with the specified block sizes, and assigns row-blocks to processors in round-robin fashion. For example, the assignment for Ad (and Zd) will look like this:\n7×6 Matrix{Dagger.ThreadProc}:\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1) ThreadProc(1, 1)\nThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1) ThreadProc(2, 1)\nThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1) ThreadProc(3, 1)\nThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1)  ThreadProc(4, 1) ThreadProc(4, 1)\nThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1)  ThreadProc(1, 1) ThreadProc(1, 1)\nThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1)  ThreadProc(2, 1) ThreadProc(2, 1)\nThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1)  ThreadProc(3, 1) ThreadProc(3, 1)\n:cycliccol Assignment:\nAd = distribute(A, Blocks(2, 2), :cycliccol)\n# DMatrix(A, Blocks(2, 2), :cycliccol)\nvd = distribute(v, Blocks(3), :cycliccol)\n# DVector(v, Blocks(3), :cycliccol)\nMd = distribute(M, Blocks(2, 2, 2), :cycliccol)\n# DArray(M, Blocks(2,2,2), :cycliccol)\nOd = ones(Blocks(2, 2), 7, 11; assignment=:cycliccol)\n# distribute(ones(7, 11), Blocks(2, 2), :cycliccol)\nThis creates distributed arrays with the specified block sizes, and assigns column-blocks to processors in round-robin fashion. For example, the assignment for Ad (and Od) will look like this:\n4×6 Matrix{Dagger.ThreadProc}:\nThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)\nThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)\nThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)\nThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Block-Cyclic Assignment with Integer Array:\n```julia  assignment2d = [2 1; 4 3]  Ad = distribute(A, Blocks(2, 2), assignment2d)\nDMatrix(A, Blocks(2, 2), [2 1; 4 3])\nassignment1d = [2,3,1,4]  vd = distribute(v, Blocks(3), assignment1d)\nDVector(v, Blocks(3), [2,3,1,4])\nassignment3d = cat([1 2; 3 4], [4 3; 2 1], dims=3)  Md = distribute(M, Blocks(2, 2, 2), assignment3d) \nDArray(M, Blocks(2, 2, 2), cat([1 2; 3 4], [4 3; 2 1], dims=3))\nRd = sprand(Blocks(2, 2), 7, 11, 0.2; assignment=assignment_2d)\ndistribute(sprand(7,11, 0.2), Blocks(2, 2), assignment_2d)\n```\nThe assignment is an integer matrix of worker IDs, the blocks are assigned in block-cyclic manner to the first CPU thread of each worker. The assignment for Ad (and Rd) would be:\njulia  4×6 Matrix{Dagger.ThreadProc}:    ThreadProc(2, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(1, 1)    ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(3, 1)    ThreadProc(2, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(1, 1)  ThreadProc(2, 1)  ThreadProc(1, 1)    ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(3, 1)  ThreadProc(4, 1)  ThreadProc(3, 1)\nBlock-Cyclic Assignment with Processor Array:\n```julia  assignment2d = [Dagger.ThreadProc(3, 2) Dagger.ThreadProc(1, 1);                   Dagger.ThreadProc(4, 3) Dagger.ThreadProc(2, 2)]  Ad = distribute(A, Blocks(2, 2), assignment2d)\nDMatrix(A, Blocks(2, 2), assignment_2d)\nassignment1d = [Dagger.ThreadProc(2,1), Dagger.ThreadProc(3,1), Dagger.ThreadProc(1,1), Dagger.ThreadProc(4,1)]  vd = distribute(v, Blocks(3), assignment1d)\nDVector(v, Blocks(3), assignment_1d)\nassignment3d = cat([Dagger.ThreadProc(1,1) Dagger.ThreadProc(2,1); Dagger.ThreadProc(3,1) Dagger.ThreadProc(4,1)],                      [Dagger.ThreadProc(4,1) Dagger.ThreadProc(3,1); Dagger.ThreadProc(2,1) Dagger.ThreadProc(1,1)], dims=3)  Md = distribute(M, Blocks(2, 2, 2), assignment3d)\nDArray(M, Blocks(2, 2, 2), assignment_3d)\nRd = rand(Blocks(2, 2), 7, 11; assignment=assignment_2d))\ndistribute(rand(7,11), Blocks(2, 2), assignment_2d)\n```\nThe assignment is a matrix of Processor objects, the blocks are assigned in block-cyclic manner to each processor. The assignment for Ad (and Rd) would be:\njulia  4×6 Matrix{Dagger.ThreadProc}:    ThreadProc(3, 2)  ThreadProc(1, 1)  ThreadProc(3, 2)  ThreadProc(1, 1)  ThreadProc(3, 2)  ThreadProc(1, 1)    ThreadProc(4, 3)  ThreadProc(2, 2)  ThreadProc(4, 3)  ThreadProc(2, 2)  ThreadProc(4, 3)  ThreadProc(2, 2)    ThreadProc(3, 2)  ThreadProc(1, 1)  ThreadProc(3, 2)  ThreadProc(1, 1)  ThreadProc(3, 2)  ThreadProc(1, 1)    ThreadProc(4, 3)  ThreadProc(2, 2)  ThreadProc(4, 3)  ThreadProc(2, 2)  ThreadProc(4, 3)  ThreadProc(2, 2)","category":"page"},{"location":"darray/#Broadcasting","page":"Distributed Arrays","title":"Broadcasting","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"As the DArray is a subtype of AbstractArray and generally satisfies Julia's array interface, a variety of common operations (such as broadcast) work as expected:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"julia> DX = rand(Blocks(50,50), 100, 100)\nDMatrix{Float64}(100, 100) with 2x2 partitions of size 50x50:\n 0.498392   0.286688    0.526038   …  0.0679859  0.246031   0.662384   0.415873\n 0.470772   0.118921    0.338746      0.368685   0.601165   0.43923    0.838116\n 0.114096   0.214045    0.973305      0.739328   0.476762   0.880491   0.923226\n 0.950628   0.937549    0.255425      0.800531   0.686832   0.554949   0.95652\n 0.887815   0.149639    0.381778      0.511954   0.567506   0.599481   0.31642\n 0.492811   0.517651    0.452395   …  0.048365   0.282697   0.117261   0.0695919\n 0.96531    0.694923    0.319353      0.0269875  0.725317   0.38704    0.889079\n 0.642189   0.139344    0.811443      0.713439   0.82764    0.0817175  0.649828\n 0.470414   0.310536    0.614132      0.91453    0.38133    0.109497   0.678592\n 0.681798   0.540348    0.898996      0.666149   0.818365   0.608407   0.959402\n 0.192863   0.319655    0.340089   …  0.339894   0.879239   0.0198826  0.576009\n 0.70397    0.789439    0.640622      0.863039   0.380762   0.830201   0.273082\n 0.859905   0.660245    0.170967      0.827866   0.456064   0.158056   0.39331\n 0.917375   0.564129    0.409167      0.0608749  0.967919   0.358908   0.313862\n 0.37067    0.619176    0.913832      0.574299   0.366162   0.209266   0.755402\n 0.272124   0.609023    0.367749   …  0.702147   0.393283   0.947087   0.642886\n 0.731806   0.246858    0.952142      0.617165   0.667969   0.955148   0.0721093\n 0.360135   0.776176    0.835084      0.183326   0.714036   0.370287   0.133747\n 0.767541   0.663163    0.244765      0.825391   0.870428   0.710432   0.936085\n 0.364802   0.161725    0.416545      0.685533   0.0313213  0.550103   0.622557\n ⋮                                 ⋱\n 0.219181   0.305549    0.137981      0.133313   0.537143   0.613063   0.583891\n 0.176936   0.930333    0.737141      0.288332   0.525941   0.33041    0.653449\n 0.49888    0.644244    0.774862      0.757912   0.411029   0.0304365  0.569458\n 0.462656   0.186863    0.946858      0.784609   0.269699   0.968227   0.409438\n 0.969422   0.167368    0.205654   …  0.033398   0.759695   0.222605   0.159356\n 0.0875248  0.498971    0.620837      0.112562   0.597004   0.208103   0.00320475\n 0.908971   0.208706    0.676567      0.5081     0.118424   0.0320135  0.897443\n 0.991279   0.835444    0.14738       0.365196   0.426543   0.987013   0.0339898\n 0.331385   0.46114     0.718353      0.210474   0.21223    0.245349   0.211097\n 0.88416    0.790778    0.352482   …  0.364377   0.0734304  0.610556   0.986503\n 0.0325297  0.649128    0.996022      0.842136   0.26821    0.598355   0.923314\n 0.793668   0.0111804   0.972974      0.401435   0.10282    0.176944   0.312946\n 0.175388   0.414811    0.930609      0.0303789  0.794293   0.664361   0.509174\n 0.056124   0.962519    0.51812       0.914509   0.972889   0.909924   0.831407\n 0.186426   0.17904     0.712901   …  0.661726   0.937605   0.70563    0.434793\n 0.182262   0.890191    0.123335      0.0570102  0.188695   0.534232   0.864526\n 0.949261   0.520407    0.0579928     0.473342   0.90016    0.525208   0.224062\n 0.817864   0.92868     0.513427      0.619016   0.0461629  0.844613   0.734735\n 0.792413   0.00791863  0.76343       0.890141   0.183165   0.530084   0.521841\n\njulia> DY = DX .+ DX\nDMatrix{Float64}(100, 100) with 2x2 partitions of size 50x50:\n 0.996784   0.573376   1.05208   1.52853    …  0.135972   0.492062   1.32477    0.831746\n 0.941543   0.237841   0.677493  0.819019      0.737371   1.20233    0.878459   1.67623\n 0.228193   0.428089   1.94661   1.97741       1.47866    0.953524   1.76098    1.84645\n 1.90126    1.8751     0.51085   1.20145       1.60106    1.37366    1.1099     1.91304\n 1.77563    0.299277   0.763556  0.800454      1.02391    1.13501    1.19896    0.63284\n 0.985622   1.0353     0.904789  0.132049   …  0.09673    0.565395   0.234523   0.139184\n 1.93062    1.38985    0.638706  0.677675      0.0539751  1.45063    0.774081   1.77816\n 1.28438    0.278689   1.62289   1.39015       1.42688    1.65528    0.163435   1.29966\n 0.940828   0.621072   1.22826   0.262374      1.82906    0.76266    0.218993   1.35718\n 1.3636     1.0807     1.79799   1.30764       1.3323     1.63673    1.21681    1.9188\n 0.385725   0.639309   0.680178  1.15371    …  0.679787   1.75848    0.0397651  1.15202\n 1.40794    1.57888    1.28124   0.740523      1.72608    0.761524   1.6604     0.546163\n 1.71981    1.32049    0.341934  0.0577456     1.65573    0.912128   0.316112   0.78662\n 1.83475    1.12826    0.818334  1.13474       0.12175    1.93584    0.717816   0.627725\n 0.741341   1.23835    1.82766   0.868958      1.1486     0.732324   0.418533   1.5108\n 0.544247   1.21805    0.735498  1.03821    …  1.40429    0.786565   1.89417    1.28577\n 1.46361    0.493715   1.90428   1.80758       1.23433    1.33594    1.9103     0.144219\n 0.720269   1.55235    1.67017   1.25524       0.366652   1.42807    0.740574   0.267495\n 1.53508    1.32633    0.48953   1.90929       1.65078    1.74086    1.42086    1.87217\n 0.729603   0.32345    0.833089  1.88305       1.37107    0.0626427  1.10021    1.24511\n ⋮                                          ⋱\n 0.438362   0.611098   0.275962  1.59538       0.266626   1.07429    1.22613    1.16778\n 0.353873   1.86067    1.47428   1.59328       0.576663   1.05188    0.66082    1.3069\n 0.997761   1.28849    1.54972   0.625172      1.51582    0.822057   0.060873   1.13892\n 0.925313   0.373726   1.89372   1.97415       1.56922    0.539397   1.93645    0.818876\n 1.93884    0.334736   0.411308  0.0129113  …  0.0667961  1.51939    0.44521    0.318712\n 0.17505    0.997942   1.24167   0.190925      0.225124   1.19401    0.416207   0.00640949\n 1.81794    0.417412   1.35313   1.16716       1.0162     0.236847   0.0640269  1.79489\n 1.98256    1.67089    0.29476   1.68775       0.730392   0.853086   1.97403    0.0679796\n 0.662771   0.922279   1.43671   1.56052       0.420949   0.424459   0.490698   0.422194\n 1.76832    1.58156    0.704965  1.34981    …  0.728755   0.146861   1.22111    1.97301\n 0.0650594  1.29826    1.99204   1.82428       1.68427    0.53642    1.19671    1.84663\n 1.58734    0.0223607  1.94595   1.45301       0.80287    0.205641   0.353888   0.625892\n 0.350777   0.829621   1.86122   1.52899       0.0607578  1.58859    1.32872    1.01835\n 0.112248   1.92504    1.03624   1.45978       1.82902    1.94578    1.81985    1.66281\n 0.372851   0.358081   1.4258    1.49133    …  1.32345    1.87521    1.41126    0.869586\n 0.364524   1.78038    0.24667   0.072136      0.11402    0.37739    1.06846    1.72905\n 1.89852    1.04081    0.115986  0.227947      0.946684   1.80032    1.05042    0.448124\n 1.63573    1.85736    1.02685   1.80253       1.23803    0.0923258  1.68923    1.46947\n 1.58483    0.0158373  1.52686   0.0511455     1.78028    0.36633    1.06017    1.04368\n\njulia> DZ = DY .* 3\nDMatrix{Float64}(100, 100) with 2x2 partitions of size 50x50:\n 2.99035   1.72013    3.15623   4.58558    …  0.407915  1.47619   3.9743    2.49524\n 2.82463   0.713524   2.03248   2.45706       2.21211   3.60699   2.63538   5.0287\n 0.684579  1.28427    5.83983   5.93224       4.43597   2.86057   5.28295   5.53936\n 5.70377   5.62529    1.53255   3.60434       4.80319   4.12099   3.32969   5.73912\n 5.32689   0.897831   2.29067   2.40136       3.07172   3.40504   3.59689   1.89852\n 2.95686   3.10591    2.71437   0.396147   …  0.29019   1.69618   0.703568  0.417551\n 5.79186   4.16954    1.91612   2.03302       0.161925  4.3519    2.32224   5.33448\n 3.85314   0.836066   4.86866   4.17046       4.28063   4.96584   0.490305  3.89897\n 2.82249   1.86322    3.68479   0.787122      5.48718   2.28798   0.65698   4.07155\n 4.09079   3.24209    5.39397   3.92293       3.99689   4.91019   3.65044   5.75641\n 1.15718   1.91793    2.04053   3.46113    …  2.03936   5.27544   0.119295  3.45606\n 4.22382   4.73663    3.84373   2.22157       5.17824   2.28457   4.98121   1.63849\n 5.15943   3.96147    1.0258    0.173237      4.9672    2.73639   0.948335  2.35986\n 5.50425   3.38477    2.455     3.40421       0.365249  5.80751   2.15345   1.88317\n 2.22402   3.71505    5.48299   2.60687       3.44579   2.19697   1.2556    4.53241\n 1.63274   3.65414    2.20649   3.11464    …  4.21288   2.3597    5.68252   3.85731\n 4.39084   1.48115    5.71285   5.42273       3.70299   4.00781   5.73089   0.432656\n 2.16081   4.65706    5.0105    3.76573       1.09996   4.28422   2.22172   0.802485\n 4.60525   3.97898    1.46859   5.72788       4.95234   5.22257   4.26259   5.61651\n 2.18881   0.970351   2.49927   5.64915       4.1132    0.187928  3.30062   3.73534\n ⋮                                         ⋱\n 1.31509   1.83329    0.827885  4.78613       0.799879  3.22286   3.67838   3.50334\n 1.06162   5.582      4.42284   4.77983       1.72999   3.15565   1.98246   3.92069\n 2.99328   3.86546    4.64917   1.87552       4.54747   2.46617   0.182619  3.41675\n 2.77594   1.12118    5.68115   5.92246       4.70765   1.61819   5.80936   2.45663\n 5.81653   1.00421    1.23392   0.0387339  …  0.200388  4.55817   1.33563   0.956135\n 0.525149  2.99383    3.72502   0.572775      0.675371  3.58202   1.24862   0.0192285\n 5.45382   1.25224    4.0594    3.50149       3.0486    0.710542  0.192081  5.38466\n 5.94767   5.01267    0.88428   5.06324       2.19118   2.55926   5.92208   0.203939\n 1.98831   2.76684    4.31012   4.68156       1.26285   1.27338   1.47209   1.26658\n 5.30496   4.74467    2.11489   4.04942    …  2.18626   0.440583  3.66334   5.91902\n 0.195178  3.89477    5.97613   5.47285       5.05282   1.60926   3.59013   5.53988\n 4.76201   0.0670822  5.83784   4.35903       2.40861   0.616922  1.06166   1.87768\n 1.05233   2.48886    5.58365   4.58697       0.182273  4.76576   3.98617   3.05505\n 0.336744  5.77511    3.10872   4.37935       5.48705   5.83733   5.45954   4.98844\n 1.11855   1.07424    4.27741   4.47399    …  3.97035   5.62563   4.23378   2.60876\n 1.09357   5.34114    0.74001   0.216408      0.342061  1.13217   3.20539   5.18716\n 5.69556   3.12244    0.347957  0.683841      2.84005   5.40096   3.15125   1.34437\n 4.90718   5.57208    3.08056   5.40758       3.71409   0.276977  5.06768   4.40841\n 4.75448   0.0475118  4.58058   0.153437      5.34085   1.09899   3.18051   3.13105","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Now, DZ will contain the result of computing (DX .+ DX) .* 3.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"julia> Dagger.chunks(DZ)\n2×2 Matrix{Any}:\n DTask (finished)  DTask (finished)\n DTask (finished)  DTask (finished)\n\njulia> Dagger.chunks(fetch(DZ))\n2×2 Matrix{Union{DTask, Dagger.Chunk}}:\n Chunk{Matrix{Float64}, DRef, ThreadProc, AnyScope}(Matrix{Float64}, ArrayDomain{2}((1:50, 1:50)), DRef(4, 8, 0x0000000000004e20), ThreadProc(4, 1), AnyScope(), true)  …  Chunk{Matrix{Float64}, DRef, ThreadProc, AnyScope}(Matrix{Float64}, ArrayDomain{2}((1:50, 1:50)), DRef(2, 5, 0x0000000000004e20), ThreadProc(2, 1), AnyScope(), true)\n Chunk{Matrix{Float64}, DRef, ThreadProc, AnyScope}(Matrix{Float64}, ArrayDomain{2}((1:50, 1:50)), DRef(5, 5, 0x0000000000004e20), ThreadProc(5, 1), AnyScope(), true)     Chunk{Matrix{Float64}, DRef, ThreadProc, AnyScope}(Matrix{Float64}, ArrayDomain{2}((1:50, 1:50)), DRef(3, 3, 0x0000000000004e20), ThreadProc(3, 1), AnyScope(), true)","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Here we can see the DArray's internal representation of the partitions, which are stored as either DTask objects (representing an ongoing or completed computation) or Chunk objects (which reference data which exist locally or on other Julia workers). Of course, one doesn't typically need to worry about these internal details unless implementing low-level operations on DArrays.","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"Finally, it's easy to see the results of this combination of broadcast operations; just use collect to get an Array:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"julia> collect(DZ)\n100×100 Matrix{Float64}:\n 2.99035   1.72013    3.15623   4.58558    …  0.407915  1.47619   3.9743    2.49524\n 2.82463   0.713524   2.03248   2.45706       2.21211   3.60699   2.63538   5.0287\n 0.684579  1.28427    5.83983   5.93224       4.43597   2.86057   5.28295   5.53936\n 5.70377   5.62529    1.53255   3.60434       4.80319   4.12099   3.32969   5.73912\n 5.32689   0.897831   2.29067   2.40136       3.07172   3.40504   3.59689   1.89852\n 2.95686   3.10591    2.71437   0.396147   …  0.29019   1.69618   0.703568  0.417551\n 5.79186   4.16954    1.91612   2.03302       0.161925  4.3519    2.32224   5.33448\n 3.85314   0.836066   4.86866   4.17046       4.28063   4.96584   0.490305  3.89897\n 2.82249   1.86322    3.68479   0.787122      5.48718   2.28798   0.65698   4.07155\n 4.09079   3.24209    5.39397   3.92293       3.99689   4.91019   3.65044   5.75641\n 1.15718   1.91793    2.04053   3.46113    …  2.03936   5.27544   0.119295  3.45606\n 4.22382   4.73663    3.84373   2.22157       5.17824   2.28457   4.98121   1.63849\n 5.15943   3.96147    1.0258    0.173237      4.9672    2.73639   0.948335  2.35986\n 5.50425   3.38477    2.455     3.40421       0.365249  5.80751   2.15345   1.88317\n 2.22402   3.71505    5.48299   2.60687       3.44579   2.19697   1.2556    4.53241\n 1.63274   3.65414    2.20649   3.11464    …  4.21288   2.3597    5.68252   3.85731\n 4.39084   1.48115    5.71285   5.42273       3.70299   4.00781   5.73089   0.432656\n 2.16081   4.65706    5.0105    3.76573       1.09996   4.28422   2.22172   0.802485\n 4.60525   3.97898    1.46859   5.72788       4.95234   5.22257   4.26259   5.61651\n 2.18881   0.970351   2.49927   5.64915       4.1132    0.187928  3.30062   3.73534\n ⋮                                         ⋱\n 1.31509   1.83329    0.827885  4.78613       0.799879  3.22286   3.67838   3.50334\n 1.06162   5.582      4.42284   4.77983       1.72999   3.15565   1.98246   3.92069\n 2.99328   3.86546    4.64917   1.87552       4.54747   2.46617   0.182619  3.41675\n 2.77594   1.12118    5.68115   5.92246       4.70765   1.61819   5.80936   2.45663\n 5.81653   1.00421    1.23392   0.0387339  …  0.200388  4.55817   1.33563   0.956135\n 0.525149  2.99383    3.72502   0.572775      0.675371  3.58202   1.24862   0.0192285\n 5.45382   1.25224    4.0594    3.50149       3.0486    0.710542  0.192081  5.38466\n 5.94767   5.01267    0.88428   5.06324       2.19118   2.55926   5.92208   0.203939\n 1.98831   2.76684    4.31012   4.68156       1.26285   1.27338   1.47209   1.26658\n 5.30496   4.74467    2.11489   4.04942    …  2.18626   0.440583  3.66334   5.91902\n 0.195178  3.89477    5.97613   5.47285       5.05282   1.60926   3.59013   5.53988\n 4.76201   0.0670822  5.83784   4.35903       2.40861   0.616922  1.06166   1.87768\n 1.05233   2.48886    5.58365   4.58697       0.182273  4.76576   3.98617   3.05505\n 0.336744  5.77511    3.10872   4.37935       5.48705   5.83733   5.45954   4.98844\n 1.11855   1.07424    4.27741   4.47399    …  3.97035   5.62563   4.23378   2.60876\n 1.09357   5.34114    0.74001   0.216408      0.342061  1.13217   3.20539   5.18716\n 5.69556   3.12244    0.347957  0.683841      2.84005   5.40096   3.15125   1.34437\n 4.90718   5.57208    3.08056   5.40758       3.71409   0.276977  5.06768   4.40841\n 4.75448   0.0475118  4.58058   0.153437      5.34085   1.09899   3.18051   3.13105","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"A variety of other operations exist on the DArray, and it should generally behave otherwise similar to any other AbstractArray type. If you find that it's missing an operation that you need, please file an issue!","category":"page"},{"location":"darray/#Known-Supported-Operations","page":"Distributed Arrays","title":"Known Supported Operations","text":"","category":"section"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"This list is not exhaustive, but documents operations which are known to work well with the DArray:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"From Base:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"getindex/setindex!\nBroadcasting\nsimilar/copy/copyto!\nmap/reduce/mapreduce\nsum/prod\nminimum/maximum/extrema\nmap!","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"From Random:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"rand!/randn!","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"From Statistics:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"mean\nvar\nstd","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"From LinearAlgebra:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"transpose/adjoint (Out-of-place transpose)\n* (Out-of-place Matrix-(Matrix/Vector) multiply)\nmul! (In-place Matrix-Matrix multiply)\ncholesky/cholesky! (In-place/Out-of-place Cholesky factorization)\nlu/lu! (In-place/Out-of-place LU factorization (NoPivot only))","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"From AbstractFFTs:","category":"page"},{"location":"darray/","page":"Distributed Arrays","title":"Distributed Arrays","text":"fft/fft!\nifft/ifft!","category":"page"}]
}
